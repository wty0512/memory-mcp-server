#!/usr/bin/env python3
"""
Python Memory MCP Server
ä¸€å€‹åŸºæ–¼ Python çš„ Model Context Protocol ä¼ºæœå™¨ï¼Œæä¾›è¨˜æ†¶ç®¡ç†åŠŸèƒ½ï¼Œæ”¯æ´ SQLite å’Œ Markdown é›™å¾Œç«¯å„²å­˜

A Python-based Model Context Protocol server providing memory management 
with SQLite and Markdown dual backend storage support.

Features / åŠŸèƒ½ç‰¹è‰²:
- SQLite Backend (Default): High-performance database with full-text search
  SQLite å¾Œç«¯ï¼ˆé è¨­ï¼‰ï¼šé«˜æ•ˆèƒ½è³‡æ–™åº«ï¼Œæ”¯æ´å…¨æ–‡æœå°‹
- Markdown Backend: Human-readable format for version control
  Markdown å¾Œç«¯ï¼šäººé¡å¯è®€æ ¼å¼ï¼Œä¾¿æ–¼ç‰ˆæœ¬æ§åˆ¶
- Auto Sync: Auto-sync Markdown projects to SQLite
  è‡ªå‹•åŒæ­¥ï¼šè‡ªå‹•å°‡ Markdown å°ˆæ¡ˆåŒæ­¥åˆ° SQLite
- Auto Project Display: Show project list on startup
  è‡ªå‹•å°ˆæ¡ˆé¡¯ç¤ºï¼šå•Ÿå‹•æ™‚é¡¯ç¤ºå°ˆæ¡ˆåˆ—è¡¨
"""

import asyncio
import json
import logging
import os
import sys
import sqlite3
import time
import csv
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from abc import ABC, abstractmethod
from contextlib import contextmanager
import difflib
import re

# è·¨å¹³å°æª”æ¡ˆé–å®šæ”¯æ´
try:
    import fcntl  # Unix/Linux/macOS
    HAS_FCNTL = True
except ImportError:
    HAS_FCNTL = False

try:
    import msvcrt  # Windows
    HAS_MSVCRT = True
except ImportError:
    HAS_MSVCRT = False

# çµæ§‹åŒ–æ—¥èªŒç³»çµ±
class StructuredLogger:
    """çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„å™¨"""
    
    def __init__(self, name: str = __name__, config_manager=None):
        self.logger = logging.getLogger(name)
        self.config = config_manager
        self._setup_logging()
    
    def _setup_logging(self):
        """è¨­å®šæ—¥èªŒé…ç½®"""
        if self.config:
            level_str = self.config.get('logging', 'level', 'INFO')
            level = getattr(logging, level_str.upper(), logging.INFO)
            format_str = self.config.get('logging', 'format', 
                                       '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        else:
            level = logging.INFO
            format_str = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        
        # å¦‚æœå·²ç¶“æœ‰ handler å°±ä¸é‡è¤‡è¨­å®š
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(format_str)
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(level)
    
    def _format_structured(self, message: str, context: dict = None, **kwargs) -> str:
        """æ ¼å¼åŒ–çµæ§‹åŒ–è¨Šæ¯"""
        parts = [message]
        
        # åˆä½µ context å’Œ kwargs
        all_context = {}
        if context:
            all_context.update(context)
        all_context.update(kwargs)
        
        if all_context:
            context_str = " | ".join([f"{k}={v}" for k, v in all_context.items()])
            parts.append(f"[{context_str}]")
        
        return " ".join(parts)
    
    def debug(self, message: str, context: dict = None, **kwargs):
        """é™¤éŒ¯ç´šåˆ¥æ—¥èªŒ"""
        formatted = self._format_structured(message, context, **kwargs)
        self.logger.debug(formatted)
    
    def info(self, message: str, context: dict = None, **kwargs):
        """è³‡è¨Šç´šåˆ¥æ—¥èªŒ"""
        formatted = self._format_structured(message, context, **kwargs)
        self.logger.info(formatted)
    
    def warning(self, message: str, context: dict = None, **kwargs):
        """è­¦å‘Šç´šåˆ¥æ—¥èªŒ"""
        formatted = self._format_structured(message, context, **kwargs)
        self.logger.warning(formatted)
    
    def error(self, message: str, context: dict = None, **kwargs):
        """éŒ¯èª¤ç´šåˆ¥æ—¥èªŒ"""
        formatted = self._format_structured(message, context, **kwargs)
        self.logger.error(formatted)
    
    def critical(self, message: str, context: dict = None, **kwargs):
        """åš´é‡éŒ¯èª¤ç´šåˆ¥æ—¥èªŒ"""
        formatted = self._format_structured(message, context, **kwargs)
        self.logger.critical(formatted)
    
    def operation(self, operation: str, status: str, context: dict = None, **kwargs):
        """æ“ä½œæ—¥èªŒ"""
        all_context = {'operation': operation, 'status': status}
        if context:
            all_context.update(context)
        all_context.update(kwargs)
        
        if status.lower() in ['success', 'completed', 'ok']:
            self.info(f"Operation {operation} completed", all_context)
        elif status.lower() in ['error', 'failed', 'failure']:
            self.error(f"Operation {operation} failed", all_context)
        else:
            self.info(f"Operation {operation} status: {status}", all_context)
    
    def performance(self, operation: str, duration: float, context: dict = None, **kwargs):
        """æ•ˆèƒ½æ—¥èªŒ"""
        all_context = {'operation': operation, 'duration_ms': round(duration * 1000, 2)}
        if context:
            all_context.update(context)
        all_context.update(kwargs)
        
        if duration > 5.0:  # è¶…é5ç§’è¨˜ç‚ºè­¦å‘Š
            self.warning(f"Slow operation: {operation}", all_context)
        elif duration > 1.0:  # è¶…é1ç§’è¨˜ç‚ºè³‡è¨Š
            self.info(f"Performance: {operation}", all_context)
        else:
            self.debug(f"Performance: {operation}", all_context)

# è¨­å®šå…¨åŸŸæ—¥èªŒè¨˜éŒ„å™¨ï¼ˆç­‰é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å¾Œæœƒé‡æ–°é…ç½®ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = StructuredLogger(__name__)

# è‡ªå®šç¾©ç•°å¸¸é¡åˆ¥
class ValidationError(ValueError):
    """è¼¸å…¥é©—è­‰éŒ¯èª¤"""
    pass

class SecurityError(Exception):
    """å®‰å…¨æ€§ç›¸é—œéŒ¯èª¤"""
    pass

class DatabaseError(Exception):
    """è³‡æ–™åº«æ“ä½œéŒ¯èª¤"""
    pass

class FileOperationError(Exception):
    """æª”æ¡ˆæ“ä½œéŒ¯èª¤"""
    pass

class ConfigurationError(Exception):
    """é…ç½®éŒ¯èª¤"""
    pass

# è¼¸å…¥é©—è­‰å·¥å…·
class InputValidator:
    """è¼¸å…¥åƒæ•¸é©—è­‰å™¨"""
    
    # å°ˆæ¡ˆIDè¦å‰‡ï¼šåªå…è¨±å­—æ¯ã€æ•¸å­—ã€åº•ç·šã€é€£å­—è™Ÿï¼Œé•·åº¦é™åˆ¶
    @classmethod
    def _get_project_id_pattern(cls):
        max_len = config.get('validation', 'max_project_id_length', 100)
        return re.compile(f'^[a-zA-Z0-9_-]{{1,{max_len}}}$')
    
    @classmethod
    def _get_limits(cls):
        return {
            'content': config.get('validation', 'max_content_length', 50 * 1024 * 1024),
            'title': config.get('validation', 'max_title_length', 500),
            'category': config.get('validation', 'max_category_length', 100),
            'query': config.get('validation', 'max_query_length', 1000),
        }
    
    @classmethod
    def validate_project_id(cls, project_id: str) -> str:
        """é©—è­‰å°ˆæ¡ˆIDæ ¼å¼"""
        if not project_id or not isinstance(project_id, str):
            raise ValidationError("å°ˆæ¡ˆIDä¸èƒ½ç‚ºç©ºä¸”å¿…é ˆç‚ºå­—ä¸²")
        
        project_id = project_id.strip()
        if not project_id:
            raise ValidationError("å°ˆæ¡ˆIDä¸èƒ½ç‚ºç©ºç™½")
        
        pattern = cls._get_project_id_pattern()
        max_len = config.get('validation', 'max_project_id_length', 100)
        if not pattern.match(project_id):
            raise ValidationError(
                f"å°ˆæ¡ˆIDåªèƒ½åŒ…å«è‹±æ–‡å­—æ¯ã€æ•¸å­—ã€åº•ç·šå’Œé€£å­—è™Ÿï¼Œé•·åº¦ä¸è¶…é{max_len}å­—å…ƒ"
            )
        
        # æª¢æŸ¥ä¿ç•™å­—
        reserved_words = {'global', 'system', 'admin', 'root', 'config'}
        if project_id.lower() in reserved_words:
            raise ValidationError(f"'{project_id}' æ˜¯ä¿ç•™å­—ï¼Œä¸èƒ½ä½œç‚ºå°ˆæ¡ˆID")
        
        return project_id
    
    @classmethod
    def validate_content(cls, content: str) -> str:
        """é©—è­‰å…§å®¹æ ¼å¼å’Œé•·åº¦"""
        if not isinstance(content, str):
            raise ValidationError("å…§å®¹å¿…é ˆç‚ºå­—ä¸²")
        
        limits = cls._get_limits()
        if len(content) > limits['content']:
            raise ValidationError(f"å…§å®¹é•·åº¦ä¸èƒ½è¶…é {limits['content'] // (1024*1024)} MB")
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æƒ¡æ„å…§å®¹
        if cls._contains_suspicious_patterns(content):
            raise SecurityError("å…§å®¹åŒ…å«å¯ç–‘æ¨¡å¼ï¼Œå¯èƒ½å­˜åœ¨å®‰å…¨é¢¨éšª")
        
        return content
    
    @classmethod
    def validate_title(cls, title: str) -> str:
        """é©—è­‰æ¨™é¡Œæ ¼å¼å’Œé•·åº¦"""
        if title is None:
            return ""
        
        if not isinstance(title, str):
            raise ValidationError("æ¨™é¡Œå¿…é ˆç‚ºå­—ä¸²")
        
        title = title.strip()
        limits = cls._get_limits()
        if len(title) > limits['title']:
            raise ValidationError(f"æ¨™é¡Œé•·åº¦ä¸èƒ½è¶…é {limits['title']} å­—å…ƒ")
        
        return title
    
    @classmethod
    def validate_category(cls, category: str) -> str:
        """é©—è­‰åˆ†é¡æ ¼å¼å’Œé•·åº¦"""
        if category is None:
            return ""
        
        if not isinstance(category, str):
            raise ValidationError("åˆ†é¡å¿…é ˆç‚ºå­—ä¸²")
        
        category = category.strip()
        limits = cls._get_limits()
        if len(category) > limits['category']:
            raise ValidationError(f"åˆ†é¡é•·åº¦ä¸èƒ½è¶…é {limits['category']} å­—å…ƒ")
        
        # åˆ†é¡åªå…è¨±ç‰¹å®šå­—å…ƒ
        if category and not re.match(r'^[a-zA-Z0-9_\-\u4e00-\u9fff\s]+$', category):
            raise ValidationError("åˆ†é¡åªèƒ½åŒ…å«è‹±æ–‡å­—æ¯ã€æ•¸å­—ã€ä¸­æ–‡ã€åº•ç·šã€é€£å­—è™Ÿå’Œç©ºæ ¼")
        
        return category
    
    @classmethod
    def validate_query(cls, query: str) -> str:
        """é©—è­‰æœå°‹æŸ¥è©¢"""
        if not isinstance(query, str):
            raise ValidationError("æœå°‹æŸ¥è©¢å¿…é ˆç‚ºå­—ä¸²")
        
        query = query.strip()
        if not query:
            raise ValidationError("æœå°‹æŸ¥è©¢ä¸èƒ½ç‚ºç©º")
        
        limits = cls._get_limits()
        if len(query) > limits['query']:
            raise ValidationError(f"æœå°‹æŸ¥è©¢é•·åº¦ä¸èƒ½è¶…é {limits['query']} å­—å…ƒ")
        
        return query
    
    @classmethod
    def _contains_suspicious_patterns(cls, content: str) -> bool:
        """æª¢æŸ¥å…§å®¹æ˜¯å¦åŒ…å«å¯ç–‘æ¨¡å¼"""
        suspicious_patterns = [
            r'<script[^>]*>.*?</script>',  # JavaScript
            r'javascript:',  # JavaScript å”è­°
            r'vbscript:',   # VBScript å”è­°
            r'on\w+\s*=',   # äº‹ä»¶è™•ç†å™¨
            r'eval\s*\(',   # eval å‡½æ•¸
            r'exec\s*\(',   # exec å‡½æ•¸
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, content, re.IGNORECASE | re.DOTALL):
                return True
        
        return False

# SQL å®‰å…¨å·¥å…·
class SQLSafetyUtils:
    """SQL å®‰å…¨å·¥å…·é¡åˆ¥"""
    
    # SQL é—œéµå­—é»‘åå–®
    DANGEROUS_KEYWORDS = {
        'drop', 'delete', 'truncate', 'alter', 'create', 'insert', 'update',
        'exec', 'execute', 'sp_', 'xp_', 'union', 'select', 'from', 'where',
        'having', 'group', 'order', 'limit', 'offset', 'join', 'inner', 'outer',
        'left', 'right', 'full', 'cross', 'on', 'as', 'distinct', 'all', 'any',
        'exists', 'in', 'like', 'between', 'is', 'null', 'not', 'and', 'or',
        'case', 'when', 'then', 'else', 'end', 'cast', 'convert'
    }
    
    @classmethod
    def sanitize_sql_identifier(cls, identifier: str) -> str:
        """æ¸…ç† SQL è­˜åˆ¥ç¬¦ï¼ˆè¡¨åã€æ¬„ä½åç­‰ï¼‰"""
        if not identifier:
            raise ValidationError("SQL è­˜åˆ¥ç¬¦ä¸èƒ½ç‚ºç©º")
        
        # åªå…è¨±å­—æ¯ã€æ•¸å­—ã€åº•ç·š
        if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', identifier):
            raise ValidationError(f"ç„¡æ•ˆçš„ SQL è­˜åˆ¥ç¬¦: {identifier}")
        
        # æª¢æŸ¥é•·åº¦
        if len(identifier) > 64:
            raise ValidationError("SQL è­˜åˆ¥ç¬¦é•·åº¦ä¸èƒ½è¶…é 64 å­—å…ƒ")
        
        # æª¢æŸ¥æ˜¯å¦ç‚º SQL é—œéµå­—
        if identifier.lower() in cls.DANGEROUS_KEYWORDS:
            raise ValidationError(f"'{identifier}' æ˜¯ SQL ä¿ç•™å­—ï¼Œä¸èƒ½ä½œç‚ºè­˜åˆ¥ç¬¦")
        
        return identifier
    
    @classmethod
    def build_safe_where_clause(cls, conditions: Dict[str, Any]) -> Tuple[str, List[Any]]:
        """å®‰å…¨åœ°æ§‹å»º WHERE å­å¥"""
        if not conditions:
            return "", []
        
        where_parts = []
        params = []
        
        for column, value in conditions.items():
            # é©—è­‰æ¬„ä½åç¨±
            safe_column = cls.sanitize_sql_identifier(column)
            where_parts.append(f"{safe_column} = ?")
            params.append(value)
        
        where_clause = " AND ".join(where_parts)
        return where_clause, params
    
    @classmethod
    def build_safe_update_clause(cls, updates: Dict[str, Any]) -> Tuple[str, List[Any]]:
        """å®‰å…¨åœ°æ§‹å»º UPDATE SET å­å¥"""
        if not updates:
            raise ValidationError("UPDATE èªå¥å¿…é ˆåŒ…å«è‡³å°‘ä¸€å€‹æ›´æ–°æ¬„ä½")
        
        set_parts = []
        params = []
        
        for column, value in updates.items():
            # é©—è­‰æ¬„ä½åç¨±
            safe_column = cls.sanitize_sql_identifier(column)
            set_parts.append(f"{safe_column} = ?")
            params.append(value)
        
        set_clause = ", ".join(set_parts)
        return set_clause, params
    
    @classmethod
    def validate_sql_query(cls, query: str) -> bool:
        """é©—è­‰ SQL æŸ¥è©¢æ˜¯å¦å®‰å…¨"""
        if not query:
            return False
        
        query_lower = query.lower().strip()
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å±éšªæ“ä½œ
        dangerous_patterns = [
            r';\s*(drop|delete|truncate|alter|create)\s',
            r'union\s+select',
            r'exec\s*\(',
            r'execute\s*\(',
            r'sp_\w+',
            r'xp_\w+',
            r'--',  # SQL æ³¨é‡‹
            r'/\*.*\*/',  # å¤šè¡Œæ³¨é‡‹
        ]
        
        for pattern in dangerous_patterns:
            if re.search(pattern, query_lower):
                logger.warning(f"Detected dangerous SQL pattern: {pattern}")
                return False
        
        return True

# æª”æ¡ˆè·¯å¾‘å®‰å…¨å·¥å…·
class PathSafetyUtils:
    """æª”æ¡ˆè·¯å¾‘å®‰å…¨é©—è­‰å·¥å…·"""
    
    @classmethod
    def validate_safe_path(cls, path: Path, base_path: Path, operation: str = "access") -> Path:
        """é©—è­‰è·¯å¾‘æ˜¯å¦å®‰å…¨ï¼Œé˜²æ­¢ç›®éŒ„éæ­·æ”»æ“Š"""
        try:
            # è§£æè·¯å¾‘ç‚ºçµ•å°è·¯å¾‘
            abs_path = path.resolve()
            abs_base = base_path.resolve()
            
            # æª¢æŸ¥è·¯å¾‘æ˜¯å¦åœ¨åŸºç¤ç›®éŒ„å…§
            try:
                abs_path.relative_to(abs_base)
            except ValueError:
                raise SecurityError(
                    f"è·¯å¾‘ '{path}' è¶…å‡ºå…è¨±çš„åŸºç¤ç›®éŒ„ '{base_path}' ç¯„åœ"
                )
            
            # æª¢æŸ¥è·¯å¾‘é•·åº¦
            max_path_len = config.get('paths', 'max_path_length', 4096)
            if len(str(abs_path)) > max_path_len:
                raise ValidationError(f"æª”æ¡ˆè·¯å¾‘é•·åº¦ä¸èƒ½è¶…é {max_path_len} å­—å…ƒ")
            
            # æª¢æŸ¥å±éšªçš„è·¯å¾‘çµ„ä»¶ï¼ˆä½†æ’é™¤æ ¹ç›®éŒ„çš„æ­£å¸¸çµ„ä»¶ï¼‰
            dangerous_components = {'..', '~', '$'}
            for component in abs_path.parts[1:]:  # è·³éæ ¹è·¯å¾‘
                if component in dangerous_components:
                    raise SecurityError(f"è·¯å¾‘åŒ…å«å±éšªçµ„ä»¶: '{component}'")
                
                # æª¢æŸ¥ç‰¹æ®Šå­—å…ƒ
                if any(char in component for char in ['<', '>', ':', '|', '?', '*']):
                    raise SecurityError(f"è·¯å¾‘çµ„ä»¶åŒ…å«éæ³•å­—å…ƒ: '{component}'")
            
            # æª¢æŸ¥æª”æ¡ˆåç¨±é•·åº¦
            max_filename_len = config.get('paths', 'max_filename_length', 255)
            if abs_path.name and len(abs_path.name) > max_filename_len:
                raise ValidationError(f"æª”æ¡ˆåç¨±é•·åº¦ä¸èƒ½è¶…é {max_filename_len} å­—å…ƒ")
            
            # è¨˜éŒ„å®‰å…¨æ“ä½œ
            logger.debug(f"Path validation passed for {operation}: {abs_path}")
            return abs_path
            
        except (OSError, ValueError) as e:
            raise SecurityError(f"è·¯å¾‘é©—è­‰å¤±æ•—: {e}")
    
    @classmethod
    def validate_filename(cls, filename: str) -> str:
        """é©—è­‰æª”æ¡ˆåç¨±æ˜¯å¦å®‰å…¨"""
        if not filename or not isinstance(filename, str):
            raise ValidationError("æª”æ¡ˆåç¨±ä¸èƒ½ç‚ºç©º")
        
        filename = filename.strip()
        if not filename:
            raise ValidationError("æª”æ¡ˆåç¨±ä¸èƒ½ç‚ºç©ºç™½")
        
        # æª¢æŸ¥é•·åº¦
        if len(filename) > 255:
            raise ValidationError("æª”æ¡ˆåç¨±éé•·")
        
        # æª¢æŸ¥éæ³•å­—å…ƒ
        illegal_chars = ['/', '\\', '<', '>', ':', '|', '?', '*', '"']
        for char in illegal_chars:
            if char in filename:
                raise SecurityError(f"æª”æ¡ˆåç¨±åŒ…å«éæ³•å­—å…ƒ: '{char}'")
        
        # æª¢æŸ¥ä¿ç•™åç¨± (Windows)
        reserved_names = {
            'CON', 'PRN', 'AUX', 'NUL', 'COM1', 'COM2', 'COM3', 'COM4', 
            'COM5', 'COM6', 'COM7', 'COM8', 'COM9', 'LPT1', 'LPT2', 
            'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9'
        }
        if filename.upper() in reserved_names:
            raise SecurityError(f"'{filename}' æ˜¯ç³»çµ±ä¿ç•™åç¨±")
        
        # æª¢æŸ¥éš±è—æª”æ¡ˆæ¨¡å¼
        if filename.startswith('.') and len(filename) > 1:
            logger.warning(f"Creating hidden file: {filename}")
        
        return filename
    
    @classmethod
    def sanitize_project_id_for_path(cls, project_id: str) -> str:
        """æ¸…ç†å°ˆæ¡ˆIDä½¿å…¶é©åˆä½œç‚ºæª”æ¡ˆè·¯å¾‘"""
        if not project_id:
            raise ValidationError("å°ˆæ¡ˆIDä¸èƒ½ç‚ºç©º")
        
        # ç§»é™¤æˆ–æ›¿æ›å±éšªå­—å…ƒ
        safe_chars = []
        for char in project_id:
            if char.isalnum() or char in '-_':
                safe_chars.append(char)
            elif char in ' \t':
                safe_chars.append('_')
            # å…¶ä»–å­—å…ƒç›´æ¥å¿½ç•¥
        
        safe_id = ''.join(safe_chars)
        if not safe_id:
            raise ValidationError("å°ˆæ¡ˆIDæ¸…ç†å¾Œç‚ºç©º")
        
        # ç¢ºä¿é•·åº¦é©ä¸­
        if len(safe_id) > 100:
            safe_id = safe_id[:100]
        
        return safe_id

# é…ç½®ç®¡ç†ç³»çµ±
class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    # é è¨­é…ç½®
    DEFAULT_CONFIG = {
        # è³‡æ–™åº«é…ç½®
        'database': {
            'timeout': 30.0,
            'check_same_thread': False,
            'text_factory': 'str',
            'encoding': 'UTF-8',
            'page_size': 4096,
            'cache_size': -2000,  # 2MB
        },
        
        # æª”æ¡ˆæ“ä½œé…ç½®
        'file_operations': {
            'lock_timeout': 30.0,
            'lock_retry_delay': 0.1,
            'atomic_write': True,
            'backup_enabled': False,
        },
        
        # æœå°‹é…ç½®
        'search': {
            'default_limit': 10,
            'max_limit': 100,
            'token_limit': 1500,
            'content_preview_length': 400,
        },
        
        # é©—è­‰é…ç½®
        'validation': {
            'max_content_length': 50 * 1024 * 1024,  # 50MB
            'max_title_length': 500,
            'max_category_length': 100,
            'max_query_length': 1000,
            'max_project_id_length': 100,
        },
        
        # è·¯å¾‘é…ç½®
        'paths': {
            'max_path_length': 4096,
            'max_filename_length': 255,
            'memory_dir': 'ai-memory',
        },
        
        # æ—¥èªŒé…ç½®
        'logging': {
            'level': 'INFO',
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            'max_file_size': 10 * 1024 * 1024,  # 10MB
            'backup_count': 5,
        },
        
        # æ•ˆèƒ½é…ç½®
        'performance': {
            'connection_pool_size': 5,
            'query_cache_size': 100,
            'enable_profiling': False,
        }
    }
    
    def __init__(self, config_file: str = None, config_dict: dict = None):
        """åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨"""
        self.config = self.DEFAULT_CONFIG.copy()
        self.config_file = config_file
        
        if config_dict:
            self._merge_config(config_dict)
        elif config_file and Path(config_file).exists():
            self._load_from_file(config_file)
        
        # è¼‰å…¥ç’°å¢ƒè®Šæ•¸è¦†å¯«
        self._load_from_env()
    
    def _merge_config(self, config_dict: dict):
        """åˆä½µé…ç½®å­—å…¸"""
        def deep_merge(base, override):
            for key, value in override.items():
                if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                    deep_merge(base[key], value)
                else:
                    base[key] = value
        
        deep_merge(self.config, config_dict)
    
    def _load_from_file(self, config_file: str):
        """å¾æª”æ¡ˆè¼‰å…¥é…ç½®"""
        try:
            import json
            with open(config_file, 'r', encoding='utf-8') as f:
                file_config = json.load(f)
            self._merge_config(file_config)
            logger.info(f"Configuration loaded from {config_file}")
        except (json.JSONDecodeError, IOError) as e:
            logger.warning(f"Failed to load config from {config_file}: {e}")
            raise ConfigurationError(f"é…ç½®æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
    
    def _load_from_env(self):
        """å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥é…ç½®"""
        env_mappings = {
            'MEMORY_DB_TIMEOUT': ('database', 'timeout', float),
            'MEMORY_LOCK_TIMEOUT': ('file_operations', 'lock_timeout', float),
            'MEMORY_SEARCH_LIMIT': ('search', 'default_limit', int),
            'MEMORY_MAX_CONTENT_SIZE': ('validation', 'max_content_length', int),
            'MEMORY_LOG_LEVEL': ('logging', 'level', str),
            'MEMORY_DIR': ('paths', 'memory_dir', str),
        }
        
        for env_key, (section, key, type_func) in env_mappings.items():
            env_value = os.getenv(env_key)
            if env_value is not None:
                try:
                    self.config[section][key] = type_func(env_value)
                    logger.debug(f"Config override from env: {env_key}={env_value}")
                except (ValueError, TypeError) as e:
                    logger.warning(f"Invalid env config {env_key}={env_value}: {e}")
    
    def get(self, section: str, key: str = None, default=None):
        """å–å¾—é…ç½®å€¼"""
        try:
            if key is None:
                return self.config.get(section, default)
            return self.config.get(section, {}).get(key, default)
        except (KeyError, TypeError):
            return default
    
    def set(self, section: str, key: str, value):
        """è¨­å®šé…ç½®å€¼"""
        if section not in self.config:
            self.config[section] = {}
        self.config[section][key] = value
    
    def save_to_file(self, filename: str):
        """å„²å­˜é…ç½®åˆ°æª”æ¡ˆ"""
        try:
            import json
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            logger.info(f"Configuration saved to {filename}")
        except IOError as e:
            logger.error(f"Failed to save config to {filename}: {e}")
            raise ConfigurationError(f"é…ç½®æª”æ¡ˆå„²å­˜å¤±æ•—: {e}")
    
    def validate(self):
        """é©—è­‰é…ç½®æœ‰æ•ˆæ€§"""
        required_sections = ['database', 'file_operations', 'search', 'validation']
        for section in required_sections:
            if section not in self.config:
                raise ConfigurationError(f"ç¼ºå°‘å¿…è¦é…ç½®å€å¡Š: {section}")
        
        # é©—è­‰æ•¸å€¼ç¯„åœ
        if self.get('database', 'timeout', 0) <= 0:
            raise ConfigurationError("è³‡æ–™åº«è¶…æ™‚æ™‚é–“å¿…é ˆå¤§æ–¼0")
        
        if self.get('search', 'max_limit', 0) <= 0:
            raise ConfigurationError("æœå°‹çµæœä¸Šé™å¿…é ˆå¤§æ–¼0")
        
        logger.info("Configuration validation passed")

# å…¨åŸŸé…ç½®å¯¦ä¾‹
config = ConfigManager()

# é‡æ–°é…ç½®æ—¥èªŒè¨˜éŒ„å™¨ä½¿ç”¨é…ç½®ç®¡ç†å™¨
logger = StructuredLogger(__name__, config)

# æ•ˆèƒ½æ¸¬é‡è£é£¾å™¨
def log_performance(operation_name: str = None):
    """æ•ˆèƒ½æ¸¬é‡è£é£¾å™¨"""
    def decorator(func):
        import functools
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            op_name = operation_name or f"{func.__name__}"
            start_time = time.time()
            
            try:
                result = func(*args, **kwargs)
                duration = time.time() - start_time
                
                # è¨˜éŒ„æˆåŠŸçš„æ“ä½œ
                context = {
                    'function': func.__name__,
                    'args_count': len(args),
                    'kwargs_count': len(kwargs)
                }
                logger.performance(op_name, duration, context, status='success')
                
                return result
                
            except Exception as e:
                duration = time.time() - start_time
                
                # è¨˜éŒ„å¤±æ•—çš„æ“ä½œ
                context = {
                    'function': func.__name__,
                    'args_count': len(args),
                    'kwargs_count': len(kwargs),
                    'error_type': type(e).__name__,
                    'error_message': str(e)
                }
                logger.performance(op_name, duration, context, status='error')
                
                raise
        
        return wrapper
    return decorator

class FileLock:
    """è·¨å¹³å°æª”æ¡ˆé–å®šå·¥å…·é¡"""
    
    def __init__(self, file_path: Path, timeout: float = None, retry_delay: float = None):
        self.file_path = file_path
        self.timeout = timeout or config.get('file_operations', 'lock_timeout', 30.0)
        self.retry_delay = retry_delay or config.get('file_operations', 'lock_retry_delay', 0.1)
        self.lock_file = None
        self.locked = False
    
    def __enter__(self):
        self.acquire()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()
    
    def acquire(self):
        """å–å¾—æª”æ¡ˆé–å®š"""
        if self.locked:
            return
        
        # å‰µå»ºé–å®šæª”æ¡ˆè·¯å¾‘
        lock_file_path = self.file_path.with_suffix(self.file_path.suffix + '.lock')
        
        start_time = time.time()
        while time.time() - start_time < self.timeout:
            try:
                # å˜—è©¦å‰µå»ºé–å®šæª”æ¡ˆ
                self.lock_file = open(lock_file_path, 'w')
                
                if HAS_FCNTL:
                    # Unix/Linux/macOS ä½¿ç”¨ fcntl
                    fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
                elif HAS_MSVCRT:
                    # Windows ä½¿ç”¨ msvcrt
                    msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_NBLCK, 1)
                else:
                    # é™ç´šæ–¹æ¡ˆï¼šä½¿ç”¨æª”æ¡ˆå­˜åœ¨æ€§æª¢æŸ¥
                    if lock_file_path.exists():
                        raise OSError("Lock file exists")
                
                # å¯«å…¥é–å®šè³‡è¨Š
                self.lock_file.write(f"Locked by PID {os.getpid()} at {time.time()}\n")
                self.lock_file.flush()
                
                self.locked = True
                logger.debug(f"Acquired lock for {self.file_path}")
                return
                
            except (OSError, IOError) as e:
                # é–å®šå¤±æ•—ï¼Œæ¸…ç†ä¸¦é‡è©¦
                if self.lock_file:
                    try:
                        self.lock_file.close()
                    except:
                        pass
                    self.lock_file = None
                
                logger.debug(f"Lock acquisition failed for {self.file_path}: {e}")
                time.sleep(self.retry_delay)
                continue
        
        # è¶…æ™‚å¤±æ•—
        raise TimeoutError(f"Failed to acquire lock for {self.file_path} within {self.timeout} seconds")
    
    def release(self):
        """é‡‹æ”¾æª”æ¡ˆé–å®š"""
        if not self.locked or not self.lock_file:
            return
        
        try:
            if HAS_FCNTL:
                fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)
            elif HAS_MSVCRT:
                msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_UNLCK, 1)
            
            self.lock_file.close()
            
            # åˆªé™¤é–å®šæª”æ¡ˆ
            lock_file_path = self.file_path.with_suffix(self.file_path.suffix + '.lock')
            try:
                lock_file_path.unlink()
            except FileNotFoundError:
                pass
            
            logger.debug(f"Released lock for {self.file_path}")
            
        except (OSError, IOError) as e:
            logger.warning(f"I/O error releasing lock for {self.file_path}: {e}")
        except PermissionError as e:
            logger.warning(f"Permission denied releasing lock for {self.file_path}: {e}")
        except Exception as e:
            logger.error(f"Unexpected error releasing lock for {self.file_path}: {e}")
        finally:
            self.lock_file = None
            self.locked = False

class AtomicFileWriter:
    """åŸå­æª”æ¡ˆå¯«å…¥å·¥å…·é¡"""
    
    def __init__(self, file_path: Path):
        self.file_path = file_path
        self.temp_path = file_path.with_suffix(file_path.suffix + '.tmp')
        self.temp_file = None
    
    def __enter__(self):
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self.file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # é–‹å•Ÿè‡¨æ™‚æª”æ¡ˆ
        self.temp_file = open(self.temp_path, 'w', encoding='utf-8')
        return self.temp_file
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.temp_file:
            self.temp_file.close()
        
        if exc_type is None:
            # æˆåŠŸï¼šåŸå­æ€§é‡å‘½å
            try:
                self.temp_path.replace(self.file_path)
                logger.debug(f"Atomic write completed for {self.file_path}")
            except (OSError, IOError) as e:
                logger.error(f"I/O error during atomic write for {self.file_path}: {e}")
                self._cleanup_temp_file()
                raise FileOperationError(f"åŸå­æ€§å¯«å…¥å¤±æ•—: {e}")
            except PermissionError as e:
                logger.error(f"Permission denied during atomic write for {self.file_path}: {e}")
                self._cleanup_temp_file()
                raise FileOperationError(f"æª”æ¡ˆæ¬Šé™éŒ¯èª¤: {e}")
            except Exception as e:
                logger.error(f"Unexpected error during atomic write for {self.file_path}: {e}")
                self._cleanup_temp_file()
                raise
        else:
            # å¤±æ•—ï¼šæ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            self._cleanup_temp_file()
    
    def _cleanup_temp_file(self):
        """æ¸…ç†è‡¨æ™‚æª”æ¡ˆ"""
        try:
            self.temp_path.unlink()
        except FileNotFoundError:
            pass
        except (OSError, IOError) as e:
            logger.warning(f"Failed to cleanup temp file {self.temp_path}: {e}")


class MemoryBackend(ABC):
    """
    è¨˜æ†¶å¾Œç«¯æŠ½è±¡åŸºé¡
    Abstract base class for memory backends
    
    å®šç¾©æ‰€æœ‰è¨˜æ†¶å¾Œç«¯å¿…é ˆå¯¦ä½œçš„ä»‹é¢æ–¹æ³•
    Defines interface methods that all memory backends must implement
    """
    
    @abstractmethod
    def save_memory(self, project_id: str, content: str, title: str = "", category: str = "") -> bool:
        """å„²å­˜è¨˜æ†¶åˆ°å¾Œç«¯"""
        pass
    
    @abstractmethod
    def get_memory(self, project_id: str) -> Optional[str]:
        """è®€å–å®Œæ•´å°ˆæ¡ˆè¨˜æ†¶"""
        pass
    
    @abstractmethod
    def search_memory(self, project_id: str, query: str, limit: int = 10) -> List[Dict[str, str]]:
        """æœå°‹è¨˜æ†¶å…§å®¹"""
        pass
    
    def rag_query(self, project_id: str, question: str, context_limit: int = 5, max_tokens: int = 2000) -> Dict[str, Any]:
        """
        RAG æŸ¥è©¢ï¼šåŸºæ–¼å°ˆæ¡ˆè¨˜æ†¶å›ç­”å•é¡Œ
        æª¢ç´¢ç›¸é—œå…§å®¹ä¸¦æ§‹å»ºç”¨æ–¼å›ç­”çš„ä¸Šä¸‹æ–‡
        """
        # 1. æª¢ç´¢ç›¸é—œå…§å®¹
        relevant_docs = self.search_memory(project_id, question, context_limit * 2)
        
        if not relevant_docs:
            return {
                'status': 'no_context',
                'answer': f'æ²’æœ‰æ‰¾åˆ°èˆ‡ã€Œ{question}ã€ç›¸é—œçš„å°ˆæ¡ˆè¨˜æ†¶å…§å®¹ã€‚',
                'context_sources': [],
                'suggestions': [
                    f"ä½¿ç”¨ save_project_memory è¨˜éŒ„èˆ‡ã€Œ{question}ã€ç›¸é—œçš„è³‡è¨Š",
                    f"å˜—è©¦ä½¿ç”¨ä¸åŒçš„é—œéµå­—æœå°‹",
                    f"æª¢æŸ¥å°ˆæ¡ˆIDã€Œ{project_id}ã€æ˜¯å¦æ­£ç¢º"
                ]
            }
        
        # 2. é¸æ“‡æœ€ç›¸é—œçš„å…§å®¹ä¸¦æ§åˆ¶ token æ•¸é‡
        selected_docs = []
        total_tokens = 0
        
        for doc in relevant_docs[:context_limit]:
            content = doc.get('entry', doc.get('content', ''))
            content_tokens = len(content) // 4  # ç²—ç•¥ä¼°è¨ˆ token æ•¸
            
            if total_tokens + content_tokens <= max_tokens:
                selected_docs.append({
                    'title': doc.get('title', 'ç„¡æ¨™é¡Œ'),
                    'category': doc.get('category', ''),
                    'timestamp': doc.get('timestamp', doc.get('created_at', '')),
                    'content': content,
                    'relevance': doc.get('similarity', doc.get('relevance', 0))
                })
                total_tokens += content_tokens
            else:
                break
        
        # 3. æ§‹å»ºçµæ§‹åŒ–ä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(selected_docs, 1):
            context_part = f"ã€è¨˜æ†¶ {i}ã€‘"
            if doc['title']:
                context_part += f" {doc['title']}"
            if doc['category']:
                context_part += f" #{doc['category']}"
            if doc['timestamp']:
                context_part += f" ({doc['timestamp'][:16]})"
            context_part += f"\n{doc['content']}\n"
            context_parts.append(context_part)
        
        context_text = "\n---\n\n".join(context_parts)
        
        # 4. æ§‹å»º RAG æç¤ºè©
        rag_prompt = f"""åŸºæ–¼ä»¥ä¸‹å°ˆæ¡ˆè¨˜æ†¶å…§å®¹å›ç­”å•é¡Œï¼š

å°ˆæ¡ˆï¼š{project_id}
å•é¡Œï¼š{question}

ç›¸é—œè¨˜æ†¶å…§å®¹ï¼š
{context_text}

---

è«‹åŸºæ–¼ä¸Šè¿°è¨˜æ†¶å…§å®¹æä¾›æº–ç¢ºçš„å›ç­”ï¼š
1. å¦‚æœè¨˜æ†¶ä¸­æœ‰ç›´æ¥ç›¸é—œçš„è³‡è¨Šï¼Œè«‹è©³ç´°å›ç­”
2. å¦‚æœåªæœ‰éƒ¨åˆ†ç›¸é—œè³‡è¨Šï¼Œè«‹èªªæ˜å·²çŸ¥çš„éƒ¨åˆ†ä¸¦æŒ‡å‡ºç¼ºå°‘ä»€éº¼
3. å¦‚æœè¨˜æ†¶å…§å®¹ä¸è¶³ä»¥å›ç­”å•é¡Œï¼Œè«‹æ˜ç¢ºèªªæ˜ä¸¦å»ºè­°ä¸‹ä¸€æ­¥è¡Œå‹•

è«‹ç”¨å°ˆæ¥­ä½†æ˜“æ‡‚çš„æ–¹å¼å›ç­”ã€‚"""

        return {
            'status': 'success',
            'prompt': rag_prompt,
            'context_sources': [
                {
                    'title': doc['title'],
                    'category': doc['category'],
                    'timestamp': doc['timestamp'],
                    'relevance': doc.get('relevance', 0),
                    'content_preview': doc['content'][:200] + ('...' if len(doc['content']) > 200 else '')
                }
                for doc in selected_docs
            ],
            'context_count': len(selected_docs),
            'estimated_tokens': total_tokens,
            'question': question
        }
    
    def _analyze_query_type(self, query: str) -> str:
        """
        æ™ºèƒ½æŸ¥è©¢é¡å‹åˆ†æ
        åˆ†ææŸ¥è©¢æ„åœ–ï¼Œæ±ºå®šä½¿ç”¨å“ªç¨®æœå°‹ç­–ç•¥
        """
        query_lower = query.lower()
        
        # ç°¡å–®æŸ¥æ‰¾é—œéµè©ï¼šåªéœ€è¦æ¨™é¡Œæˆ–æ‘˜è¦
        simple_keywords = [
            'åˆ—è¡¨', 'list', 'æœ‰å“ªäº›', 'æ‰¾åˆ°', 'find', 'æœå°‹', 'search',
            'é¡¯ç¤º', 'show', 'æŸ¥çœ‹', 'view', 'æ‰€æœ‰', 'all', 'æœ€æ–°', 'recent'
        ]
        
        # è¤‡é›œå•é¡Œé—œéµè©ï¼šéœ€è¦å®Œæ•´å…§å®¹åˆ†æ  
        complex_keywords = [
            'ç‚ºä»€éº¼', 'why', 'å¦‚ä½•', 'how', 'è§£é‡‹', 'explain', 'åˆ†æ', 'analyze',
            'æ¯”è¼ƒ', 'compare', 'å·®ç•°', 'difference', 'åŸå› ', 'reason', 'æ–¹æ³•', 'method',
            'å»ºè­°', 'suggest', 'æ¨è–¦', 'recommend', 'å„ªç¼ºé»', 'pros and cons'
        ]
        
        # æª¢æŸ¥ç°¡å–®æŸ¥æ‰¾æ¨¡å¼
        if any(keyword in query_lower for keyword in simple_keywords):
            return 'simple_lookup'
        
        # æª¢æŸ¥è¤‡é›œå•é¡Œæ¨¡å¼
        if any(keyword in query_lower for keyword in complex_keywords):
            return 'complex_question'
        
        # æ ¹æ“šé•·åº¦åˆ¤æ–·
        if len(query) > 50:
            return 'complex_question'
        elif len(query.split()) > 8:
            return 'complex_question'
        
        return 'simple_lookup'
    
    def smart_search(self, project_id: str, query: str, limit: int = 10) -> Dict[str, Any]:
        """
        æ™ºèƒ½æœç´¢è·¯ç”±ï¼šæ ¹æ“šæŸ¥è©¢é¡å‹è‡ªå‹•é¸æ“‡æœ€ä½³ç­–ç•¥
        é€™å€‹æ–¹æ³•å°ç”¨æˆ¶é€æ˜ï¼Œè‡ªå‹•å„ªåŒ– token ä½¿ç”¨
        """
        query_type = self._analyze_query_type(query)
        
        logger.info(f"æ™ºèƒ½è·¯ç”±: æŸ¥è©¢é¡å‹={query_type}, æŸ¥è©¢='{query[:30]}...'")
        
        if query_type == 'simple_lookup':
            # ç°¡å–®æŸ¥æ‰¾ï¼šè¿”å›æ¨™é¡Œå’Œæ‘˜è¦ï¼Œå¤§å¹…ç¯€çœ token
            search_results = self.search_memory(project_id, query, limit)
            
            # åªè¿”å›é—œéµè³‡è¨Šï¼Œç¯€çœ token
            simplified_results = []
            for result in search_results:
                content = result.get('entry', result.get('content', ''))
                # åªä¿ç•™å‰ 150 å­—ç¬¦ä½œç‚ºæ‘˜è¦
                summary = content[:150] + ('...' if len(content) > 150 else '')
                
                simplified_results.append({
                    'title': result.get('title', 'ç„¡æ¨™é¡Œ'),
                    'category': result.get('category', ''),
                    'timestamp': result.get('timestamp', result.get('created_at', '')),
                    'summary': summary,
                    'full_content_available': len(content) > 150
                })
            
            return {
                'strategy': 'simple_lookup',
                'token_saved': True,
                'results': simplified_results,
                'result_count': len(simplified_results),
                'message': f"æ‰¾åˆ° {len(simplified_results)} æ¢ç›¸é—œè¨˜æ†¶ï¼ˆæ‘˜è¦æ¨¡å¼ï¼Œå·²ç¯€çœ ~70% tokenï¼‰"
            }
            
        elif query_type == 'complex_question':
            # è¤‡é›œå•é¡Œï¼šä½¿ç”¨ RAG ç­–ç•¥
            rag_result = self.rag_query(project_id, query, limit, max_tokens=1500)
            
            if rag_result['status'] == 'success':
                return {
                    'strategy': 'complex_question',
                    'token_optimized': True,
                    'results': rag_result['context_sources'],
                    'result_count': rag_result['context_count'],
                    'rag_prompt': rag_result['prompt'],
                    'message': f"å·²æº–å‚™å›ç­”è¤‡é›œå•é¡Œï¼ˆRAGæ¨¡å¼ï¼Œä½¿ç”¨ {rag_result['estimated_tokens']} tokensï¼‰"
                }
            else:
                return {
                    'strategy': 'complex_question',
                    'results': [],
                    'result_count': 0,
                    'message': rag_result.get('answer', 'ç„¡æ³•è™•ç†æ­¤è¤‡é›œå•é¡Œ')
                }
        
        else:
            # é è¨­ï¼šæ··åˆç­–ç•¥
            return {
                'strategy': 'hybrid',
                'results': self.search_memory(project_id, query, limit),
                'message': f"ä½¿ç”¨æ··åˆæœå°‹ç­–ç•¥"
            }
    
    def summarize_project(self, project_id: str, summary_type: str = 'brief', max_entries: int = 20) -> Dict[str, Any]:
        """
        ç”Ÿæˆå°ˆæ¡ˆå…§å®¹æ‘˜è¦
        æ ¹æ“šä¸åŒé¡å‹æä¾›å°ˆæ¡ˆæ¦‚æ³
        """
        try:
            # ç²å–å°ˆæ¡ˆçµ±è¨ˆè³‡è¨Š
            stats = self.get_memory_stats(project_id) if hasattr(self, 'get_memory_stats') else {'exists': False}
            
            if not stats.get('exists', False):
                return {
                    'status': 'not_found',
                    'message': f'å°ˆæ¡ˆ "{project_id}" ä¸å­˜åœ¨æˆ–æ²’æœ‰å…§å®¹',
                    'suggestions': [
                        f"ä½¿ç”¨ save_project_memory é–‹å§‹è¨˜éŒ„å°ˆæ¡ˆå…§å®¹",
                        f"æª¢æŸ¥å°ˆæ¡ˆIDæ‹¼å¯«æ˜¯å¦æ­£ç¢º",
                        f"ä½¿ç”¨ list_memory_projects æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å°ˆæ¡ˆ"
                    ]
                }
            
            # ç²å–è¨˜æ†¶å…§å®¹é€²è¡Œåˆ†æ
            recent_entries = self.get_recent_memory(project_id, max_entries) if hasattr(self, 'get_recent_memory') else []
            
            if summary_type == 'brief':
                return self._generate_brief_summary(project_id, stats, recent_entries[:5])
            elif summary_type == 'detailed':
                return self._generate_detailed_summary(project_id, stats, recent_entries[:15])
            elif summary_type == 'timeline':
                return self._generate_timeline_summary(project_id, stats, recent_entries)
            else:
                return self._generate_brief_summary(project_id, stats, recent_entries[:5])
                
        except Exception as e:
            logger.error(f"Error generating project summary: {e}")
            return {
                'status': 'error',
                'message': f'ç”Ÿæˆå°ˆæ¡ˆæ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}'
            }
    
    def _generate_brief_summary(self, project_id: str, stats: Dict, entries: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆç°¡è¦æ‘˜è¦ (200-300 tokens)"""
        
        # åˆ†æåˆ†é¡å’Œä¸»é¡Œ
        categories = stats.get('categories', [])
        entry_count = stats.get('total_entries', 0)
        
        # æå–é—œéµä¸»é¡Œ
        key_topics = []
        recent_activities = []
        
        for entry in entries[:3]:
            if entry.get('title'):
                key_topics.append(entry['title'])
            if entry.get('timestamp'):
                recent_activities.append(entry['timestamp'][:10])  # æ—¥æœŸéƒ¨åˆ†
        
        summary = f"**{project_id}** æ˜¯ä¸€å€‹åŒ…å« {entry_count} æ¢è¨˜æ†¶çš„å°ˆæ¡ˆã€‚"
        
        if categories:
            summary += f"\n\n**ä¸»è¦åˆ†é¡**: {', '.join(categories[:5])}"
        
        if key_topics:
            summary += f"\n**è¿‘æœŸä¸»é¡Œ**: {', '.join(key_topics)}"
        
        if recent_activities:
            summary += f"\n**æ´»å‹•æ™‚é–“**: æœ€è¿‘æ›´æ–°æ–¼ {max(recent_activities)}"
        
        return {
            'status': 'success',
            'type': 'brief',
            'summary': summary,
            'key_metrics': {
                'total_entries': entry_count,
                'categories': len(categories),
                'recent_activity': max(recent_activities) if recent_activities else 'Unknown'
            }
        }
    
    def _generate_detailed_summary(self, project_id: str, stats: Dict, entries: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆè©³ç´°æ‘˜è¦ (800-1000 tokens)"""
        
        categories = stats.get('categories', [])
        entry_count = stats.get('total_entries', 0)
        total_words = stats.get('total_words', 0)
        
        # åˆ†æå…§å®¹çµæ§‹
        category_analysis = {}
        timeline_analysis = {}
        key_topics = []
        
        for entry in entries:
            # åˆ†é¡åˆ†æ
            cat = entry.get('category', 'æœªåˆ†é¡')
            if cat not in category_analysis:
                category_analysis[cat] = 0
            category_analysis[cat] += 1
            
            # æ™‚é–“è»¸åˆ†æ
            if entry.get('timestamp'):
                date = entry['timestamp'][:7]  # YYYY-MM
                if date not in timeline_analysis:
                    timeline_analysis[date] = 0
                timeline_analysis[date] += 1
            
            # é—œéµä¸»é¡Œ
            if entry.get('title'):
                key_topics.append(entry['title'])
        
        summary = f"## {project_id} å°ˆæ¡ˆè©³ç´°åˆ†æ\n\n"
        summary += f"é€™æ˜¯ä¸€å€‹æ´»èºçš„å°ˆæ¡ˆï¼ŒåŒ…å« {entry_count} æ¢è¨˜æ†¶ï¼Œç¸½è¨ˆç´„ {total_words} å€‹è©å½™ã€‚\n\n"
        
        # å…§å®¹çµæ§‹åˆ†æ
        summary += "### ğŸ“Š å…§å®¹çµæ§‹\n"
        if category_analysis:
            summary += "**åˆ†é¡åˆ†ä½ˆ**:\n"
            for cat, count in sorted(category_analysis.items(), key=lambda x: x[1], reverse=True)[:5]:
                percentage = (count / len(entries)) * 100
                summary += f"- {cat}: {count} æ¢ ({percentage:.1f}%)\n"
        
        # æ´»å‹•æ™‚é–“è»¸
        if timeline_analysis:
            summary += "\n### ğŸ“… æ´»å‹•æ™‚é–“è»¸\n"
            summary += "**æœˆä»½æ´»å‹•**:\n"
            for month, count in sorted(timeline_analysis.items(), reverse=True)[:6]:
                summary += f"- {month}: {count} æ¢è¨˜æ†¶\n"
        
        # ä¸»è¦ä¸»é¡Œ
        if key_topics:
            summary += f"\n### ğŸ¯ ä¸»è¦ä¸»é¡Œ\n"
            summary += f"æœ€è¿‘é—œæ³¨çš„ä¸»é¡ŒåŒ…æ‹¬: {', '.join(key_topics[:8])}\n"
        
        return {
            'status': 'success',
            'type': 'detailed',
            'summary': summary,
            'analysis': {
                'category_distribution': category_analysis,
                'timeline': timeline_analysis,
                'key_topics': key_topics[:10]
            }
        }
    
    def _generate_timeline_summary(self, project_id: str, stats: Dict, entries: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ™‚é–“è»¸æ‘˜è¦ (500-700 tokens)"""
        
        # æŒ‰æ™‚é–“æ’åº
        sorted_entries = sorted(entries, key=lambda x: x.get('timestamp', ''), reverse=True)
        
        summary = f"## {project_id} å°ˆæ¡ˆç™¼å±•æ™‚é–“è»¸\n\n"
        
        # æŒ‰æœˆä»½åˆ†çµ„
        monthly_groups = {}
        for entry in sorted_entries[:15]:
            if entry.get('timestamp'):
                month = entry['timestamp'][:7]  # YYYY-MM
                if month not in monthly_groups:
                    monthly_groups[month] = []
                monthly_groups[month].append(entry)
        
        # ç”Ÿæˆæ™‚é–“è»¸
        for month in sorted(monthly_groups.keys(), reverse=True)[:6]:
            entries_in_month = monthly_groups[month]
            summary += f"### ğŸ“… {month}\n"
            
            for entry in entries_in_month[:3]:  # æ¯æœˆæœ€å¤š3æ¢
                title = entry.get('title', 'ç„¡æ¨™é¡Œ')
                category = entry.get('category', '')
                day = entry.get('timestamp', '')[:10] if entry.get('timestamp') else ''
                
                summary += f"- **{day}** - {title}"
                if category:
                    summary += f" #{category}"
                summary += "\n"
            
            if len(entries_in_month) > 3:
                summary += f"  ... é‚„æœ‰ {len(entries_in_month) - 3} æ¢è¨˜æ†¶\n"
            summary += "\n"
        
        return {
            'status': 'success',
            'type': 'timeline',
            'summary': summary,
            'timeline_data': monthly_groups
        }
    
    @abstractmethod
    def list_projects(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆåŠå…¶çµ±è¨ˆè³‡è¨Š"""
        pass
    
    @abstractmethod
    def get_recent_memory(self, project_id: str, limit: int = 5) -> List[Dict[str, str]]:
        """å–å¾—æœ€è¿‘çš„è¨˜æ†¶æ¢ç›®"""
        pass
    
    @abstractmethod
    def delete_memory(self, project_id: str) -> bool:
        """åˆªé™¤å°ˆæ¡ˆè¨˜æ†¶æª”æ¡ˆ"""
        pass
    
    @abstractmethod
    def delete_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None, 
                           title: str = None, category: str = None, content_match: str = None) -> Dict[str, Any]:
        """åˆªé™¤ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        pass
    
    @abstractmethod
    def edit_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None,
                         new_title: str = None, new_category: str = None, new_content: str = None) -> Dict[str, Any]:
        """ç·¨è¼¯ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        pass
    
    @abstractmethod
    def list_memory_entries(self, project_id: str) -> Dict[str, Any]:
        """åˆ—å‡ºå°ˆæ¡ˆä¸­çš„æ‰€æœ‰è¨˜æ†¶æ¢ç›®ï¼Œå¸¶æœ‰ç´¢å¼•"""
        pass
    
    @abstractmethod
    def get_memory_stats(self, project_id: str) -> Dict[str, Any]:
        """å–å¾—è¨˜æ†¶çµ±è¨ˆè³‡è¨Š"""
        pass

    @abstractmethod
    def rename_project(self, old_project_id: str, new_project_id: str) -> bool:
        """é‡æ–°å‘½åå°ˆæ¡ˆ"""
        raise NotImplementedError("Subclasses must implement rename_project method")



class MarkdownMemoryManager(MemoryBackend):
    """
    Markdown è¨˜æ†¶ç®¡ç†å™¨
    Markdown Memory Manager
    
    ä½¿ç”¨ Markdown æª”æ¡ˆæ ¼å¼å„²å­˜å’Œç®¡ç† AI è¨˜æ†¶ï¼Œæ”¯æ´æª”æ¡ˆé–å®šå’ŒåŸå­å¯«å…¥
    Stores and manages AI memory using Markdown file format with file locking and atomic writes
    """
    
    def __init__(self, memory_dir: str = "ai-memory"):
        # ç¸½æ˜¯ä½¿ç”¨è…³æœ¬æ‰€åœ¨ç›®éŒ„ä½œç‚ºåŸºæº–ï¼Œç¢ºä¿è·¯å¾‘ç©©å®šæ€§
        script_dir = Path(__file__).parent.resolve()  # ä½¿ç”¨ resolve() ç²å–çµ•å°è·¯å¾‘
        
        if Path(memory_dir).is_absolute():
            # å¦‚æœæä¾›çµ•å°è·¯å¾‘ï¼Œç›´æ¥ä½¿ç”¨
            self.memory_dir = Path(memory_dir)
        else:
            # ç›¸å°è·¯å¾‘ç¸½æ˜¯ç›¸å°æ–¼è…³æœ¬ç›®éŒ„
            self.memory_dir = script_dir / memory_dir
        
        # ç¢ºä¿ä½¿ç”¨çµ•å°è·¯å¾‘
        self.memory_dir = self.memory_dir.resolve()
        
        # è¨˜éŒ„è·¯å¾‘ä¿¡æ¯ç”¨æ–¼èª¿è©¦
        logger.info(f"Script directory: {script_dir}")
        logger.info(f"Target memory directory: {self.memory_dir}")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        try:
            self.memory_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Memory directory successfully initialized: {self.memory_dir}")
            
            # é©—è­‰ç›®éŒ„æ˜¯å¦å¯å¯«
            test_file = self.memory_dir / ".write_test"
            try:
                test_file.write_text("test")
                test_file.unlink()  # åˆªé™¤æ¸¬è©¦æ–‡ä»¶
                logger.info("Memory directory write test passed")
            except Exception as write_error:
                logger.error(f"Memory directory is not writable: {write_error}")
                raise OSError(f"Memory directory not writable: {self.memory_dir}")
                
        except OSError as e:
            logger.error(f"Failed to create memory directory at {self.memory_dir}: {e}")
            # å¦‚æœç„¡æ³•å‰µå»ºï¼Œå˜—è©¦åœ¨ç”¨æˆ¶ä¸»ç›®éŒ„å‰µå»º
            import tempfile
            fallback_dir = Path.home() / ".ai-memory"
            try:
                fallback_dir.mkdir(parents=True, exist_ok=True)
                self.memory_dir = fallback_dir
                logger.warning(f"Using fallback directory in user home: {self.memory_dir}")
            except OSError:
                # æœ€å¾Œçš„å‚™é¸æ–¹æ¡ˆï¼šä½¿ç”¨è‡¨æ™‚ç›®éŒ„
                self.memory_dir = Path(tempfile.mkdtemp(prefix="ai-memory-"))
                logger.warning(f"Using temporary directory: {self.memory_dir}")

    def get_memory_file(self, project_id: str) -> Path:
        """å–å¾—å°ˆæ¡ˆè¨˜æ†¶æª”æ¡ˆè·¯å¾‘"""
        try:
            # ä½¿ç”¨å®‰å…¨çš„å°ˆæ¡ˆIDæ¸…ç†
            clean_id = PathSafetyUtils.sanitize_project_id_for_path(project_id)
            
            # é©—è­‰æª”æ¡ˆåç¨±å®‰å…¨æ€§
            filename = PathSafetyUtils.validate_filename(f"{clean_id}.md")
            
            # å»ºç«‹å®Œæ•´è·¯å¾‘
            file_path = self.memory_dir / filename
            
            # é©—è­‰è·¯å¾‘å®‰å…¨æ€§
            safe_path = PathSafetyUtils.validate_safe_path(
                file_path, 
                self.memory_dir, 
                f"memory file access for project '{project_id}'"
            )
            
            return safe_path
            
        except (ValidationError, SecurityError) as e:
            logger.error(f"Path validation failed for project '{project_id}': {e}")
            raise

    def get_memory(self, project_id: str) -> Optional[str]:
        """è®€å–å®Œæ•´å°ˆæ¡ˆè¨˜æ†¶"""
        logger.info(f"[MARKDOWN] Calling get_memory for project: {project_id}")
        try:
            memory_file = self.get_memory_file(project_id)
            if memory_file.exists():
                return memory_file.read_text(encoding='utf-8')
            return None
        except Exception as e:
            logger.error(f"Error reading memory for {project_id}: {e}")
            return None

    def save_memory(self, project_id: str, content: str, title: str = "", category: str = "") -> bool:
        """å„²å­˜è¨˜æ†¶åˆ° markdown æª”æ¡ˆï¼ˆä½¿ç”¨æª”æ¡ˆé–å®šï¼‰"""
        logger.info(f"[MARKDOWN] Calling save_memory for project: {project_id}")
        try:
            memory_file = self.get_memory_file(project_id)
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # å»ºç«‹è¨˜æ†¶æ¢ç›®
            entry_parts = [f"\n## {timestamp}"]
            if title:
                entry_parts.append(f" - {title}")
            if category:
                entry_parts.append(f" #{category}")
            
            entry = "".join(entry_parts) + f"\n\n{content}\n\n---\n"
            
            # ä½¿ç”¨æª”æ¡ˆé–å®šé€²è¡Œå®‰å…¨å¯«å…¥
            with FileLock(memory_file):
                if memory_file.exists():
                    # è¿½åŠ åˆ°ç¾æœ‰æª”æ¡ˆ
                    with open(memory_file, 'a', encoding='utf-8') as f:
                        f.write(entry)
                else:
                    # å‰µå»ºæ–°æª”æ¡ˆï¼ˆä½¿ç”¨åŸå­å¯«å…¥ï¼‰
                    header = f"# AI Memory for {project_id}\n\n"
                    header += f"Created: {timestamp}\n\n"
                    with AtomicFileWriter(memory_file) as f:
                        f.write(header + entry)
            
            logger.info(f"Memory saved for project: {project_id}")
            return True
            
        except TimeoutError as e:
            logger.error(f"Timeout acquiring lock for {project_id}: {e}")
            return False
        except (ValidationError, SecurityError) as e:
            logger.error(f"Validation error saving memory for {project_id}: {e}")
            return False
        except (OSError, IOError) as e:
            logger.error(f"I/O error saving memory for {project_id}: {e}")
            raise FileOperationError(f"æª”æ¡ˆæ“ä½œéŒ¯èª¤: {e}")
        except PermissionError as e:
            logger.error(f"Permission denied saving memory for {project_id}: {e}")
            raise FileOperationError(f"æª”æ¡ˆæ¬Šé™éŒ¯èª¤: {e}")
        except Exception as e:
            logger.error(f"Unexpected error saving memory for {project_id}: {e}")
            return False

    def search_memory(self, project_id: str, query: str, limit: int = 10) -> List[Dict[str, str]]:
        """æœå°‹è¨˜æ†¶å…§å®¹"""
        logger.info(f"[MARKDOWN] Calling search_memory for project: {project_id}, query: {query}")
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return []

            results = []
            sections = self._parse_memory_sections(memory)
            
            for section in sections:
                if query.lower() in section['content'].lower():
                    results.append({
                        'timestamp': section['timestamp'],
                        'title': section['title'],
                        'category': section['category'],
                        'content': section['content'][:500] + "..." if len(section['content']) > 500 else section['content'],
                        'relevance': section['content'].lower().count(query.lower())
                    })
            
            # æŒ‰ç›¸é—œæ€§æ’åº
            results.sort(key=lambda x: x['relevance'], reverse=True)
            return results[:limit]
            
        except Exception as e:
            logger.error(f"Error searching memory for {project_id}: {e}")
            return []

    def _parse_memory_sections(self, memory: str) -> List[Dict[str, str]]:
        """è§£æè¨˜æ†¶æª”æ¡ˆçš„å„å€‹å€æ®µ"""
        sections = []
        lines = memory.split('\n')
        current_section = None
        current_content = []

        for line in lines:
            if line.startswith('## '):
                # å„²å­˜ä¸Šä¸€å€‹å€æ®µ
                if current_section:
                    sections.append({
                        'timestamp': current_section['timestamp'],
                        'title': current_section['title'],
                        'category': current_section['category'],
                        'content': '\n'.join(current_content).strip()
                    })
                
                # è§£ææ–°å€æ®µæ¨™é¡Œ
                header = line[3:].strip()
                timestamp, title, category = self._parse_section_header(header)
                current_section = {
                    'timestamp': timestamp,
                    'title': title,
                    'category': category
                }
                current_content = []
                
            elif line.strip() == '---':
                # å€æ®µçµæŸ
                continue
            else:
                current_content.append(line)

        # è™•ç†æœ€å¾Œä¸€å€‹å€æ®µ
        if current_section:
            sections.append({
                'timestamp': current_section['timestamp'],
                'title': current_section['title'],
                'category': current_section['category'],
                'content': '\n'.join(current_content).strip()
            })

        return sections

    def _parse_section_header(self, header: str) -> Tuple[str, str, str]:
        """è§£æå€æ®µæ¨™é¡Œï¼Œæå–æ™‚é–“æˆ³ã€æ¨™é¡Œå’Œåˆ†é¡"""
        timestamp = ""
        title = ""
        category = ""
        
        # æå–åˆ†é¡ (hashtag)
        if '#' in header:
            header, category = header.split('#', 1)
            category = category.strip()
        
        # æå–æ™‚é–“æˆ³å’Œæ¨™é¡Œ
        if ' - ' in header:
            timestamp, title = header.split(' - ', 1)
            timestamp = timestamp.strip()
            title = title.strip()
        else:
            timestamp = header.strip()
        
        return timestamp, title, category

    def list_projects(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆåŠå…¶çµ±è¨ˆè³‡è¨Šï¼ˆæ’é™¤å…¨å±€è¨˜æ†¶ï¼‰"""
        logger.info(f"[MARKDOWN] Calling list_projects")
        projects = []
        for file in self.memory_dir.glob("*.md"):
            # éæ¿¾æ‰å…¨å±€è¨˜æ†¶æª”æ¡ˆ
            if file.stem == "__global__":
                continue
                
            try:
                content = file.read_text(encoding='utf-8')
                sections = self._parse_memory_sections(content)
                projects.append({
                    'id': file.stem,
                    'name': file.stem.replace('-', ' ').title(),
                    'file_path': str(file),
                    'entries_count': len(sections),
                    'last_modified': datetime.fromtimestamp(file.stat().st_mtime).strftime("%Y-%m-%d %H:%M:%S"),
                    'categories': list(set(s['category'] for s in sections if s['category']))
                })
            except Exception as e:
                logger.error(f"Error reading project {file.stem}: {e}")
                continue
        
        return sorted(projects, key=lambda x: x['last_modified'], reverse=True)

    def get_recent_memory(self, project_id: str, limit: int = 5) -> List[Dict[str, str]]:
        """å–å¾—æœ€è¿‘çš„è¨˜æ†¶æ¢ç›®"""
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return []

            sections = self._parse_memory_sections(memory)
            
            # è¿”å›æœ€è¿‘çš„æ¢ç›®
            return sections[-limit:] if len(sections) > limit else sections
            
        except Exception as e:
            logger.error(f"Error getting recent memory for {project_id}: {e}")
            return []

    def delete_memory(self, project_id: str) -> bool:
        """åˆªé™¤å°ˆæ¡ˆè¨˜æ†¶æª”æ¡ˆï¼ˆä½¿ç”¨æª”æ¡ˆé–å®šï¼‰"""
        logger.info(f"[MARKDOWN] Calling delete_memory for project: {project_id}")
        try:
            memory_file = self.get_memory_file(project_id)
            
            # ä½¿ç”¨æª”æ¡ˆé–å®šé€²è¡Œå®‰å…¨åˆªé™¤
            with FileLock(memory_file):
                if memory_file.exists():
                    memory_file.unlink()
                    logger.info(f"Memory deleted for project: {project_id}")
                    return True
                return False
                
        except TimeoutError as e:
            logger.error(f"Timeout acquiring lock for deleting {project_id}: {e}")
            return False
        except Exception as e:
            logger.error(f"Error deleting memory for {project_id}: {e}")
            return False

    def delete_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None, 
                           title: str = None, category: str = None, content_match: str = None) -> Dict[str, Any]:
        """åˆªé™¤ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        logger.info(f"[MARKDOWN] Calling delete_memory_entry for project: {project_id}")
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return {'success': False, 'message': f'No memory found for project: {project_id}'}

            sections = self._parse_memory_sections(memory)
            original_count = len(sections)
            
            # æ ¹æ“šä¸åŒæ¢ä»¶ç¯©é¸è¦åˆªé™¤çš„æ¢ç›®
            sections_to_keep = []
            deleted_entries = []
            
            for i, section in enumerate(sections):
                should_delete = False
                
                # æ ¹æ“šç´¢å¼•åˆªé™¤ (entry_id æ˜¯å¾1é–‹å§‹çš„ç´¢å¼•)
                if entry_id is not None:
                    try:
                        entry_index = int(entry_id) - 1  # è½‰æ›ç‚º0åŸºç´¢å¼•
                        if i == entry_index:
                            should_delete = True
                    except ValueError:
                        pass
                
                # æ ¹æ“šæ™‚é–“æˆ³åˆªé™¤
                elif timestamp and timestamp in section['timestamp']:
                    should_delete = True
                
                # æ ¹æ“šæ¨™é¡Œåˆªé™¤
                elif title and title.lower() in section['title'].lower():
                    should_delete = True
                
                # æ ¹æ“šåˆ†é¡åˆªé™¤
                elif category and category.lower() in section['category'].lower():
                    should_delete = True
                
                # æ ¹æ“šå…§å®¹åŒ¹é…åˆªé™¤
                elif content_match and content_match.lower() in section['content'].lower():
                    should_delete = True
                
                if should_delete:
                    deleted_entries.append(section)
                else:
                    sections_to_keep.append(section)
            
            if len(deleted_entries) == 0:
                return {'success': False, 'message': 'No matching entries found to delete'}
            
            # é‡å»ºè¨˜æ†¶æª”æ¡ˆ
            success = self._rebuild_memory_file(project_id, sections_to_keep)
            
            if success:
                message = f"Deleted {len(deleted_entries)} entries from project {project_id}"
                return {
                    'success': True, 
                    'message': message,
                    'deleted_count': len(deleted_entries),
                    'remaining_count': len(sections_to_keep),
                    'deleted_entries': [{'timestamp': e['timestamp'], 'title': e['title']} for e in deleted_entries]
                }
            else:
                return {'success': False, 'message': 'Failed to update memory file'}
                
        except Exception as e:
            logger.error(f"Error deleting memory entry for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}

    def edit_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None,
                         new_title: str = None, new_category: str = None, new_content: str = None) -> Dict[str, Any]:
        """ç·¨è¼¯ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        logger.info(f"[MARKDOWN] Calling edit_memory_entry for project: {project_id}")
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return {'success': False, 'message': f'No memory found for project: {project_id}'}

            sections = self._parse_memory_sections(memory)
            
            # æ‰¾åˆ°è¦ç·¨è¼¯çš„æ¢ç›®
            target_section = None
            target_index = -1
            
            for i, section in enumerate(sections):
                # æ ¹æ“šç´¢å¼•æŸ¥æ‰¾
                if entry_id is not None:
                    try:
                        entry_index = int(entry_id) - 1
                        if i == entry_index:
                            target_section = section
                            target_index = i
                            break
                    except ValueError:
                        pass
                
                # æ ¹æ“šæ™‚é–“æˆ³æŸ¥æ‰¾
                elif timestamp and timestamp in section['timestamp']:
                    target_section = section
                    target_index = i
                    break
            
            if target_section is None:
                return {'success': False, 'message': 'No matching entry found to edit'}
            
            # æ›´æ–°æ¢ç›®å…§å®¹
            if new_title is not None:
                sections[target_index]['title'] = new_title
            if new_category is not None:
                sections[target_index]['category'] = new_category
            if new_content is not None:
                sections[target_index]['content'] = new_content
            
            # é‡å»ºè¨˜æ†¶æª”æ¡ˆ
            success = self._rebuild_memory_file(project_id, sections)
            
            if success:
                return {
                    'success': True,
                    'message': f"Successfully edited entry in project {project_id}",
                    'edited_entry': {
                        'timestamp': sections[target_index]['timestamp'],
                        'title': sections[target_index]['title'],
                        'category': sections[target_index]['category']
                    }
                }
            else:
                return {'success': False, 'message': 'Failed to update memory file'}
                
        except Exception as e:
            logger.error(f"Error editing memory entry for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}

    def list_memory_entries(self, project_id: str) -> Dict[str, Any]:
        """åˆ—å‡ºå°ˆæ¡ˆä¸­çš„æ‰€æœ‰è¨˜æ†¶æ¢ç›®ï¼Œå¸¶æœ‰ç´¢å¼•"""
        logger.info(f"[MARKDOWN] Calling list_memory_entries for project: {project_id}")
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return {'success': False, 'message': f'No memory found for project: {project_id}'}

            sections = self._parse_memory_sections(memory)
            
            entries = []
            for i, section in enumerate(sections):
                entries.append({
                    'id': i + 1,  # 1-based index for user convenience
                    'timestamp': section['timestamp'],
                    'title': section['title'],
                    'category': section['category'],
                    'content_preview': section['content'][:100] + "..." if len(section['content']) > 100 else section['content']
                })
            
            return {
                'success': True,
                'total_entries': len(entries),
                'entries': entries
            }
            
        except Exception as e:
            logger.error(f"Error listing memory entries for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}

    def _rebuild_memory_file(self, project_id: str, sections: List[Dict[str, str]]) -> bool:
        """é‡å»ºè¨˜æ†¶æª”æ¡ˆï¼ˆä½¿ç”¨æª”æ¡ˆé–å®šï¼‰"""
        try:
            memory_file = self.get_memory_file(project_id)
            
            # ä½¿ç”¨æª”æ¡ˆé–å®šé€²è¡Œå®‰å…¨é‡å»º
            with FileLock(memory_file):
                if not sections:
                    # å¦‚æœæ²’æœ‰æ¢ç›®ï¼Œåˆªé™¤æª”æ¡ˆ
                    if memory_file.exists():
                        memory_file.unlink()
                    return True
                
                # é‡å»ºæª”æ¡ˆå…§å®¹
                content_parts = [f"# AI Memory for {project_id}\n\n"]
                content_parts.append(f"Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                for section in sections:
                    # é‡å»ºæ¢ç›®
                    entry_parts = [f"## {section['timestamp']}"]
                    if section['title']:
                        entry_parts.append(f" - {section['title']}")
                    if section['category']:
                        entry_parts.append(f" #{section['category']}")
                    
                    entry = "".join(entry_parts) + f"\n\n{section['content']}\n\n---\n\n"
                    content_parts.append(entry)
                
                # ä½¿ç”¨åŸå­å¯«å…¥
                with AtomicFileWriter(memory_file) as f:
                    f.write("".join(content_parts))
            
            logger.info(f"Memory file rebuilt for project: {project_id}")
            return True
            
        except TimeoutError as e:
            logger.error(f"Timeout acquiring lock for rebuilding {project_id}: {e}")
            return False
        except Exception as e:
            logger.error(f"Error rebuilding memory file for {project_id}: {e}")
            return False

    def get_memory_stats(self, project_id: str) -> Dict[str, Any]:
        """å–å¾—è¨˜æ†¶çµ±è¨ˆè³‡è¨Š"""
        logger.info(f"[MARKDOWN] Calling get_memory_stats for project: {project_id}")
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return {'exists': False}

            sections = self._parse_memory_sections(memory)
            categories = [s['category'] for s in sections if s['category']]
            
            return {
                'exists': True,
                'total_entries': len(sections),
                'total_words': len(memory.split()),
                'total_characters': len(memory),
                'categories': list(set(categories)),
                'latest_entry': sections[-1]['timestamp'] if sections else None,
                'oldest_entry': sections[0]['timestamp'] if sections else None
            }
            
        except Exception as e:
            logger.error(f"Error getting memory stats for {project_id}: {e}")
            return {'exists': False, 'error': str(e)}

    def rename_project(self, project_id: str, new_name: str) -> bool:
        """é‡æ–°å‘½åå°ˆæ¡ˆï¼ˆæ›´æ–°æª”æ¡ˆä¸­çš„å°ˆæ¡ˆåç¨±ï¼‰"""
        logger.info(f"[MARKDOWN] Calling rename_project for project: {project_id} to: {new_name}")
        try:
            file_path = self.get_memory_file(project_id)
            
            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if not file_path.exists():
                logger.error(f"Project {project_id} does not exist")
                return False
            
            # ä½¿ç”¨æª”æ¡ˆé–å®šé€²è¡Œå®‰å…¨æ›´æ–°
            with FileLock(file_path):
                # è®€å–åŸå§‹å…§å®¹
                content = file_path.read_text(encoding='utf-8')
                
                # æ›´æ–°æª”æ¡ˆæ¨™é¡Œ
                lines = content.split('\n')
                if lines and lines[0].startswith('# AI Memory for '):
                    lines[0] = f"# AI Memory for {new_name}"
                    
                    updated_content = '\n'.join(lines)
                    
                    # å¯«å›æª”æ¡ˆ
                    with AtomicFileWriter(file_path) as f:
                        f.write(updated_content)
                
                logger.info(f"Project {project_id} renamed to '{new_name}'")
                return True
                
        except TimeoutError as e:
            logger.error(f"Timeout acquiring lock for renaming {project_id}: {e}")
            return False
        except Exception as e:
            logger.error(f"Error renaming project {project_id} to '{new_name}': {e}")
            return False

class SQLiteBackend(MemoryBackend):
    """
    SQLite è¨˜æ†¶å¾Œç«¯
    SQLite Memory Backend
    
    ä½¿ç”¨ SQLite è³‡æ–™åº«å„²å­˜å’Œç®¡ç† AI è¨˜æ†¶ï¼Œæ”¯æ´å…¨æ–‡æœå°‹å’Œé«˜æ•ˆèƒ½æŸ¥è©¢
    Stores and manages AI memory using SQLite database with full-text search and high-performance queries
    
    Features / åŠŸèƒ½:
    - FTS5 å…¨æ–‡æœå°‹ / FTS5 full-text search
    - äº‹å‹™å®‰å…¨ / Transaction safety  
    - ç´¢å¼•å„ªåŒ– / Index optimization
    - è‡ªå‹•å‚™ä»½ / Automatic backup
    """
    
    def __init__(self, db_path: str):
        if not db_path or not str(db_path).strip():
            raise ValueError("SQLiteBackend requires an explicit db_path. Use --db-path to specify the database file path.")
        # å±•é–‹å®¶ç›®éŒ„ä¸¦è§£æç‚ºçµ•å°è·¯å¾‘
        resolved_path = Path(os.path.expanduser(str(db_path))).resolve()
        self.db_path = resolved_path
        # æº–å‚™ç›®éŒ„ä¸¦é©—è­‰å¯å¯«
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        # é©—è­‰ç›®éŒ„å¯å¯«
        try:
            test_file = self.db_path.parent / ".write_test"
            with open(test_file, 'w', encoding='utf-8') as f:
                f.write("test")
            test_file.unlink()
        except Exception as e:
            raise OSError(
                f"Database directory not writable: {self.db_path.parent}. "
                f"Please run with --db-path pointing to a writable location. Error: {e}"
            )
        # å•Ÿå‹•æ™‚æª¢æ¸¬å¸¸è¦‹èˆŠè·¯å¾‘ï¼Œä½†ä¸æ¡ç”¨ã€ä¸é·ç§»ï¼Œåªè­¦å‘Š
        try:
            candidates = [Path("ai-memory/memory.db"), Path("memory.db")]  # ç›¸å°æ–¼ç•¶å‰å·¥ä½œç›®éŒ„
            for cand in candidates:
                try:
                    if cand.exists():
                        cand_abs = cand.resolve()
                        if cand_abs != self.db_path:
                            logger.warning(
                                f"Detected legacy database at '{cand_abs}', but current --db-path is '{self.db_path}'. "
                                f"The legacy DB will NOT be used. If you need it, pass --db-path explicitly.")
                except Exception:
                    continue
        except Exception:
            pass
        # åˆå§‹åŒ–è³‡æ–™åº«
        self._init_database()
    
    @contextmanager
    def get_connection(self):
        """å–å¾—è³‡æ–™åº«é€£æ¥"""
        conn = sqlite3.connect(
            self.db_path,
            timeout=30.0,
            check_same_thread=False
        )
        # ç¢ºä¿ä½¿ç”¨ UTF-8 ç·¨ç¢¼è™•ç†æ–‡æœ¬
        conn.text_factory = str
        conn.row_factory = sqlite3.Row
        # è¨­ç½® UTF-8 ç·¨ç¢¼
        conn.execute("PRAGMA encoding = 'UTF-8'")
        self._configure_connection(conn)
        try:
            yield conn
        finally:
            conn.close()
    
    def _configure_connection(self, conn):
        """é…ç½®é€£æ¥æœ€ä½³åŒ–åƒæ•¸"""
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")
        conn.execute("PRAGMA cache_size=10000")
        conn.execute("PRAGMA temp_store=MEMORY")
        conn.execute("PRAGMA foreign_keys=ON")
    
    def _init_database(self):
        """åˆå§‹åŒ–è³‡æ–™åº«çµæ§‹"""
        with self.get_connection() as conn:
            # å»ºç«‹æœ€çµ‚ç‰ˆè¨˜æ†¶æ¢ç›®è¡¨ï¼ˆæ–°çµæ§‹ï¼Œç„¡ _new å¾Œç¶´ï¼‰
            conn.execute("""
                CREATE TABLE IF NOT EXISTS memory_entries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    project TEXT NOT NULL,
                    category TEXT,
                    entry_type TEXT NOT NULL DEFAULT 'note',
                    title TEXT NOT NULL,
                    summary TEXT,
                    entry TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # å»ºç«‹ç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_project ON memory_entries(project)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_category ON memory_entries(category)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_entry_type ON memory_entries(entry_type)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_created ON memory_entries(created_at)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_updated ON memory_entries(updated_at)")

            # å»ºç«‹ FTS5 å…¨æ–‡æœå°‹ï¼ˆå°æ‡‰æœ€çµ‚ç‰ˆè¡¨ï¼‰
            conn.execute("""
                CREATE VIRTUAL TABLE IF NOT EXISTS memory_fts USING fts5(
                    title, summary, entry,
                    content='memory_entries',
                    content_rowid='id',
                    tokenize='trigram'
                )
            """)

            # å»ºç«‹ FTS5 è§¸ç™¼å™¨ï¼ˆå°æ‡‰æœ€çµ‚ç‰ˆè¡¨ï¼‰
            conn.execute("""
                CREATE TRIGGER IF NOT EXISTS memory_fts_insert AFTER INSERT ON memory_entries BEGIN
                    INSERT INTO memory_fts(rowid, title, summary, entry) 
                    VALUES (new.id, new.title, new.summary, new.entry);
                END
            """)

            conn.execute("""
                CREATE TRIGGER IF NOT EXISTS memory_fts_delete AFTER DELETE ON memory_entries BEGIN
                    INSERT INTO memory_fts(memory_fts, rowid, title, summary, entry)
                    VALUES('delete', old.id, old.title, old.summary, old.entry);
                END
            """)

            conn.execute("""
                CREATE TRIGGER IF NOT EXISTS memory_fts_update AFTER UPDATE ON memory_entries BEGIN
                    INSERT INTO memory_fts(memory_fts, rowid, title, summary, entry)
                    VALUES('delete', old.id, old.title, old.summary, old.entry);
                    INSERT INTO memory_fts(rowid, title, summary, entry)
                    VALUES (new.id, new.title, new.summary, new.entry);
                END
            """)

            # tree_entries è¡¨å·²ç§»é™¤ï¼Œç›´æ¥ä½¿ç”¨ memory_entries è¡¨
            
            
            # è³‡æ–™é·ç§»å·²å®Œæˆï¼Œä¸å†éœ€è¦ v2 è¡¨æ ¼
            
            conn.commit()
            logger.info(f"SQLite database initialized at: {self.db_path}")
    
    
    
    @log_performance("sqlite_save_memory")
    def save_memory(self, project_id: str, content: str, title: str = "", category: str = "") -> bool:
        """å„²å­˜è¨˜æ†¶åˆ°æ–°çš„ç°¡æ½”è¡¨çµæ§‹"""
        context = {
            'project_id': project_id,
            'content_length': len(content),
            'has_title': bool(title),
            'has_category': bool(category)
        }
        logger.info("Starting save memory operation", context)
        
        try:
            # è¼¸å…¥é©—è­‰
            project_id = InputValidator.validate_project_id(project_id)
            content = InputValidator.validate_content(content)
            title = InputValidator.validate_title(title)
            category = InputValidator.validate_category(category)
            
            # ä½¿ç”¨æœ€çµ‚è¡¨çš„ add_memory æ–¹æ³•
            entry_id = self.add_memory(
                project=project_id,
                title=title or "æœªå‘½åæ¢ç›®",
                entry=content,
                category=category,
                entry_type="note",
                summary=None  # è®“ AI è‡ªå·±ç†è§£å…§å®¹ï¼Œä¸é å…ˆç”Ÿæˆæ‘˜è¦
            )
            
            logger.operation("save_memory", "success", {
                'project_id': project_id,
                'entry_id': entry_id,
                'content_length': len(content)
            })
            return True
            
        except (ValidationError, SecurityError) as e:
            logger.operation("save_memory", "validation_error", {
                'project_id': project_id,
                'error': str(e)
            })
            return False
        except Exception as e:
            logger.operation("save_memory", "error", {
                'project_id': project_id,
                'error_type': type(e).__name__,
                'error': str(e)
            })
            return False
    
    def get_memory(self, project_id: str) -> Optional[str]:
        """è®€å–å®Œæ•´å°ˆæ¡ˆè¨˜æ†¶ï¼Œè½‰æ›ç‚º Markdown æ ¼å¼ï¼ˆä½¿ç”¨æ–°è¡¨ï¼‰"""
        logger.info(f"[SQLITE] Calling get_memory for project: {project_id}")
        try:
            # ä½¿ç”¨æœ€çµ‚è¡¨çš„ list_memories æ–¹æ³•å–å¾—æ‰€æœ‰æ¢ç›®
            entries = self.list_memories(project=project_id, limit=1000)
            
            if not entries:
                return None
                
            # è½‰æ›ç‚º Markdown æ ¼å¼
            markdown_parts = [f"# AI Memory for {project_id}\n\n"]
            
            for entry in entries:
                # æ ¼å¼åŒ–æ™‚é–“æˆ³
                timestamp = entry['created_at']
                if isinstance(timestamp, str):
                    # SQLite è¿”å›çš„å¯èƒ½æ˜¯å­—ä¸²æ ¼å¼
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        formatted_time = timestamp
                else:
                    formatted_time = timestamp
                
                # å»ºç«‹æ¢ç›®æ¨™é¡Œ
                header_parts = [f"## {formatted_time}"]
                if entry['title']:
                    header_parts.append(f" - {entry['title']}")
                if entry['category']:
                    header_parts.append(f" #{entry['category']}")
                
                # çµ„åˆæ¢ç›®
                entry_md = "".join(header_parts) + f"\n\n{entry['entry']}\n\n---\n\n"
                markdown_parts.append(entry_md)
            
            return "".join(markdown_parts)
                
        except sqlite3.Error as e:
            logger.error(f"Database error reading memory for {project_id}: {e}")
            raise DatabaseError(f"è³‡æ–™åº«è®€å–éŒ¯èª¤: {e}")
        except (ValidationError, SecurityError):
            raise  # é‡æ–°æ‹‹å‡ºé©—è­‰éŒ¯èª¤
        except Exception as e:
            logger.error(f"Unexpected error reading memory for {project_id}: {e}")
            return None
    
    @log_performance("sqlite_search_memory")
    def search_memory(self, project_id: str, query: str, limit: int = 10) -> List[Dict[str, str]]:
        """æ™ºèƒ½æœå°‹ï¼šè‡ªå‹•é¸æ“‡æœ€çœtokençš„ç­–ç•¥"""
        context = {
            'project_id': project_id,
            'query_length': len(query),
            'limit': limit
        }
        logger.info("Starting smart search operation", context)
        
        try:
            # è¼¸å…¥é©—è­‰
            project_id = InputValidator.validate_project_id(project_id)
            query = InputValidator.validate_query(query)
            
            # é™åˆ¶ limit åƒæ•¸ç¯„åœ
            if not isinstance(limit, int) or limit < 1 or limit > 100:
                limit = 10
                logger.warning(f"Invalid limit parameter, using default: {limit}")
            
            # æ™ºèƒ½åˆ¤æ–·æŸ¥è©¢é¡å‹
            query_type = self._analyze_query_type(query)
            logger.info(f"[SQLITE] Query type detected: {query_type}")
            
            if query_type == 'simple_lookup':
                # ç°¡å–®æŸ¥æ‰¾ï¼šä½¿ç”¨search_indexï¼ˆè¶…çœtokenï¼‰
                return self._simple_search(project_id, query, limit)
                
            elif query_type == 'complex_question':
                # è¤‡é›œå•é¡Œï¼šä½¿ç”¨RAGç­–ç•¥ï¼ˆæ™ºèƒ½çœtokenï¼‰
                return self._rag_search(project_id, query, limit)
                
            else:
                # é»˜èªï¼šæ··åˆç­–ç•¥
                return self._hybrid_search(project_id, query, limit)
                
        except (ValidationError, SecurityError) as e:
            logger.error(f"[SQLITE] Validation error in search_memory: {e}")
            return []
        except Exception as e:
            logger.error(f"Error in smart search for {project_id}: {e}")
            # é™ç´šåˆ°åŸå§‹æœç´¢
            return self._fallback_search(project_id, query, limit)
    
    def _fts_search(self, project_id: str, query: str, limit: int) -> List[Dict[str, str]]:
        """ä½¿ç”¨ FTS5 å…¨æ–‡æœå°‹"""
        with self.get_connection() as conn:
            cursor = conn.execute("""
                SELECT me.title, me.category, me.content, me.created_at,
                       rank
                FROM memory_fts 
                JOIN memory_entries me ON memory_fts.rowid = me.id
                WHERE memory_fts MATCH ? AND me.project_id = ?
                ORDER BY rank
                LIMIT ?
            """, (query, project_id, limit))
            
            results = []
            for row in cursor.fetchall():
                results.append(self._format_search_result(row, 1))
            
            return results
    
    def _like_search(self, project_id: str, query: str, limit: int) -> List[Dict[str, str]]:
        """ä½¿ç”¨ LIKE æŸ¥è©¢ä½œç‚ºå‚™ç”¨æœå°‹æ–¹æ¡ˆ"""
        with self.get_connection() as conn:
            # ä½¿ç”¨ LIKE æŸ¥è©¢æœå°‹æ¨™é¡Œã€åˆ†é¡å’Œå…§å®¹
            like_pattern = f"%{query}%"
            cursor = conn.execute("""
                SELECT title, category, content, created_at
                FROM memory_entries 
                WHERE project_id = ? AND (
                    content LIKE ? OR 
                    title LIKE ? OR 
                    category LIKE ?
                )
                ORDER BY created_at DESC
                LIMIT ?
            """, (project_id, like_pattern, like_pattern, like_pattern, limit))
            
            results = []
            for row in cursor.fetchall():
                # è¨ˆç®—ç›¸é—œæ€§ï¼ˆå‡ºç¾æ¬¡æ•¸ï¼‰
                content = row['content'] or ''
                title = row['title'] or ''
                category = row['category'] or ''
                
                relevance = (
                    content.lower().count(query.lower()) +
                    title.lower().count(query.lower()) * 2 +  # æ¨™é¡Œæ¬Šé‡æ›´é«˜
                    category.lower().count(query.lower()) * 1.5  # åˆ†é¡æ¬Šé‡ä¸­ç­‰
                )
                
                results.append(self._format_search_result(row, relevance))
            
            # æŒ‰ç›¸é—œæ€§æ’åº
            results.sort(key=lambda x: x['relevance'], reverse=True)
            return results
    
    def _analyze_query_type(self, query: str) -> str:
        """åˆ†ææŸ¥è©¢é¡å‹ï¼Œæ±ºå®šä½¿ç”¨å“ªç¨®ç­–ç•¥"""
        query_lower = query.lower()
        
        # ç°¡å–®æŸ¥æ‰¾é—œéµè©
        simple_keywords = [
            'åˆ—è¡¨', 'list', 'æœ‰å“ªäº›', 'æ‰¾åˆ°', 'find', 'æœå°‹', 'search',
            'é¡¯ç¤º', 'show', 'æŸ¥çœ‹', 'view', 'æ‰€æœ‰', 'all', 'å°ˆæ¡ˆ', 'project'
        ]
        if any(keyword in query_lower for keyword in simple_keywords):
            return 'simple_lookup'
        
        # è¤‡é›œå•é¡Œé—œéµè©
        complex_keywords = [
            'ç‚ºä»€éº¼', 'why', 'å¦‚ä½•', 'how', 'è§£é‡‹', 'explain', 'åˆ†æ', 'analyze',
            'å¯¦ç¾', 'implement', 'åŸç†', 'principle', 'æµç¨‹', 'process', 'æ­¥é©Ÿ', 'step'
        ]
        if any(keyword in query_lower for keyword in complex_keywords):
            return 'complex_question'
        
        # æ ¹æ“šé•·åº¦åˆ¤æ–·
        if len(query) > 50:
            return 'complex_question'
        elif len(query) < 15:
            return 'simple_lookup'
        
        return 'hybrid'
    
    def _simple_search(self, project_id: str, query: str, limit: int) -> List[Dict[str, str]]:
        """ç°¡å–®æœç´¢ï¼šä½¿ç”¨search_indexï¼Œè¶…çœtoken"""
        logger.info(f"[SQLITE] Using simple search (token-efficient)")
        
        if hasattr(self, 'search_index'):
            index_results = self.search_index(project_id, query, limit)
            
            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
            formatted_results = []
            for result in index_results:
                formatted_results.append({
                    'timestamp': result.get('created_at', ''),
                    'title': result.get('title', ''),
                    'category': result.get('category', ''),
                    'content': result.get('summary', result.get('entry', ''))[:300] + '...',  # é™åˆ¶å…§å®¹é•·åº¦
                    'relevance': result.get('match_type', 'index')
                })
            
            logger.info(f"Simple search returned {len(formatted_results)} results (token-optimized)")
            return formatted_results
        else:
            # é™ç´šåˆ°åŸºæœ¬æœç´¢
            return self._fallback_search(project_id, query, limit)
    
    def _rag_search(self, project_id: str, query: str, limit: int) -> List[Dict[str, str]]:
        """RAGç­–ç•¥ï¼šæ™ºèƒ½é¸æ“‡æœ€ç›¸é—œå…§å®¹ï¼Œæ§åˆ¶tokenä½¿ç”¨"""
        logger.info(f"[SQLITE] Using RAG search (intelligent token control)")
        
        try:
            # ç¬¬ä¸€éšæ®µï¼šè¼•é‡ç´šç¯©é¸
            if hasattr(self, 'search_index'):
                candidates = self.search_index(project_id, query, limit * 3)
            else:
                candidates = self.search_mem_entries(project=project_id, query=query, limit=limit * 2)
            
            # ç¬¬äºŒéšæ®µï¼šæ™ºèƒ½é¸æ“‡æœ€ç›¸é—œçš„å…§å®¹
            selected_results = []
            total_tokens = 0
            max_tokens = 1500  # æ§åˆ¶ç¸½tokenæ•¸
            
            for candidate in candidates[:limit * 2]:  # é™åˆ¶å€™é¸æ•¸é‡
                if total_tokens >= max_tokens or len(selected_results) >= limit:
                    break
                
                # ç²å–å…§å®¹
                if 'entry' in candidate:
                    content = candidate['entry']
                else:
                    # å¦‚æœæ˜¯ç´¢å¼•çµæœï¼Œç²å–å®Œæ•´å…§å®¹
                    try:
                        full_entry = self.get_memory_entry(candidate.get('id'))
                        content = full_entry['entry'] if full_entry else candidate.get('summary', '')
                    except:
                        content = candidate.get('summary', '')
                
                # æ™ºèƒ½æˆªå–ç›¸é—œå…§å®¹
                relevant_content = self._extract_relevant_content(content, query)
                content_tokens = len(relevant_content) // 4  # ç²—ç•¥ä¼°ç®—token
                
                if total_tokens + content_tokens <= max_tokens:
                    selected_results.append({
                        'timestamp': candidate.get('created_at', ''),
                        'title': candidate.get('title', ''),
                        'category': candidate.get('category', ''),
                        'content': relevant_content,
                        'relevance': candidate.get('similarity', 0)
                    })
                    total_tokens += content_tokens
            
            logger.info(f"RAG search returned {len(selected_results)} results (~{total_tokens} tokens)")
            return selected_results
            
        except Exception as e:
            logger.error(f"Error in RAG search: {e}")
            return self._fallback_search(project_id, query, limit)
    
    def _hybrid_search(self, project_id: str, query: str, limit: int) -> List[Dict[str, str]]:
        """æ··åˆç­–ç•¥ï¼šçµåˆç´¢å¼•å’Œå…§å®¹æœç´¢"""
        logger.info(f"[SQLITE] Using hybrid search")
        
        try:
            # å…ˆå˜—è©¦ç´¢å¼•æœç´¢
            index_results = self._simple_search(project_id, query, limit // 2)
            
            # å†è£œå……ä¸€äº›å®Œæ•´å…§å®¹æœç´¢
            content_results = self._fallback_search(project_id, query, limit // 2)
            
            # åˆä½µçµæœï¼Œå»é‡
            all_results = index_results + content_results
            seen_titles = set()
            unique_results = []
            
            for result in all_results:
                title_key = f"{result['title']}_{result['timestamp']}"
                if title_key not in seen_titles:
                    seen_titles.add(title_key)
                    unique_results.append(result)
                    if len(unique_results) >= limit:
                        break
            
            logger.info(f"Hybrid search returned {len(unique_results)} results")
            return unique_results
            
        except Exception as e:
            logger.error(f"Error in hybrid search: {e}")
            return self._fallback_search(project_id, query, limit)
    
    def _extract_relevant_content(self, content: str, query: str) -> str:
        """æå–èˆ‡å•é¡Œæœ€ç›¸é—œçš„å…§å®¹ç‰‡æ®µ"""
        if not content:
            return ""
        
        # ç°¡å–®å¯¦ç¾ï¼šæ‰¾åŒ…å«å•é¡Œé—œéµè©çš„æ®µè½
        query_keywords = [word.lower() for word in query.split() if len(word) > 2]
        paragraphs = content.split('\n\n')
        
        relevant_paragraphs = []
        for paragraph in paragraphs:
            if any(keyword in paragraph.lower() for keyword in query_keywords):
                relevant_paragraphs.append(paragraph)
        
        if relevant_paragraphs:
            # é™åˆ¶é•·åº¦ï¼Œä¿ç•™æœ€ç›¸é—œçš„éƒ¨åˆ†
            result = '\n\n'.join(relevant_paragraphs)
            if len(result) > 800:  # é™åˆ¶æ¯å€‹æ¢ç›®æœ€å¤§é•·åº¦
                result = result[:800] + '...'
            return result
        else:
            # å¦‚æœæ²’æœ‰ç›´æ¥åŒ¹é…ï¼Œè¿”å›é–‹é ­éƒ¨åˆ†
            return content[:400] + '...' if len(content) > 400 else content
    
    def _fallback_search(self, project_id: str, query: str, limit: int) -> List[Dict[str, str]]:
        """é™ç´šæœç´¢ï¼šä½¿ç”¨åŸå§‹æœç´¢æ–¹æ³•"""
        logger.info(f"[SQLITE] Using fallback search")
        
        try:
            # ä½¿ç”¨æœ€çµ‚è¡¨çš„ search_mem_entries æ–¹æ³•
            results = self.search_mem_entries(project=project_id, query=query, limit=limit)
            
            # è½‰æ›æ ¼å¼ä»¥ç¬¦åˆåŸå§‹ API
            formatted_results = []
            for result in results:
                formatted_results.append({
                    'timestamp': result.get('created_at', ''),
                    'title': result['title'] or '',
                    'category': result['category'] or '',
                    'content': result['entry'],
                    'relevance': 'fallback'
                })
            
            logger.info(f"Fallback search found {len(formatted_results)} results")
            return formatted_results
                
        except Exception as e:
            logger.error(f"Error in fallback search: {e}")
            return []

    def _format_search_result(self, row, relevance: float) -> Dict[str, str]:
        """æ ¼å¼åŒ–æœå°‹çµæœ"""
        # æ ¼å¼åŒ–æ™‚é–“æˆ³
        timestamp = row['created_at']
        if isinstance(timestamp, str):
            try:
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
            except:
                formatted_time = timestamp
        else:
            formatted_time = str(timestamp)
        
        content = row['content'] or ''
        return {
            'timestamp': formatted_time,
            'title': row['title'] or '',
            'category': row['category'] or '',
            'content': content[:500] + "..." if len(content) > 500 else content,
            'relevance': relevance
        }
    
    def list_projects(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆåŠå…¶çµ±è¨ˆè³‡è¨Šï¼ˆä½¿ç”¨æ–°è¡¨ï¼‰"""
        logger.info(f"[SQLITE] Calling list_projects")
        try:
            # ä½¿ç”¨æœ€çµ‚è¡¨çš„ list_projects_stats æ–¹æ³•
            project_stats = self.list_projects_stats()
            
            # è½‰æ›æ ¼å¼ä»¥ç¬¦åˆåŸå§‹ API
            projects = []
            for stat in project_stats:
                # æ ¼å¼åŒ–æ™‚é–“
                updated_at = stat['last_updated']
                if isinstance(updated_at, str):
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
                        formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        formatted_time = updated_at
                else:
                    formatted_time = str(updated_at)
                
                # å–å¾—è©²å°ˆæ¡ˆçš„åˆ†é¡
                categories = []
                try:
                    with self.get_connection() as conn:
                        cursor = conn.execute("""
                            SELECT DISTINCT category 
                            FROM memory_entries 
                            WHERE project = ? AND category IS NOT NULL
                        """, (stat['project'],))
                        categories = [row[0] for row in cursor.fetchall()]
                except:
                    pass
                
                projects.append({
                    'id': stat['project'],
                    'name': stat['project'].replace('-', ' ').title(),
                    'file_path': f"SQLite: {self.db_path}",
                    'entries_count': stat['total_entries'],
                    'last_modified': formatted_time,
                    'categories': categories
                })
            
            # åªè¨˜éŒ„ç°¡åŒ–çš„å°ˆæ¡ˆè³‡è¨Š
            simplified_projects = [
                {
                    'id': p['id'],
                    'name': p['name'],
                    'entries_count': p['entries_count'],
                    'last_modified': p['last_modified']
                }
                for p in projects
            ]
            logger.info(f"[SQLITE] list_projects response : {simplified_projects}")
            return projects
                
        except Exception as e:
            logger.error(f"Error listing projects: {e}")
            return []
    
    def get_recent_memory(self, project_id: str, limit: int = 5) -> List[Dict[str, str]]:
        """å–å¾—æœ€è¿‘çš„è¨˜æ†¶æ¢ç›®ï¼ˆä½¿ç”¨æ–°è¡¨ï¼‰"""
        logger.info(f"[SQLITE] Calling get_recent_memory for project: {project_id}")
        try:
            # ä½¿ç”¨æœ€çµ‚è¡¨çš„ list_memories æ–¹æ³•
            results = self.list_memories(project=project_id, limit=limit)
            
            # è½‰æ›æ ¼å¼ä»¥ç¬¦åˆåŸå§‹ API
            formatted_results = []
            for result in results:
                # æ ¼å¼åŒ–æ™‚é–“æˆ³
                timestamp = result['created_at']
                if isinstance(timestamp, str):
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        formatted_time = timestamp
                else:
                    formatted_time = str(timestamp)
                
                formatted_results.append({
                    'timestamp': formatted_time,
                    'title': result['title'] or '',
                    'category': result['category'] or '',
                    'content': result['entry']
                })
            
            return list(reversed(formatted_results))  # è¿”å›æ™‚é–“é †åºï¼ˆèˆŠåˆ°æ–°ï¼‰
                
        except Exception as e:
            logger.error(f"Error getting recent memory for {project_id}: {e}")
            return []
    
    def delete_memory(self, project_id: str) -> bool:
        """åˆªé™¤å°ˆæ¡ˆè¨˜æ†¶ï¼ˆä½¿ç”¨æ–°è¡¨ï¼‰"""
        logger.info(f"[SQLITE] Calling delete_memory for project: {project_id}")
        try:
            with self.get_connection() as conn:
                # åˆªé™¤æ–°è¡¨ä¸­çš„æ‰€æœ‰è¨˜æ†¶æ¢ç›®ï¼ˆFTS è§¸ç™¼å™¨æœƒè‡ªå‹•æ¸…ç†ï¼‰
                cursor = conn.execute("DELETE FROM memory_entries WHERE project = ?", (project_id,))
                deleted_entries = cursor.rowcount
                
                conn.commit()
                
                if deleted_entries > 0:
                    logger.info(f"Memory deleted for project: {project_id} ({deleted_entries} entries)")
                    return True
                else:
                    logger.warning(f"No entries found for project: {project_id}")
                    return False
                
        except Exception as e:
            logger.error(f"Error deleting memory for {project_id}: {e}")
            return False
    
    def delete_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None, 
                           title: str = None, category: str = None, content_match: str = None) -> Dict[str, Any]:
        """åˆªé™¤ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®ï¼ˆä½¿ç”¨æ–°è¡¨ï¼‰"""
        logger.info(f"[SQLITE] Calling delete_memory_entry for project: {project_id}")
        try:
            with self.get_connection() as conn:
                # å»ºç«‹å®‰å…¨çš„æŸ¥è©¢æ¢ä»¶
                conditions = {"project": project_id}
                
                if entry_id is not None:
                    try:
                        conditions["id"] = int(entry_id)
                    except (ValueError, TypeError):
                        return {'success': False, 'message': 'Invalid entry_id format'}
                
                # ä½¿ç”¨ LIKE æŸ¥è©¢çš„æ¢ä»¶éœ€è¦ç‰¹æ®Šè™•ç†
                like_conditions = []
                like_params = []
                
                if timestamp:
                    like_conditions.append("created_at LIKE ?")
                    like_params.append(f"%{timestamp}%")
                
                if title:
                    like_conditions.append("title LIKE ?") 
                    like_params.append(f"%{title}%")
                
                if category:
                    like_conditions.append("category LIKE ?")
                    like_params.append(f"%{category}%")
                
                if content_match:
                    like_conditions.append("entry LIKE ?")
                    like_params.append(f"%{content_match}%")
                
                # å»ºç«‹å®‰å…¨çš„ WHERE å­å¥
                where_clause, where_params = SQLSafetyUtils.build_safe_where_clause(conditions)
                
                # åˆä½µæ‰€æœ‰æ¢ä»¶
                all_conditions = [where_clause] if where_clause else []
                all_conditions.extend(like_conditions)
                all_params = where_params + like_params
                
                final_where_clause = " AND ".join(all_conditions)
                
                # å…ˆæŸ¥è©¢è¦åˆªé™¤çš„æ¢ç›®
                select_sql = f"""
                    SELECT id, title, created_at FROM memory_entries 
                    WHERE {final_where_clause}
                """
                cursor = conn.execute(select_sql, all_params)
                entries_to_delete = cursor.fetchall()
                
                if not entries_to_delete:
                    return {'success': False, 'message': 'No matching entries found to delete'}
                
                # åŸ·è¡Œåˆªé™¤
                delete_sql = f"DELETE FROM memory_entries WHERE {final_where_clause}"
                cursor = conn.execute(delete_sql, all_params)
                deleted_count = cursor.rowcount
                
                conn.commit()
                
                # æ ¼å¼åŒ–å›æ‡‰
                deleted_entries = []
                for entry in entries_to_delete:
                    timestamp = entry['created_at']
                    if isinstance(timestamp, str):
                        try:
                            from datetime import datetime
                            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                            formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            formatted_time = timestamp
                    else:
                        formatted_time = str(timestamp)
                    
                    deleted_entries.append({
                        'timestamp': formatted_time,
                        'title': entry['title'] or ''
                    })
                
                # è¨ˆç®—å‰©é¤˜æ¢ç›®æ•¸é‡
                cursor = conn.execute("SELECT COUNT(*) FROM memory_entries WHERE project = ?", (project_id,))
                remaining_count = cursor.fetchone()[0]
                
                return {
                    'success': True,
                    'message': f"Deleted {deleted_count} entries from project {project_id}",
                    'deleted_count': deleted_count,
                    'remaining_count': remaining_count,
                    'deleted_entries': deleted_entries
                }
                
        except Exception as e:
            logger.error(f"Error deleting memory entry for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}
    
    def edit_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None,
                         new_title: str = None, new_category: str = None, new_content: str = None) -> Dict[str, Any]:
        """ç·¨è¼¯ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®ï¼ˆä½¿ç”¨æ–°è¡¨ï¼‰"""
        logger.info(f"[SQLITE] Calling edit_memory_entry for project: {project_id}")
        try:
            with self.get_connection() as conn:
                # å»ºç«‹æŸ¥è©¢æ¢ä»¶
                where_conditions = ["project = ?"]
                params = [project_id]
                
                if entry_id is not None:
                    try:
                        where_conditions.append("id = ?")
                        params.append(int(entry_id))
                    except ValueError:
                        return {'success': False, 'message': 'Invalid entry_id format'}
                
                if timestamp:
                    where_conditions.append("created_at LIKE ?")
                    params.append(f"%{timestamp}%")
                
                # å…ˆæŸ¥è©¢æ¢ç›®æ˜¯å¦å­˜åœ¨
                select_sql = f"""
                    SELECT id, title, category, created_at FROM memory_entries 
                    WHERE {' AND '.join(where_conditions)}
                    LIMIT 1
                """
                cursor = conn.execute(select_sql, params)
                entry = cursor.fetchone()
                
                if not entry:
                    return {'success': False, 'message': 'No matching entry found to edit'}
                
                # ä½¿ç”¨æœ€çµ‚è¡¨çš„ edit_memory æ–¹æ³•
                success = self.edit_memory(
                    entry_id=entry['id'],
                    title=new_title,
                    entry=new_content,
                    category=new_category
                )
                
                if not success:
                    return {'success': False, 'message': 'Failed to update entry'}
                
                # å–å¾—æ›´æ–°å¾Œçš„æ¢ç›®
                updated_entry = self.get_memory_entry(entry['id'])
                
                if not updated_entry:
                    return {'success': False, 'message': 'Failed to retrieve updated entry'}
                
                # æ ¼å¼åŒ–æ™‚é–“æˆ³
                timestamp = updated_entry['created_at']
                if isinstance(timestamp, str):
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        formatted_time = timestamp
                else:
                    formatted_time = str(timestamp)
                
                return {
                    'success': True,
                    'message': f"Successfully edited entry in project {project_id}",
                    'edited_entry': {
                        'timestamp': formatted_time,
                        'title': updated_entry['title'] or '',
                        'category': updated_entry['category'] or ''
                    }
                }
                
        except Exception as e:
            logger.error(f"Error editing memory entry for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}
    
    def list_memory_entries(self, project_id: str) -> Dict[str, Any]:
        """åˆ—å‡ºå°ˆæ¡ˆä¸­çš„æ‰€æœ‰è¨˜æ†¶æ¢ç›®ï¼Œå¸¶æœ‰ç´¢å¼•ï¼ˆä½¿ç”¨æœ€çµ‚è¡¨ï¼‰"""
        logger.info(f"[SQLITE] Calling list_memory_entries for project: {project_id}")
        try:
            # ä½¿ç”¨æœ€çµ‚è¡¨çš„ list_memories æ–¹æ³•
            results = self.list_memories(project=project_id, limit=1000)
            
            entries = []
            for result in results:
                # æ ¼å¼åŒ–æ™‚é–“æˆ³
                timestamp = result['created_at']
                if isinstance(timestamp, str):
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        formatted_time = timestamp
                else:
                    formatted_time = str(timestamp)
                
                content = result['entry']
                entries.append({
                    'id': result['id'],
                    'timestamp': formatted_time,
                    'title': result['title'] or '',
                    'category': result['category'] or '',
                    'content_preview': content[:100] + "..." if len(content) > 100 else content
                })
            
            return {
                'success': True,
                'total_entries': len(entries),
                'entries': entries
            }
                
        except Exception as e:
            logger.error(f"Error listing memory entries for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}
    
    def get_memory_stats(self, project_id: str) -> Dict[str, Any]:
        """å–å¾—è¨˜æ†¶çµ±è¨ˆè³‡è¨Šï¼ˆä½¿ç”¨æœ€çµ‚è¡¨ï¼‰"""
        logger.info(f"[SQLITE] Calling get_memory_stats for project: {project_id}")
        try:
            # ä½¿ç”¨æœ€çµ‚è¡¨çš„ get_project_stats æ–¹æ³•
            stats = self.get_project_stats(project_id)
            
            if not stats or stats.get('total_entries', 0) == 0:
                return {'exists': False}
            
            # å–å¾—åˆ†é¡è³‡è¨Š
            categories = []
            try:
                with self.get_connection() as conn:
                    cursor = conn.execute("""
                        SELECT DISTINCT category 
                        FROM memory_entries 
                        WHERE project = ? AND category IS NOT NULL
                    """, (project_id,))
                    categories = [row[0] for row in cursor.fetchall()]
            except:
                pass
            
            # å–å¾—å…§å®¹çµ±è¨ˆ
            total_characters = 0
            try:
                with self.get_connection() as conn:
                    cursor = conn.execute("""
                        SELECT SUM(LENGTH(entry)) as total_characters
                        FROM memory_entries 
                        WHERE project = ?
                    """, (project_id,))
                    result = cursor.fetchone()
                    total_characters = result[0] if result and result[0] else 0
            except:
                pass
            
            # æ ¼å¼åŒ–æ™‚é–“
            def format_timestamp(ts):
                if not ts:
                    return None
                if isinstance(ts, str):
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                        return dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        return ts
                return str(ts)
            
            # è¨ˆç®—å­—æ•¸ï¼ˆç°¡å–®ä¼°ç®—ï¼‰
            total_words = total_characters // 5
            
            return {
                'exists': True,
                'total_entries': stats['total_entries'],
                'total_words': total_words,
                'total_characters': total_characters,
                'categories': categories,
                'latest_entry': format_timestamp(stats.get('last_updated')),
                'oldest_entry': format_timestamp(stats.get('first_entry'))
            }
                
        except Exception as e:
            logger.error(f"Error getting memory stats for {project_id}: {e}")
            return {'exists': False, 'error': str(e)}

    def rename_project(self, project_id: str, new_name: str) -> bool:
        """é‡æ–°å‘½åå°ˆæ¡ˆï¼ˆä½¿ç”¨æœ€çµ‚è¡¨ï¼‰"""
        logger.info(f"[SQLITE] Calling rename_project for project: {project_id} to: {new_name}")
        try:
            with self.get_connection() as conn:
                # æª¢æŸ¥å°ˆæ¡ˆæ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨æœ€çµ‚è¡¨ï¼‰
                cursor = conn.execute("SELECT COUNT(*) FROM memory_entries WHERE project = ?", (project_id,))
                if cursor.fetchone()[0] == 0:
                    logger.error(f"Project {project_id} does not exist in memory_entries")
                    return False
                
                # åœ¨æœ€çµ‚è¡¨æ¶æ§‹ä¸­ï¼Œå°ˆæ¡ˆé‡å‘½åå¯¦éš›ä¸Šæ˜¯æ›´æ–°æ‰€æœ‰æ¢ç›®çš„ project æ¬„ä½
                # é€™ç›¸ç•¶æ–¼å°‡å°ˆæ¡ˆå¾èˆŠåç¨±é·ç§»åˆ°æ–°åç¨±
                cursor = conn.execute("""
                    UPDATE memory_entries 
                    SET project = ?, updated_at = CURRENT_TIMESTAMP 
                    WHERE project = ?
                """, (new_name, project_id,))
                
                updated_count = cursor.rowcount
                conn.commit()
                
                if updated_count > 0:
                    logger.info(f"Project {project_id} renamed to '{new_name}' ({updated_count} entries updated)")
                    return True
                else:
                    logger.warning(f"No entries found for project {project_id}")
                    return False
                    
        except Exception as e:
            logger.error(f"Error renaming project {project_id} to '{new_name}': {e}")
            return False
    
    def _count_entries(self, conn, project_id: str) -> int:
        """è¨ˆç®—å°ˆæ¡ˆçš„è¨˜æ†¶æ¢ç›®æ•¸é‡"""
        cursor = conn.execute("SELECT COUNT(*) FROM memory_entries WHERE project_id = ?", (project_id,))
        return cursor.fetchone()[0]
    
    def _generate_summary(self, content: str, title: str = None) -> str:
        """ç”Ÿæˆå…§å®¹æ‘˜è¦ï¼Œå¤§å¹…æ¸›å°‘ token ä½¿ç”¨"""
        if not content:
            return title or "Empty content"
        
        # æ¸…ç†å…§å®¹
        content = content.strip()
        
        # å¦‚æœæœ‰æ¨™é¡Œï¼Œå„ªå…ˆä½¿ç”¨æ¨™é¡Œä½œç‚ºæ‘˜è¦åŸºç¤
        if title and title.strip():
            title_clean = title.strip()
            # å¦‚æœæ¨™é¡Œå·²ç¶“å¾ˆå¥½åœ°æ¦‚æ‹¬äº†å…§å®¹ï¼Œç›´æ¥ä½¿ç”¨
            if len(title_clean) > 10 and len(title_clean) < 100:
                return title_clean
        
        # æå–å‰200å­—å…ƒï¼Œä½†åœ¨å¥å­é‚Šç•Œæˆªæ–·
        if len(content) <= 200:
            return content
        
        summary = content[:200]
        
        # åœ¨å¥å­é‚Šç•Œæˆªæ–·
        sentence_endings = ['. ', 'ã€‚', 'ï¼', '!', 'ï¼Ÿ', '?', '\n\n']
        best_cut = 0
        
        for ending in sentence_endings:
            pos = summary.rfind(ending)
            if pos > 50:  # ç¢ºä¿æ‘˜è¦æœ‰è¶³å¤ é•·åº¦
                best_cut = max(best_cut, pos + len(ending))
        
        if best_cut > 0:
            return summary[:best_cut].strip()
        
        # å¦‚æœæ‰¾ä¸åˆ°å¥½çš„æˆªæ–·é»ï¼Œåœ¨è©é‚Šç•Œæˆªæ–·
        words = summary.split()
        if len(words) > 1:
            return ' '.join(words[:-1]) + '...'
        
        return summary + '...'
    
    def _extract_keywords(self, content: str, title: str = None, category: str = None) -> str:
        """æå–é—œéµå­—ï¼Œæå‡æœå°‹ç²¾åº¦"""
        keywords = set()
        
        # å¾æ¨™é¡Œæå–
        if title:
            title_words = title.replace('-', ' ').replace('_', ' ').split()
            keywords.update([w.lower() for w in title_words if len(w) > 2])
        
        # å¾åˆ†é¡æå–
        if category:
            category_words = category.replace('-', ' ').replace('_', ' ').split()
            keywords.update([w.lower() for w in category_words if len(w) > 2])
        
        # å¾å…§å®¹æå–æŠ€è¡“é—œéµå­—
        if content:
            # å¸¸è¦‹æŠ€è¡“è¡“èª
            tech_terms = ['api', 'sql', 'database', 'index', 'table', 'function', 'class', 'method', 
                         'bug', 'fix', 'error', 'issue', 'feature', 'implement', 'design', 'architecture',
                         'mcp', 'server', 'client', 'backend', 'frontend', 'token', 'memory', 'search',
                         'sqlite', 'markdown', 'hierarchy', 'optimization', 'performance']
            
            content_lower = content.lower()
            for term in tech_terms:
                if term in content_lower:
                    keywords.add(term)
        
        # é™åˆ¶é—œéµå­—æ•¸é‡ï¼Œé¿å…éé•·
        keywords_list = list(keywords)[:10]
        return ', '.join(sorted(keywords_list))
    
    def _detect_hierarchy_level(self, content: str, title: str = None) -> int:
        """æª¢æ¸¬éšå±¤ç´šåˆ¥ï¼Œæ”¯æ´éšå±¤çµæ§‹"""
        if not content:
            return 0
        
        # æª¢æŸ¥æ¨™é¡Œä¸­çš„ Markdown æ¨™è¨˜
        if title:
            if title.startswith('###'):
                return 2  # å°æ¨™é¡Œ
            elif title.startswith('##'):
                return 1  # ä¸­æ¨™é¡Œ
            elif title.startswith('#'):
                return 0  # å¤§æ¨™é¡Œ
        
        # æª¢æŸ¥å…§å®¹ä¸­çš„æ¨™è¨˜
        lines = content.split('\n')
        for line in lines[:5]:  # åªæª¢æŸ¥å‰5è¡Œ
            line = line.strip()
            if line.startswith('###'):
                return 2
            elif line.startswith('##'):
                return 1
            elif line.startswith('#'):
                return 0
        
        # æ ¹æ“šå…§å®¹ç‰¹å¾µåˆ¤æ–·
        if any(keyword in content.lower() for keyword in ['å¯¦ä½œ', 'implementation', 'æ­¥é©Ÿ', 'step', 'ç´°ç¯€', 'detail']):
            return 2  # å¯¦ä½œç´°ç¯€
        elif any(keyword in content.lower() for keyword in ['åŠŸèƒ½', 'feature', 'æ¨¡çµ„', 'module', 'éšæ®µ', 'phase']):
            return 1  # åŠŸèƒ½æ¨¡çµ„
        
        return 0  # é è¨­ç‚ºæ ¹ç´šåˆ¥
    
    def _generate_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå…§å®¹é›œæ¹Šï¼Œç”¨æ–¼æª¢æ¸¬è®Šæ›´"""
        import hashlib
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
    
    
    # ==================== INDEX TABLE METHODS ====================
    
    def search_index(self, project_id: str, query: str, limit: int = 10, 
                    entry_type: str = None, status: str = None, need_full_content: bool = False) -> List[Dict]:
        """çµæ§‹åŒ–æœå°‹ - ç›´æ¥ä½¿ç”¨ memory_entries è¡¨"""
        try:
            with self.get_connection() as conn:
                # ç›´æ¥æœå°‹ memory_entries è¡¨
                where_conditions = ["project = ?"]
                params = [project_id]
                
                # æ–‡å­—æœå°‹æ¢ä»¶
                if query:
                    where_conditions.append("(title LIKE ? OR entry LIKE ?)")
                    params.extend([f"%{query}%", f"%{query}%"])
                
                # åŸ·è¡Œæœå°‹
                cursor = conn.execute(f"""
                    SELECT id, project, title, category, entry_type, entry, created_at, updated_at
                    FROM memory_entries
                    WHERE {' AND '.join(where_conditions)}
                    ORDER BY created_at DESC
                    LIMIT ?
                """, params + [limit])
                
                # è½‰æ›çµæœç‚ºå­—å…¸æ ¼å¼
                results = []
                for row in cursor.fetchall():
                    result = dict(row)
                    result['match_type'] = 'content'
                    results.append(result)
                
                logger.info(f"âœ… Search found {len(results)} results for '{query}' in {project_id}")
                return results
                
        except Exception as e:
            logger.error(f"Error searching index for {project_id}: {e}")
            return []
    
    def get_index_entry(self, entry_id: int) -> Dict:
        """ç²å–ç‰¹å®šæ¢ç›®çš„ç´¢å¼•è³‡è¨Š - åŸºæ–¼ V2 è¡¨"""
        try:
            with self.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT mi.*, me.title, me.category, me.content, me.created_at as entry_created_at
                    FROM memory_index mi
                    JOIN memory_entries me ON mi.entry_id = me.id
                    WHERE mi.entry_id = ?
                """, (entry_id,))
                
                row = cursor.fetchone()
                if row:
                    return dict(row)
                return {}
                
        except Exception as e:
            logger.error(f"Error getting index entry {entry_id}: {e}")
            return {}
    
    def update_index_entry(self, entry_id: int, **kwargs) -> bool:
        """æ›´æ–°ç´¢å¼•æ¢ç›®"""
        try:
            with self.get_connection() as conn:
                # å…è¨±æ›´æ–°çš„æ¬„ä½
                allowed_fields = ['summary', 'keywords', 'hierarchy_level', 'entry_type', 
                                'importance_level', 'parent_entry_id', 'hierarchy_path']
                
                update_fields = []
                params = []
                
                for field, value in kwargs.items():
                    if field in allowed_fields:
                        update_fields.append(f"{field} = ?")
                        params.append(value)
                
                if not update_fields:
                    return False
                
                update_fields.append("updated_at = CURRENT_TIMESTAMP")
                params.append(entry_id)
                
                conn.execute(f"""
                    UPDATE memory_index 
                    SET {', '.join(update_fields)}
                    WHERE entry_id = ?
                """, params)
                
                conn.commit()
                logger.info(f"Updated index entry {entry_id}")
                return True
                
        except Exception as e:
            logger.error(f"Error updating index entry {entry_id}: {e}")
            return False
    
    def get_hierarchy_tree(self, project_id: str) -> Dict:
        """ç²å–å°ˆæ¡ˆçš„éšå±¤æ¨¹ç‹€çµæ§‹"""
        try:
            with self.get_connection() as conn:
                # ç›´æ¥å¾ memory_entries ç²å–æ¢ç›®
                cursor = conn.execute("""
                    SELECT id, title, category, entry_type, created_at
                    FROM memory_entries
                    WHERE project = ?
                    ORDER BY created_at DESC
                """, (project_id,))
                
                entries = []
                for row in cursor.fetchall():
                    entries.append(dict(row))
                
                return {'children': entries}
                
        except Exception as e:
            logger.error(f"Error getting hierarchy tree for {project_id}: {e}")
            return {'children': []}
    
    def rebuild_index_for_project(self, project_id: str) -> Dict:
        """ç‚ºå°ˆæ¡ˆé‡å»ºæ‰€æœ‰ç´¢å¼•æ¢ç›® (åŸºæ–¼ memory_entries è¡¨)"""
        try:
            with self.get_connection() as conn:
                # ç²å–æ‰€æœ‰è¨˜æ†¶æ¢ç›®
                cursor = conn.execute("""
                    SELECT id, title, category, entry, created_at
                    FROM memory_entries
                    WHERE project = ?
                    ORDER BY created_at DESC
                """, (project_id,))
                
                entries = cursor.fetchall()
                total_entries = len(entries)
                
                if total_entries == 0:
                    return {
                        'success': False,
                        'message': f'Project {project_id} has no entries to index'
                    }
                
                logger.info(f"âœ… Found {total_entries} entries for project {project_id}")
                return {
                    'success': True,
                    'total_entries': total_entries,
                    'indexed_entries': total_entries,
                    'message': f'Project {project_id} has {total_entries} entries available for search'
                }
                
        except Exception as e:
            logger.error(f"Error rebuilding index for {project_id}: {e}")
            return {'success': False, 'message': str(e)}
    
    def get_index_stats(self, project_id: str = None) -> Dict:
        """ç²å–ç´¢å¼•çµ±è¨ˆè³‡è¨Š"""
        try:
            with self.get_connection() as conn:
                if project_id:
                    # å°ˆæ¡ˆç‰¹å®šçµ±è¨ˆ
                    cursor = conn.execute("""
                        SELECT COUNT(*) as total,
                               COUNT(DISTINCT entry_type) as types
                        FROM memory_entries
                        WHERE project = ?
                    """, (project_id,))
                    
                    result = cursor.fetchone()
                    return {
                        'total_indexed': result['total'] or 0,
                        'entry_types': result['types'] or 0,
                        'max_hierarchy_level': 0
                    }
                else:
                    # å…¨å±€çµ±è¨ˆ
                    cursor = conn.execute("""
                        SELECT COUNT(*) as total,
                               COUNT(DISTINCT project) as projects
                        FROM memory_entries
                    """)
                    
                    result = cursor.fetchone()
                    return {
                        'total_indexed': result['total'] or 0,
                        'indexed_projects': result['projects'] or 0
                    }
                    
        except Exception as e:
            logger.error(f"Error getting index stats: {e}")
            return {'total_indexed': 0, 'indexed_projects': 0}
    
    def _detect_entry_type(self, content: str, category: str = None) -> str:
        """æª¢æ¸¬æ¢ç›®é¡å‹"""
        content_lower = content.lower()
        category_lower = (category or '').lower()
        
        # æ ¹æ“šåˆ†é¡åˆ¤æ–·
        if any(keyword in category_lower for keyword in ['bug', 'error', 'fix', 'ä¿®å¾©', 'éŒ¯èª¤']):
            return 'bug'
        elif any(keyword in category_lower for keyword in ['feature', 'enhancement', 'åŠŸèƒ½', 'å¢å¼·']):
            return 'feature'
        elif any(keyword in category_lower for keyword in ['milestone', 'release', 'é‡Œç¨‹ç¢‘', 'ç™¼å¸ƒ']):
            return 'milestone'
        
        # æ ¹æ“šå…§å®¹åˆ¤æ–·
        if any(keyword in content_lower for keyword in ['bug', 'error', 'fix', 'crash', 'éŒ¯èª¤', 'ä¿®å¾©', 'å´©æ½°']):
            return 'bug'
        elif any(keyword in content_lower for keyword in ['feature', 'implement', 'add', 'åŠŸèƒ½', 'å¯¦ä½œ', 'æ–°å¢']):
            return 'feature'
        elif any(keyword in content_lower for keyword in ['milestone', 'release', 'version', 'é‡Œç¨‹ç¢‘', 'ç™¼å¸ƒ', 'ç‰ˆæœ¬']):
            return 'milestone'
        
        return 'discussion'  # é è¨­ç‚ºè¨è«–
    
    def delete_index_entry(self, entry_id: int) -> bool:
        """åˆªé™¤ç´¢å¼•æ¢ç›®"""
        try:
            with self.get_connection() as conn:
                conn.execute("DELETE FROM memory_index WHERE entry_id = ?", (entry_id,))
                conn.commit()
                logger.info(f"Deleted index entry {entry_id}")
                return True
                
        except Exception as e:
            logger.error(f"Error deleting index entry {entry_id}: {e}")
            return False
    
    
    
    
    # ==================== NEW TABLE METHODS ====================
    
    def add_memory(self, project: str, title: str, entry: str, 
                      category: str = None, entry_type: str = "note", 
                      summary: str = None) -> int:
        """æ–°å¢è¨˜æ†¶æ¢ç›®åˆ°æœ€çµ‚è¡¨"""
        with self.get_connection() as conn:
            cursor = conn.execute("""
                INSERT INTO memory_entries 
                (project, category, entry_type, title, summary, entry, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (project, category, entry_type, title, summary, entry))
            conn.commit()  # æ‰‹å‹•æäº¤äº‹å‹™
            return cursor.lastrowid
    
    def search_mem_entries(self, project: str = None, query: str = None, 
                         category: str = None, entry_type: str = None, 
                         limit: int = 10) -> List[Dict[str, Any]]:
        """æœå°‹æœ€çµ‚è¡¨ä¸­çš„è¨˜æ†¶æ¢ç›®ï¼ˆå…§éƒ¨æ–¹æ³•ï¼‰"""
        with self.get_connection() as conn:
            if query:
                # ä½¿ç”¨ FTS5 å…¨æ–‡æœå°‹
                sql = """
                    SELECT m.id, m.project, m.category, m.entry_type, m.title, 
                           m.summary, m.entry, m.created_at, m.updated_at
                    FROM memory_entries m
                    JOIN memory_fts fts ON m.id = fts.rowid
                    WHERE memory_fts MATCH ?
                """
                params = [query]
                
                if project:
                    sql += " AND m.project = ?"
                    params.append(project)
                if category:
                    sql += " AND m.category = ?"
                    params.append(category)
                if entry_type:
                    sql += " AND m.entry_type = ?"
                    params.append(entry_type)
                    
                sql += " ORDER BY bm25(memory_fts) LIMIT ?"
                params.append(limit)
            else:
                # ä¸€èˆ¬æŸ¥è©¢
                sql = """
                    SELECT id, project, category, entry_type, title, 
                           summary, entry, created_at, updated_at
                    FROM memory_entries
                    WHERE 1=1
                """
                params = []
                
                if project:
                    sql += " AND project = ?"
                    params.append(project)
                if category:
                    sql += " AND category = ?"
                    params.append(category)
                if entry_type:
                    sql += " AND entry_type = ?"
                    params.append(entry_type)
                    
                sql += " ORDER BY updated_at DESC LIMIT ?"
                params.append(limit)
            
            cursor = conn.execute(sql, params)
            return [dict(row) for row in cursor.fetchall()]
    
    def get_memory_entry(self, entry_id: int) -> Dict[str, Any]:
        """æ ¹æ“š ID å–å¾—è¨˜æ†¶æ¢ç›®ï¼ˆæœ€çµ‚è¡¨ï¼‰"""
        with self.get_connection() as conn:
            cursor = conn.execute("""
                SELECT id, project, category, entry_type, title, 
                       summary, entry, created_at, updated_at
                FROM memory_entries
                WHERE id = ?
            """, (entry_id,))
            row = cursor.fetchone()
            return dict(row) if row else None
    
    def list_memories(self, project: str = None, category: str = None, 
                         entry_type: str = None, limit: int = 50) -> List[Dict[str, Any]]:
        """åˆ—å‡ºè¨˜æ†¶æ¢ç›®ï¼ˆæœ€çµ‚è¡¨ï¼‰"""
        with self.get_connection() as conn:
            sql = """
                SELECT id, project, category, entry_type, title, 
                       summary, entry, created_at, updated_at
                FROM memory_entries
                WHERE 1=1
            """
            params = []
            
            if project:
                sql += " AND project = ?"
                params.append(project)
            if category:
                sql += " AND category = ?"
                params.append(category)
            if entry_type:
                sql += " AND entry_type = ?"
                params.append(entry_type)
                
            sql += " ORDER BY updated_at DESC LIMIT ?"
            params.append(limit)
            
            cursor = conn.execute(sql, params)
            return [dict(row) for row in cursor.fetchall()]
    
    def edit_memory(self, entry_id: int, title: str = None, entry: str = None,
                       category: str = None, entry_type: str = None, 
                       summary: str = None) -> bool:
        """ç·¨è¼¯è¨˜æ†¶æ¢ç›®ï¼ˆæœ€çµ‚è¡¨ï¼‰"""
        with self.get_connection() as conn:
            # å»ºç«‹å®‰å…¨çš„æ›´æ–°æ¬„ä½
            update_fields = {}
            
            if title is not None:
                update_fields["title"] = title
            if entry is not None:
                update_fields["entry"] = entry
            if category is not None:
                update_fields["category"] = category
            if entry_type is not None:
                update_fields["entry_type"] = entry_type
            if summary is not None:
                update_fields["summary"] = summary
                
            if not update_fields:
                return False
            
            # ä½¿ç”¨å®‰å…¨çš„ UPDATE å­å¥å»ºç«‹å™¨
            update_clause, update_params = SQLSafetyUtils.build_safe_update_clause(update_fields)
            
            # åŠ å…¥æ™‚é–“æˆ³æ›´æ–°
            update_clause += ", updated_at = CURRENT_TIMESTAMP"
            update_params.append(entry_id)
            
            sql = f"UPDATE memory_entries SET {update_clause} WHERE id = ?"
            cursor = conn.execute(sql, update_params)
            conn.commit()  # æ‰‹å‹•æäº¤äº‹å‹™
            return cursor.rowcount > 0
    
    def delete_memory_by_id(self, entry_id: int) -> bool:
        """åˆªé™¤è¨˜æ†¶æ¢ç›®ï¼ˆæœ€çµ‚è¡¨ï¼‰"""
        with self.get_connection() as conn:
            cursor = conn.execute("DELETE FROM memory_entries WHERE id = ?", (entry_id,))
            conn.commit()  # æ‰‹å‹•æäº¤äº‹å‹™
            return cursor.rowcount > 0
    
    def get_project_stats(self, project: str) -> Dict[str, Any]:
        """å–å¾—å°ˆæ¡ˆçµ±è¨ˆè³‡è¨Šï¼ˆæœ€çµ‚è¡¨ï¼‰"""
        with self.get_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    COUNT(*) as total_entries,
                    COUNT(DISTINCT category) as categories,
                    COUNT(DISTINCT entry_type) as entry_types,
                    MIN(created_at) as first_entry,
                    MAX(updated_at) as last_updated
                FROM memory_entries
                WHERE project = ?
            """, (project,))
            return dict(cursor.fetchone())
    
    def list_projects_stats(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆåŠçµ±è¨ˆï¼ˆæœ€çµ‚è¡¨ï¼‰"""
        with self.get_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    project,
                    COUNT(*) as total_entries,
                    COUNT(DISTINCT category) as categories,
                    COUNT(DISTINCT entry_type) as entry_types,
                    MAX(updated_at) as last_updated
                FROM memory_entries
                GROUP BY project
                ORDER BY last_updated DESC
            """)
            return [dict(row) for row in cursor.fetchall()]
    
    # ==================== DATA MIGRATION METHODS ====================
    

class DataSyncManager:
    """
    è³‡æ–™åŒæ­¥ç®¡ç†å™¨ - è² è²¬ Markdown åˆ° SQLite çš„åŒæ­¥
    Data Sync Manager - Handles Markdown to SQLite synchronization
    
    æä¾›è‡ªå‹•åŒæ­¥åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç›¸ä¼¼åº¦æª¢æ¸¬ã€å…§å®¹åˆä½µå’Œè¡çªè§£æ±º
    Provides auto sync features including similarity detection, content merging, and conflict resolution
    
    Sync Modes / åŒæ­¥æ¨¡å¼:
    - auto: è‡ªå‹•åˆä½µ / Automatic merging
    - interactive: äº’å‹•å¼é¸æ“‡ / Interactive selection  
    - preview: é è¦½æ¨¡å¼ / Preview mode
    """
    
    def __init__(self, markdown_backend: MemoryBackend, sqlite_backend: MemoryBackend):
        self.markdown = markdown_backend
        self.sqlite = sqlite_backend
        self.sync_log = []
    
    def sync_all_projects(self, mode='auto', similarity_threshold=0.8):
        """åŒæ­¥æ‰€æœ‰ Markdown å°ˆæ¡ˆåˆ° SQLite
        
        Args:
            mode: 'auto', 'interactive', 'preview'
            similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ (0.0-1.0)
        """
        logger.info("é–‹å§‹åŒæ­¥ Markdown å°ˆæ¡ˆåˆ° SQLite...")
        
        # ç²å–æ‰€æœ‰ Markdown å°ˆæ¡ˆ
        markdown_projects = self.markdown.list_projects()
        
        if not markdown_projects:
            logger.info("æ²’æœ‰æ‰¾åˆ° Markdown å°ˆæ¡ˆ")
            return {'success': True, 'message': 'æ²’æœ‰å°ˆæ¡ˆéœ€è¦åŒæ­¥', 'synced': 0}
        
        logger.info(f"æ‰¾åˆ° {len(markdown_projects)} å€‹ Markdown å°ˆæ¡ˆ")
        
        synced_count = 0
        skipped_count = 0
        error_count = 0
        
        for project in markdown_projects:
            try:
                result = self.sync_project(project['id'], mode, similarity_threshold)
                if result['action'] == 'synced':
                    synced_count += 1
                elif result['action'] == 'skipped':
                    skipped_count += 1
                
                self.sync_log.append({
                    'project_id': project['id'],
                    'action': result['action'],
                    'message': result['message']
                })
                
            except Exception as e:
                error_count += 1
                error_msg = f"åŒæ­¥å°ˆæ¡ˆ {project['id']} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
                logger.error(error_msg)
                self.sync_log.append({
                    'project_id': project['id'],
                    'action': 'error',
                    'message': error_msg
                })
        
        summary = {
            'success': True,
            'total_projects': len(markdown_projects),
            'synced': synced_count,
            'skipped': skipped_count,
            'errors': error_count,
            'log': self.sync_log
        }
        
        logger.info(f"åŒæ­¥å®Œæˆ: {synced_count} å€‹å°ˆæ¡ˆå·²åŒæ­¥, {skipped_count} å€‹è·³é, {error_count} å€‹éŒ¯èª¤")
        return summary
    
    def sync_project(self, project_id, mode='auto', similarity_threshold=0.8):
        """åŒæ­¥å–®å€‹å°ˆæ¡ˆ"""
        logger.info(f"æ­£åœ¨åŒæ­¥å°ˆæ¡ˆ: {project_id}")
        
        # ç²å– Markdown å…§å®¹
        markdown_content = self.markdown.get_memory(project_id)
        if not markdown_content:
            return {'action': 'skipped', 'message': f'å°ˆæ¡ˆ {project_id} æ²’æœ‰å…§å®¹'}
        
        # æª¢æŸ¥ SQLite ä¸­æ˜¯å¦å·²å­˜åœ¨
        sqlite_content = self.sqlite.get_memory(project_id)
        
        if sqlite_content:
            # å°ˆæ¡ˆå·²å­˜åœ¨ï¼Œè™•ç†åˆä½µé‚è¼¯
            return self.handle_existing_project(project_id, markdown_content, sqlite_content, mode, similarity_threshold)
        else:
            # æ–°å°ˆæ¡ˆï¼Œç›´æ¥åŒ¯å…¥
            return self.import_project(project_id, markdown_content, mode)
    
    def handle_existing_project(self, project_id, markdown_content, sqlite_content, mode, similarity_threshold):
        """è™•ç†å·²å­˜åœ¨çš„å°ˆæ¡ˆ"""
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarity = self.calculate_similarity(markdown_content, sqlite_content)
        
        logger.info(f"å°ˆæ¡ˆ {project_id} ç›¸ä¼¼åº¦: {similarity:.2f}")
        
        if mode == 'preview':
            return {
                'action': 'preview',
                'message': f'å°ˆæ¡ˆå·²å­˜åœ¨ï¼Œç›¸ä¼¼åº¦: {similarity:.2f}ï¼Œå»ºè­°å‹•ä½œ: {"åˆä½µ" if similarity > similarity_threshold else "å‰µå»ºæ–°å°ˆæ¡ˆ"}'
            }
        
        if similarity > similarity_threshold:
            # é«˜ç›¸ä¼¼åº¦ï¼Œè‡ªå‹•åˆä½µ
            if mode == 'auto':
                merged_content = self.merge_contents(markdown_content, sqlite_content)
                success = self.replace_project_content(project_id, merged_content)
                if success:
                    return {'action': 'synced', 'message': f'å°ˆæ¡ˆ {project_id} å·²åˆä½µ (ç›¸ä¼¼åº¦: {similarity:.2f})'}
                else:
                    return {'action': 'error', 'message': f'åˆä½µå°ˆæ¡ˆ {project_id} å¤±æ•—'}
            elif mode == 'interactive':
                # äº’å‹•æ¨¡å¼ - é€™è£¡ç°¡åŒ–ç‚ºè‡ªå‹•åˆä½µï¼Œå¯¦éš›å¯ä»¥æ·»åŠ ç”¨æˆ¶è¼¸å…¥
                logger.info(f"äº’å‹•æ¨¡å¼: è‡ªå‹•åˆä½µé«˜ç›¸ä¼¼åº¦å°ˆæ¡ˆ {project_id}")
                merged_content = self.merge_contents(markdown_content, sqlite_content)
                success = self.replace_project_content(project_id, merged_content)
                if success:
                    return {'action': 'synced', 'message': f'å°ˆæ¡ˆ {project_id} å·²åˆä½µ (äº’å‹•æ¨¡å¼)'}
                else:
                    return {'action': 'error', 'message': f'åˆä½µå°ˆæ¡ˆ {project_id} å¤±æ•—'}
        else:
            # ä½ç›¸ä¼¼åº¦ï¼Œå‰µå»ºæ–°å°ˆæ¡ˆ
            new_project_id = f"{project_id}-markdown-import"
            success = self.import_project(new_project_id, markdown_content, mode)
            if success['action'] == 'synced':
                return {'action': 'synced', 'message': f'å‰µå»ºæ–°å°ˆæ¡ˆ {new_project_id} (åŸå°ˆæ¡ˆç›¸ä¼¼åº¦éä½: {similarity:.2f})'}
            else:
                return success
    
    def import_project(self, project_id, markdown_content, mode):
        """åŒ¯å…¥æ–°å°ˆæ¡ˆ"""
        if mode == 'preview':
            return {'action': 'preview', 'message': f'å°‡å‰µå»ºæ–°å°ˆæ¡ˆ: {project_id}'}
        
        # è§£æ Markdown å…§å®¹ä¸¦åŒ¯å…¥åˆ° SQLite
        entries = self.parse_markdown_entries(markdown_content)
        
        success_count = 0
        for entry in entries:
            success = self.sqlite.save_memory(
                project_id,
                entry['content'],
                entry['title'],
                entry['category']
            )
            if success:
                success_count += 1
        
        if success_count > 0:
            return {'action': 'synced', 'message': f'å°ˆæ¡ˆ {project_id} å·²åŒ¯å…¥ ({success_count} å€‹æ¢ç›®)'}
        else:
            return {'action': 'error', 'message': f'åŒ¯å…¥å°ˆæ¡ˆ {project_id} å¤±æ•—'}
    
    def calculate_similarity(self, content1, content2):
        """è¨ˆç®—å…©å€‹å…§å®¹çš„ç›¸ä¼¼åº¦ (ç°¡åŒ–ç‰ˆæœ¬)"""
        # ç°¡åŒ–çš„ç›¸ä¼¼åº¦è¨ˆç®— - åŸºæ–¼é—œéµè©é‡ç–Š
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def merge_contents(self, markdown_content, sqlite_content):
        """åˆä½µå…©å€‹å…§å®¹ (ç°¡åŒ–ç‰ˆæœ¬)"""
        # ç°¡åŒ–çš„åˆä½µé‚è¼¯ - å°‡ Markdown å…§å®¹è¿½åŠ åˆ° SQLite å…§å®¹
        markdown_entries = self.parse_markdown_entries(markdown_content)
        
        # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²æ›´å¥½åœ°å»é‡å’Œæ’åº
        return markdown_entries
    
    def parse_markdown_entries(self, markdown_content):
        """è§£æ Markdown å…§å®¹ç‚ºæ¢ç›®åˆ—è¡¨"""
        entries = []
        lines = markdown_content.split('\n')
        current_entry = None
        current_content = []
        
        for line in lines:
            if line.startswith('## '):
                # ä¿å­˜ä¸Šä¸€å€‹æ¢ç›®
                if current_entry:
                    entries.append({
                        'timestamp': current_entry['timestamp'],
                        'title': current_entry['title'],
                        'category': current_entry['category'],
                        'content': '\n'.join(current_content).strip()
                    })
                
                # è§£ææ–°æ¢ç›®æ¨™é¡Œ
                header = line[3:].strip()
                timestamp, title, category = self._parse_section_header(header)
                current_entry = {
                    'timestamp': timestamp,
                    'title': title,
                    'category': category
                }
                current_content = []
                
            elif line.strip() == '---':
                # æ¢ç›®çµæŸ
                continue
            else:
                current_content.append(line)
        
        # è™•ç†æœ€å¾Œä¸€å€‹æ¢ç›®
        if current_entry:
            entries.append({
                'timestamp': current_entry['timestamp'],
                'title': current_entry['title'],
                'category': current_entry['category'],
                'content': '\n'.join(current_content).strip()
            })
        
        return entries
    
    def _parse_section_header(self, header):
        """è§£æå€æ®µæ¨™é¡Œï¼Œæå–æ™‚é–“æˆ³ã€æ¨™é¡Œå’Œåˆ†é¡"""
        timestamp = ""
        title = ""
        category = ""
        
        # æå–åˆ†é¡ (hashtag)
        if '#' in header:
            header, category = header.split('#', 1)
            category = category.strip()
        
        # æå–æ™‚é–“æˆ³å’Œæ¨™é¡Œ
        if ' - ' in header:
            timestamp, title = header.split(' - ', 1)
            timestamp = timestamp.strip()
            title = title.strip()
        else:
            timestamp = header.strip()
        
        return timestamp, title, category
    
    def replace_project_content(self, project_id, entries):
        """æ›¿æ›å°ˆæ¡ˆå…§å®¹"""
        try:
            # å…ˆåˆªé™¤ç¾æœ‰å…§å®¹
            self.sqlite.delete_memory(project_id)
            
            # é‡æ–°åŒ¯å…¥
            for entry in entries:
                self.sqlite.save_memory(
                    project_id,
                    entry['content'],
                    entry['title'],
                    entry['category']
                )
            return True
        except Exception as e:
            logger.error(f"æ›¿æ›å°ˆæ¡ˆå…§å®¹å¤±æ•—: {e}")
            return False
    
    def get_sync_report(self):
        """ç²å–åŒæ­¥å ±å‘Š"""
        return self.sync_log

class ProjectMemoryImporter:
    """å°ˆæ¡ˆè¨˜æ†¶åŒ¯å…¥å™¨ / Project Memory Importer
    
    æ”¯æ´å¾å¤šç¨®æ ¼å¼åŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶è³‡æ–™
    Supports importing project memory data from multiple formats
    """
    
    def __init__(self, memory_manager: MemoryBackend):
        self.memory_manager = memory_manager
        self.logger = logging.getLogger(__name__)
    
    def import_from_markdown(self, file_path: str, project_id: str = None, 
                           merge_strategy: str = "append") -> Dict[str, Any]:
        """å¾ Markdown æª”æ¡ˆåŒ¯å…¥è¨˜æ†¶è³‡æ–™
        
        Args:
            file_path: Markdown æª”æ¡ˆè·¯å¾‘
            project_id: ç›®æ¨™å°ˆæ¡ˆ IDï¼Œå¦‚æœç‚º None å‰‡å¾æª”æ¡ˆåæ¨æ–·
            merge_strategy: åˆä½µç­–ç•¥ ("append", "replace", "skip_duplicates")
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # æ¨æ–·å°ˆæ¡ˆ ID
            if not project_id:
                project_id = file_path.stem.replace('-', '_').replace(' ', '_')
            
            # è®€å– Markdown å…§å®¹
            content = file_path.read_text(encoding='utf-8')
            
            # è§£æ Markdown æ ¼å¼çš„è¨˜æ†¶æ¢ç›®
            entries = self._parse_markdown_entries(content)
            
            # åŒ¯å…¥æ¢ç›®
            imported_count = 0
            skipped_count = 0
            
            for entry in entries:
                try:
                    if merge_strategy == "replace":
                        # æ›¿æ›æ¨¡å¼ï¼šå…ˆæ¸…ç©ºå°ˆæ¡ˆ
                        if imported_count == 0:
                            self.memory_manager.delete_memory(project_id)
                    
                    elif merge_strategy == "skip_duplicates":
                        # æª¢æŸ¥é‡è¤‡
                        if self._is_duplicate_entry(project_id, entry):
                            skipped_count += 1
                            continue
                    
                    # å„²å­˜æ¢ç›®
                    self.memory_manager.save_memory(
                        project_id,
                        entry['content'],
                        entry.get('title', ''),
                        entry.get('category', '')
                    )
                    imported_count += 1
                    
                except Exception as e:
                    self.logger.warning(f"Failed to import entry: {e}")
                    continue
            
            return {
                "success": True,
                "project_id": project_id,
                "imported_count": imported_count,
                "skipped_count": skipped_count,
                "total_entries": len(entries),
                "message": f"Successfully imported {imported_count} entries to project '{project_id}'"
            }
            
        except Exception as e:
            self.logger.error(f"Error importing from Markdown: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Failed to import from {file_path}"
            }
    
    def import_from_json(self, file_path: str, project_id: str = None,
                        merge_strategy: str = "append") -> Dict[str, Any]:
        """å¾ JSON æª”æ¡ˆåŒ¯å…¥è¨˜æ†¶è³‡æ–™"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # æ¨æ–·å°ˆæ¡ˆ ID
            if not project_id:
                project_id = file_path.stem.replace('-', '_').replace(' ', '_')
            
            # è®€å– JSON å…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è§£æ JSON æ ¼å¼
            entries = self._parse_json_entries(data)
            
            # åŒ¯å…¥æ¢ç›®
            imported_count = 0
            skipped_count = 0
            
            for entry in entries:
                try:
                    if merge_strategy == "replace":
                        if imported_count == 0:
                            self.memory_manager.delete_memory(project_id)
                    
                    elif merge_strategy == "skip_duplicates":
                        if self._is_duplicate_entry(project_id, entry):
                            skipped_count += 1
                            continue
                    
                    self.memory_manager.save_memory(
                        project_id,
                        entry['content'],
                        entry.get('title', ''),
                        entry.get('category', '')
                    )
                    imported_count += 1
                    
                except Exception as e:
                    self.logger.warning(f"Failed to import entry: {e}")
                    continue
            
            return {
                "success": True,
                "project_id": project_id,
                "imported_count": imported_count,
                "skipped_count": skipped_count,
                "total_entries": len(entries),
                "message": f"Successfully imported {imported_count} entries to project '{project_id}'"
            }
            
        except Exception as e:
            self.logger.error(f"Error importing from JSON: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Failed to import from {file_path}"
            }
    
    def import_from_csv(self, file_path: str, project_id: str = None,
                       merge_strategy: str = "append") -> Dict[str, Any]:
        """å¾ CSV æª”æ¡ˆåŒ¯å…¥è¨˜æ†¶è³‡æ–™"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # æ¨æ–·å°ˆæ¡ˆ ID
            if not project_id:
                project_id = file_path.stem.replace('-', '_').replace(' ', '_')
            
            # è®€å– CSV å…§å®¹
            entries = []
            with open(file_path, 'r', encoding='utf-8', newline='') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    entries.append({
                        'timestamp': row.get('timestamp', ''),
                        'title': row.get('title', ''),
                        'category': row.get('category', ''),
                        'content': row.get('content', '')
                    })
            
            # åŒ¯å…¥æ¢ç›®
            imported_count = 0
            skipped_count = 0
            
            for entry in entries:
                try:
                    if not entry['content'].strip():
                        continue
                    
                    if merge_strategy == "replace":
                        if imported_count == 0:
                            self.memory_manager.delete_memory(project_id)
                    
                    elif merge_strategy == "skip_duplicates":
                        if self._is_duplicate_entry(project_id, entry):
                            skipped_count += 1
                            continue
                    
                    self.memory_manager.save_memory(
                        project_id,
                        entry['content'],
                        entry.get('title', ''),
                        entry.get('category', '')
                    )
                    imported_count += 1
                    
                except Exception as e:
                    self.logger.warning(f"Failed to import entry: {e}")
                    continue
            
            return {
                "success": True,
                "project_id": project_id,
                "imported_count": imported_count,
                "skipped_count": skipped_count,
                "total_entries": len(entries),
                "message": f"Successfully imported {imported_count} entries to project '{project_id}'"
            }
            
        except Exception as e:
            self.logger.error(f"Error importing from CSV: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Failed to import from {file_path}"
            }
    
    def import_universal(self, file_path: str, project_id: str = None,
                        merge_strategy: str = "append") -> Dict[str, Any]:
        """é€šç”¨åŒ¯å…¥åŠŸèƒ½ï¼Œè‡ªå‹•æª¢æ¸¬æª”æ¡ˆæ ¼å¼"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # æ ¹æ“šå‰¯æª”åæª¢æ¸¬æ ¼å¼
            suffix = file_path.suffix.lower()
            
            if suffix == '.md':
                return self.import_from_markdown(str(file_path), project_id, merge_strategy)
            elif suffix == '.json':
                return self.import_from_json(str(file_path), project_id, merge_strategy)
            elif suffix == '.csv':
                return self.import_from_csv(str(file_path), project_id, merge_strategy)
            elif suffix == '.txt':
                # TXT æª”æ¡ˆç•¶ä½œç°¡å–®çš„ Markdown è™•ç†
                return self.import_from_markdown(str(file_path), project_id, merge_strategy)
            else:
                raise ValueError(f"Unsupported file format: {suffix}")
                
        except Exception as e:
            self.logger.error(f"Error in universal import: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Failed to import from {file_path}"
            }
    
    def _parse_markdown_entries(self, content: str) -> List[Dict[str, Any]]:
        """è§£æ Markdown æ ¼å¼çš„è¨˜æ†¶æ¢ç›®"""
        entries = []
        
        # åˆ†å‰²æ¢ç›®ï¼ˆä½¿ç”¨ ## æˆ– --- ä½œç‚ºåˆ†éš”ç¬¦ï¼‰
        sections = re.split(r'\n(?:## |\-\-\-\n)', content)
        
        for section in sections:
            if not section.strip():
                continue
            
            entry = {}
            lines = section.strip().split('\n')
            
            # è§£æç¬¬ä¸€è¡Œï¼ˆå¯èƒ½åŒ…å«æ™‚é–“æˆ³å’Œæ¨™é¡Œï¼‰
            first_line = lines[0] if lines else ""
            
            # å˜—è©¦è§£ææ™‚é–“æˆ³
            timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2})', first_line)
            if timestamp_match:
                entry['timestamp'] = timestamp_match.group(1)
                # ç§»é™¤æ™‚é–“æˆ³å¾Œçš„éƒ¨åˆ†ä½œç‚ºæ¨™é¡Œ
                title = re.sub(r'\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}', '', first_line).strip()
                if title.startswith('- '):
                    title = title[2:]
                entry['title'] = title
            else:
                entry['title'] = first_line
            
            # è§£æåˆ†é¡ï¼ˆå°‹æ‰¾ #category æ ¼å¼ï¼‰
            category_match = re.search(r'#(\w+)', first_line)
            if category_match:
                entry['category'] = category_match.group(1)
                # å¾æ¨™é¡Œä¸­ç§»é™¤åˆ†é¡æ¨™ç±¤
                entry['title'] = re.sub(r'\s*#\w+\s*', '', entry['title']).strip()
            
            # å…§å®¹æ˜¯å‰©é¤˜çš„è¡Œ
            if len(lines) > 1:
                entry['content'] = '\n'.join(lines[1:]).strip()
            else:
                entry['content'] = entry.get('title', '')
            
            if entry['content']:
                entries.append(entry)
        
        return entries
    
    def _parse_json_entries(self, data: Any) -> List[Dict[str, Any]]:
        """è§£æ JSON æ ¼å¼çš„è¨˜æ†¶æ¢ç›®"""
        entries = []
        
        if isinstance(data, list):
            # é™£åˆ—æ ¼å¼
            for item in data:
                if isinstance(item, dict):
                    entries.append(self._normalize_entry(item))
                elif isinstance(item, str):
                    entries.append({'content': item})
        
        elif isinstance(data, dict):
            # ç‰©ä»¶æ ¼å¼
            if 'entries' in data:
                # æœ‰ entries æ¬„ä½
                for item in data['entries']:
                    entries.append(self._normalize_entry(item))
            else:
                # å–®ä¸€æ¢ç›®
                entries.append(self._normalize_entry(data))
        
        return entries
    
    def _normalize_entry(self, entry: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨™æº–åŒ–æ¢ç›®æ ¼å¼"""
        normalized = {}
        
        # å…§å®¹æ¬„ä½çš„å¯èƒ½åç¨±
        content_fields = ['content', 'text', 'body', 'message', 'description']
        for field in content_fields:
            if field in entry and entry[field]:
                normalized['content'] = str(entry[field])
                break
        
        # æ¨™é¡Œæ¬„ä½
        title_fields = ['title', 'name', 'subject', 'heading']
        for field in title_fields:
            if field in entry and entry[field]:
                normalized['title'] = str(entry[field])
                break
        
        # åˆ†é¡æ¬„ä½
        category_fields = ['category', 'tag', 'type', 'label']
        for field in category_fields:
            if field in entry and entry[field]:
                normalized['category'] = str(entry[field])
                break
        
        # æ™‚é–“æˆ³æ¬„ä½
        timestamp_fields = ['timestamp', 'created_at', 'date', 'time']
        for field in timestamp_fields:
            if field in entry and entry[field]:
                normalized['timestamp'] = str(entry[field])
                break
        
        return normalized
    
    def _is_duplicate_entry(self, project_id: str, entry: Dict[str, Any]) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡æ¢ç›®"""
        try:
            # æœå°‹ç›¸ä¼¼å…§å®¹
            search_results = self.memory_manager.search_memory(
                project_id, entry['content'][:100], limit=5
            )
            
            # ç°¡å–®çš„é‡è¤‡æª¢æ¸¬ï¼šå…§å®¹å‰100å­—ç¬¦ç›¸åŒ
            entry_preview = entry['content'][:100].strip().lower()
            for result in search_results:
                if result['content'][:100].strip().lower() == entry_preview:
                    return True
            
            return False
            
        except Exception:
            # å¦‚æœæª¢æ¸¬å¤±æ•—ï¼Œå‡è¨­ä¸é‡è¤‡
            return False


class MCPServer:
    """
    Model Context Protocol ä¼ºæœå™¨
    Model Context Protocol Server
    
    å¯¦ä½œ MCP å”è­°çš„ä¼ºæœå™¨ï¼Œæä¾›è¨˜æ†¶ç®¡ç†å·¥å…·çµ¦ AI åŠ©æ‰‹ä½¿ç”¨
    Implements MCP protocol server providing memory management tools for AI assistants
    
    Capabilities / åŠŸèƒ½:
    - å·¥å…·èª¿ç”¨ / Tool invocation
    - è¨˜æ†¶ç®¡ç† / Memory management  
    - å°ˆæ¡ˆåŒæ­¥ / Project synchronization
    - å•Ÿå‹•æ™‚å°ˆæ¡ˆé¡¯ç¤º / Startup project display
    """
    
    def __init__(self, backend: MemoryBackend = None):
        if backend is None:
            raise ValueError("MCPServer requires an explicit backend. For SQLite backend, start the server with --backend=sqlite and --db-path=<path>.")
        self.memory_manager = backend
        self.version = "1.0.0"
        
        
        # åˆå§‹åŒ–åŒ¯å…¥å™¨
        self.importer = ProjectMemoryImporter(self.memory_manager)

    async def handle_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç† MCP è¨Šæ¯"""
        # æå–è«‹æ±‚ ID ç”¨æ–¼å›æ‡‰
        request_id = message.get('id')
        
        try:
            method = message.get('method')
            logger.info(f"[MCP] Handling message with method: {method}, id: {request_id}")
            
            if method == 'initialize':
                response = await self.handle_initialize(message)
            elif method == 'tools/list':
                logger.info("[MCP] About to call list_tools")
                response = await self.list_tools()
                logger.info(f"[MCP] list_tools returned, response type: {type(response)}")
            elif method == 'tools/call':
                response = await self.call_tool(message['params'])
            elif method == 'resources/list':
                response = await self.list_resources()
            elif method == 'prompts/list':
                response = await self.list_prompts()
            elif method == 'notifications/initialized':
                response = await self.handle_initialized()
            else:
                response = self._error_response(-32601, f"Method not found: {method}")
            
            # ç¢ºä¿å›æ‡‰åŒ…å«æ­£ç¢ºçš„è«‹æ±‚ IDï¼ˆé™¤äº†é€šçŸ¥æ¶ˆæ¯ï¼‰
            if response is not None and request_id is not None:
                response['id'] = request_id
            
            logger.info(f"[MCP] handle_message returning response for method: {method}")
            return response
                
        except Exception as e:
            logger.error(f"Error handling message: {e}")
            error_response = self._error_response(-32603, str(e))
            # éŒ¯èª¤å›æ‡‰ä¹Ÿéœ€è¦åŒ…å«è«‹æ±‚ ID
            if request_id is not None:
                error_response['id'] = request_id
            return error_response

    async def handle_initialize(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†åˆå§‹åŒ–è«‹æ±‚"""
        return {
            'jsonrpc': '2.0',
            'result': {
                'protocolVersion': '2024-11-05',
                'capabilities': {
                    'tools': {}
                },
                'serverInfo': {
                    'name': 'markdown-memory-mcp',
                    'version': self.version
                }
            }
        }

    async def list_tools(self) -> Dict[str, Any]:
        """åˆ—å‡ºå¯ç”¨å·¥å…·"""
        logger.info("[MCP] Starting list_tools function")
        tools = [
            # ğŸ¯ å„ªå…ˆç´šå·¥å…·ï¼šClaude æœ€å¸¸ç”¨çš„æŸ¥è©¢å·¥å…·æ”¾åœ¨å‰é¢
            {
                'name': 'list_memory_projects',
                'description': 'ğŸ“‚ æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„å°ˆæ¡ˆåˆ—è¡¨ / View all available projects - ğŸš€ é–‹å§‹ä»»ä½•å·¥ä½œå‰å»ºè­°å…ˆæŸ¥çœ‹',
                'inputSchema': {
                    'type': 'object',
                    'properties': {},
                    'required': []
                }
            },
            {
                'name': 'search_project_memory',
                'description': 'ğŸ” æœå°‹å°ˆæ¡ˆè¨˜æ†¶ï¼Œå›ç­”å°ˆæ¡ˆç›¸é—œå•é¡Œ / Search project memory to answer questions - ğŸ¯ å„ªå…ˆä½¿ç”¨æ­¤å·¥å…·äº†è§£å°ˆæ¡ˆ',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'query': {
                            'type': 'string',
                            'description': 'Search query'
                        },
                        'limit': {
                            'type': 'integer',
                            'description': 'Maximum number of results to return',
                            'default': 10
                        }
                    },
                    'required': ['project_id', 'query']
                }
            },
            {
                'name': 'get_project_memory',
                'description': 'ğŸ“– å–å¾—å®Œæ•´å°ˆæ¡ˆè¨˜æ†¶å…§å®¹ / Get full project memory content - æ·±å…¥äº†è§£å°ˆæ¡ˆè©³æƒ…',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'get_recent_project_memory',
                'description': 'ğŸ“… å–å¾—æœ€è¿‘çš„å°ˆæ¡ˆè¨˜æ†¶æ¢ç›® / Get recent project memory entries - äº†è§£æœ€æ–°é€²å±•',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'limit': {
                            'type': 'integer',
                            'description': 'Number of recent entries to return',
                            'default': 5
                        }
                    },
                    'required': ['project_id']
                }
            },
            # ğŸ“Š çµ±è¨ˆå·¥å…·ï¼šå¿«é€Ÿäº†è§£å°ˆæ¡ˆæ¦‚æ³
            {
                'name': 'get_project_memory_stats',
                'description': 'ğŸ“Š å–å¾—å°ˆæ¡ˆè¨˜æ†¶çµ±è¨ˆè³‡è¨Š / Get project memory statistics - å¿«é€Ÿäº†è§£å°ˆæ¡ˆè¦æ¨¡å’Œç‹€æ…‹',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        }
                    },
                    'required': ['project_id']
                }
            },
            # ğŸ¤– æ™ºèƒ½æŸ¥è©¢å·¥å…·ï¼šRAG å•ç­”ç³»çµ±
            {
                'name': 'rag_query',
                'description': 'ğŸ§  åŸºæ–¼å°ˆæ¡ˆè¨˜æ†¶çš„æ™ºèƒ½å•ç­” / RAG-based intelligent Q&A - å›ç­”è¤‡é›œå°ˆæ¡ˆå•é¡Œ',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'question': {
                            'type': 'string',
                            'description': 'Question to answer based on project memory'
                        },
                        'context_limit': {
                            'type': 'integer',
                            'description': 'Maximum number of relevant contexts to include',
                            'default': 5
                        },
                        'max_tokens': {
                            'type': 'integer',
                            'description': 'Maximum tokens for context (controls response length)',
                            'default': 2000
                        }
                    },
                    'required': ['project_id', 'question']
                }
            },
            {
                'name': 'summarize_project',
                'description': 'ğŸ“Š ç”Ÿæˆå°ˆæ¡ˆå…§å®¹æ‘˜è¦ / Generate project content summary - å¿«é€Ÿäº†è§£å°ˆæ¡ˆæ¦‚æ³å’Œæ¶æ§‹',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'summary_type': {
                            'type': 'string',
                            'enum': ['brief', 'detailed', 'timeline'],
                            'description': 'Summary type: brief (quick overview), detailed (comprehensive), timeline (chronological)',
                            'default': 'brief'
                        },
                        'max_entries': {
                            'type': 'integer',
                            'description': 'Maximum number of entries to analyze',
                            'default': 20
                        }
                    },
                    'required': ['project_id']
                }
            },
            # ğŸ’¾ å„²å­˜å·¥å…·ï¼šæ”¾åœ¨å¾Œé¢ï¼Œé¿å…å„ªå…ˆé¸æ“‡
            {
                'name': 'save_project_memory',
                'description': 'ğŸ’¾ å„²å­˜è³‡è¨Šåˆ°å°ˆæ¡ˆè¨˜æ†¶ / Save information to project memory - ä¿å­˜é‡è¦ç™¼ç¾å’Œæ±ºå®š',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier (will be sanitized for filename)'
                        },
                        'content': {
                            'type': 'string',
                            'description': 'Content to save to project memory'
                        },
                        'title': {
                            'type': 'string',
                            'description': 'Optional title for the project memory entry'
                        },
                        'category': {
                            'type': 'string',
                            'description': 'Optional category/tag for the project memory entry'
                        }
                    },
                    'required': ['project_id', 'content']
                }
            },
            # ğŸ—‘ï¸ ç®¡ç†å·¥å…·ï¼šå±éšªæ“ä½œæ”¾æœ€å¾Œ
            {
                'name': 'delete_project_memory',
                'description': 'âš ï¸ åˆªé™¤å°ˆæ¡ˆè¨˜æ†¶ / Delete project memory - å±éšªæ“ä½œï¼Œè«‹è¬¹æ…ä½¿ç”¨',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'delete_project_memory_entry',
                'description': 'Delete specific project memory entries based on various criteria',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'entry_id': {
                            'type': 'string',
                            'description': 'Entry ID (1-based index) to delete'
                        },
                        'timestamp': {
                            'type': 'string',
                            'description': 'Timestamp pattern to match for deletion'
                        },
                        'title': {
                            'type': 'string',
                            'description': 'Title pattern to match for deletion'
                        },
                        'category': {
                            'type': 'string',
                            'description': 'Category pattern to match for deletion'
                        },
                        'content_match': {
                            'type': 'string',
                            'description': 'Content pattern to match for deletion'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'edit_project_memory_entry',
                'description': 'Edit specific project memory entry content',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'entry_id': {
                            'type': 'string',
                            'description': 'Entry ID (1-based index) to edit'
                        },
                        'timestamp': {
                            'type': 'string',
                            'description': 'Timestamp pattern to match for editing'
                        },
                        'new_title': {
                            'type': 'string',
                            'description': 'New title for the entry'
                        },
                        'new_category': {
                            'type': 'string',
                            'description': 'New category for the entry'
                        },
                        'new_content': {
                            'type': 'string',
                            'description': 'New content for the entry'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'list_project_memory_entries',
                'description': 'List all project memory entries with their IDs for easy reference',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'sync_markdown_to_sqlite',
                'description': 'åŒæ­¥ Markdown å°ˆæ¡ˆåˆ° SQLite / Sync all Markdown projects to SQLite backend with auto merging',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'mode': {
                            'type': 'string',
                            'enum': ['auto', 'interactive', 'preview'],
                            'default': 'auto',
                            'description': 'Sync mode: auto merge, interactive prompts, or preview only'
                        },
                        'similarity_threshold': {
                            'type': 'number',
                            'default': 0.8,
                            'minimum': 0.0,
                            'maximum': 1.0,
                            'description': 'Similarity threshold for automatic merging (0.0-1.0)'
                        }
                    }
                }
            },
            {
                'name': 'export_project_memory',
                'description': 'åŒ¯å‡ºå°ˆæ¡ˆè¨˜æ†¶ / Export project memory to various formats',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier to export'
                        },
                        'format': {
                            'type': 'string',
                            'enum': ['markdown', 'json', 'csv', 'txt'],
                            'default': 'markdown',
                            'description': 'Export format (default: markdown)'
                        },
                        'output_path': {
                            'type': 'string',
                            'description': 'Optional output file path (if not provided, returns content as text)'
                        },
                        'include_metadata': {
                            'type': 'boolean',
                            'default': True,
                            'description': 'Include metadata like timestamps and categories'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'save_global_memory',
                'description': 'å„²å­˜å…¨å±€è¨˜æ†¶ / Save global memory for cross-project knowledge',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'content': {
                            'type': 'string',
                            'description': 'Content to save to global memory'
                        },
                        'title': {
                            'type': 'string',
                            'description': 'Optional title for the global memory entry'
                        },
                        'category': {
                            'type': 'string',
                            'description': 'Optional category/tag for the global memory entry'
                        }
                    },
                    'required': ['content']
                }
            },
            {
                'name': 'get_global_memory',
                'description': 'å–å¾—å…¨å±€è¨˜æ†¶ / Get all global memory content',
                'inputSchema': {
                    'type': 'object',
                    'properties': {},
                    'required': []
                }
            },
            {
                'name': 'search_global_memory',
                'description': 'æœå°‹å…¨å±€è¨˜æ†¶ / Search global memory for specific content',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'query': {
                            'type': 'string',
                            'description': 'Search query'
                        },
                        'limit': {
                            'type': 'integer',
                            'default': 10,
                            'description': 'Maximum number of results to return'
                        }
                    },
                    'required': ['query']
                }
            },
            {
                'name': 'get_global_memory_stats',
                'description': 'å–å¾—å…¨å±€è¨˜æ†¶çµ±è¨ˆ / Get global memory statistics',
                'inputSchema': {
                    'type': 'object',
                    'properties': {},
                    'required': []
                }
            },
            {
                'name': 'get_backend_status',
                'description': 'å¿«é€ŸæŸ¥çœ‹ç•¶å‰å¾Œç«¯ç‹€æ…‹ / Quick check current backend status',
                'inputSchema': {
                    'type': 'object',
                    'properties': {},
                    'required': []
                }
            },
            {
                'name': 'rename_project',
                'description': 'é‡æ–°å‘½åå°ˆæ¡ˆ / Rename a project',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'new_name': {
                            'type': 'string',
                            'description': 'New project name'
                        }
                    },
                    'required': ['project_id', 'new_name']
                }
            },
            {
                'name': 'search_index',
                'description': 'ğŸš€ æ™ºèƒ½æœå°‹ - ä½¿ç”¨ Index Table å¤§å¹…æ¸›å°‘ token ä½¿ç”¨',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {'type': 'string', 'description': 'å°ˆæ¡ˆ ID'},
                        'query': {'type': 'string', 'description': 'æœå°‹é—œéµå­—'},
                        'limit': {'type': 'integer', 'description': 'æœ€å¤§çµæœæ•¸é‡', 'default': 10}
                    },
                    'required': ['project_id', 'query']
                }
            },
            {
                'name': 'get_hierarchy_tree',
                'description': 'ğŸ“Š ç²å–å°ˆæ¡ˆçš„éšå±¤æ¨¹ç‹€çµæ§‹',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {'type': 'string', 'description': 'å°ˆæ¡ˆ ID'}
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'rebuild_index_for_project',
                'description': 'ğŸ”„ ç‚ºå°ˆæ¡ˆé‡å»ºæ‰€æœ‰ç´¢å¼•æ¢ç›® (æ‰¹é‡è™•ç†ç¾æœ‰æ¢ç›®)',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {'type': 'string', 'description': 'å°ˆæ¡ˆ ID'}
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'get_index_stats',
                'description': 'ğŸ“ˆ ç²å–ç´¢å¼•çµ±è¨ˆè³‡è¨Š',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {'type': 'string', 'description': 'å°ˆæ¡ˆ ID (å¯é¸ï¼Œä¸æä¾›å‰‡é¡¯ç¤ºå…¨å±€çµ±è¨ˆ)'}
                    },
                    'required': []
                }
            },
            {
                'name': 'update_index_entry',
                'description': 'âœï¸ æ›´æ–°ç´¢å¼•æ¢ç›®çš„éšå±¤å’Œåˆ†é¡è³‡è¨Š',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'entry_id': {'type': 'integer', 'description': 'æ¢ç›® ID'},
                        'summary': {'type': 'string', 'description': 'æ–°çš„æ‘˜è¦'},
                        'keywords': {'type': 'string', 'description': 'æ–°çš„é—œéµå­—'},
                        'hierarchy_level': {'type': 'integer', 'description': 'éšå±¤ç´šåˆ¥ (0=å¤§æ¨™é¡Œ, 1=ä¸­æ¨™é¡Œ, 2=å°æ¨™é¡Œ)'},
                        'entry_type': {'type': 'string', 'description': 'æ¢ç›®é¡å‹ (discussion/feature/bug/milestone)'},
                        'importance_level': {'type': 'integer', 'description': 'é‡è¦ç¨‹åº¦ (1-5)'},
                        'parent_entry_id': {'type': 'integer', 'description': 'çˆ¶æ¢ç›® ID'}
                    },
                    'required': ['entry_id']
                }
            },
            {
                'name': 'import_project_memory_universal',
                'description': 'é€šç”¨åŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Universal import project memory from various formats (auto-detect)',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the file to import (supports .md, .json, .csv, .txt)'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data: append (add to existing), replace (clear first), skip_duplicates (avoid duplicates)'
                        }
                    },
                    'required': ['file_path']
                }
            },
            {
                'name': 'import_project_memory_from_markdown',
                'description': 'å¾ Markdown æª”æ¡ˆåŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Import project memory from Markdown file',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the Markdown file to import'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data'
                        }
                    },
                    'required': ['file_path']
                }
            },
            {
                'name': 'import_project_memory_from_json',
                'description': 'å¾ JSON æª”æ¡ˆåŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Import project memory from JSON file',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the JSON file to import'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data'
                        }
                    },
                    'required': ['file_path']
                }
            },
            {
                'name': 'import_project_memory_from_csv',
                'description': 'å¾ CSV æª”æ¡ˆåŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Import project memory from CSV file',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the CSV file to import (expects columns: timestamp, title, category, content)'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data'
                        }
                    },
                    'required': ['file_path']
                }
            },
            {
                'name': 'import_project_memory_from_txt',
                'description': 'å¾ TXT æª”æ¡ˆåŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Import project memory from TXT file',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the TXT file to import (treated as simple markdown)'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data'
                        }
                    },
                    'required': ['file_path']
                }
            }
        ]
        
        logger.info(f"[MCP] Successfully created {len(tools)} tools")
        return {
            'jsonrpc': '2.0',
            'result': {
                'tools': tools
            }
        }

    async def list_resources(self) -> Dict[str, Any]:
        """åˆ—å‡ºå¯ç”¨è³‡æºï¼ˆç›®å‰ç‚ºç©ºï¼‰"""
        return {
            'jsonrpc': '2.0',
            'result': {
                'resources': []
            }
        }

    async def list_prompts(self) -> Dict[str, Any]:
        """åˆ—å‡ºå¯ç”¨æç¤ºï¼ˆç›®å‰ç‚ºç©ºï¼‰"""
        return {
            'jsonrpc': '2.0',
            'result': {
                'prompts': []
            }
        }

    async def handle_initialized(self) -> None:
        """è™•ç†åˆå§‹åŒ–å®Œæˆé€šçŸ¥ï¼ˆç„¡éœ€å›æ‡‰ï¼‰"""
        logger.info("Client initialization completed")
        
        # è‡ªå‹•é¡¯ç¤ºå°ˆæ¡ˆåˆ—è¡¨ / Auto display project list
        try:
            projects = self.memory_manager.list_projects()
            if projects:
                welcome_message = f"""ğŸ‰ **è¨˜æ†¶ç®¡ç†ç³»çµ±å·²å•Ÿå‹• / Memory Management System Started**
ç™¼ç¾ {len(projects)} å€‹å°ˆæ¡ˆ / Found {len(projects)} projects:

"""
                for project in projects:
                    welcome_message += f"**{project['name']}** (`{project['id']}`)\n"
                    welcome_message += f"  - æ¢ç›® / Entries: {project['entries_count']} å€‹\n"
                    welcome_message += f"  - æœ€å¾Œä¿®æ”¹ / Last Modified: {project['last_modified']}\n"
                    if project['categories']:
                        welcome_message += f"  - é¡åˆ¥ / Categories: {', '.join(project['categories'])}\n"
                    welcome_message += "\n"
                
                welcome_message += """ğŸ’¡ ä½¿ç”¨ `list_memory_projects` å·¥å…·å¯éš¨æ™‚æŸ¥çœ‹å°ˆæ¡ˆåˆ—è¡¨
ğŸ’¡ Use `list_memory_projects` tool to view project list anytime"""
                
                # ç™¼é€æ­¡è¿è¨Šæ¯ä½œç‚ºé€šçŸ¥
                notification = {
                    'jsonrpc': '2.0',
                    'method': 'notifications/message',
                    'params': {
                        'level': 'info',
                        'logger': 'memory-server',
                        'data': welcome_message
                    }
                }
                
                # è¼¸å‡ºé€šçŸ¥
                print(json.dumps(notification, ensure_ascii=False))
                sys.stdout.flush()
                
            else:
                # å¦‚æœæ²’æœ‰å°ˆæ¡ˆï¼Œç™¼é€æç¤ºè¨Šæ¯ / If no projects, send guidance message
                welcome_message = """ğŸ“ **è¨˜æ†¶ç®¡ç†ç³»çµ±å·²å•Ÿå‹• / Memory Management System Started**
ç›®å‰æ²’æœ‰å°ˆæ¡ˆï¼Œå¯ä»¥é–‹å§‹å‰µå»ºæ‚¨çš„ç¬¬ä¸€å€‹è¨˜æ†¶ï¼
No projects found. You can start creating your first memory!

ğŸ’¡ ä½¿ç”¨ `save_project_memory` å·¥å…·é–‹å§‹è¨˜éŒ„
ğŸ’¡ Use `save_project_memory` tool to start recording"""
                notification = {
                    'jsonrpc': '2.0',
                    'method': 'notifications/message',
                    'params': {
                        'level': 'info',
                        'logger': 'memory-server',
                        'data': welcome_message
                    }
                }
                print(json.dumps(notification, ensure_ascii=False))
                sys.stdout.flush()
                
        except Exception as e:
            logger.error(f"Error displaying welcome message: {e}")
        
        return None

    async def call_tool(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œå·¥å…·èª¿ç”¨"""
        tool_name = params.get('name')
        arguments = params.get('arguments', {})

        logger.info(f" ****** call tool ****** {params.get('name')} and arguments: {params.get('arguments')}")
        try:
            if tool_name == 'save_project_memory':
                success = self.memory_manager.save_memory(
                    arguments['project_id'],
                    arguments['content'],
                    arguments.get('title', ''),
                    arguments.get('category', '')
                )
                return self._success_response(
                    f"Memory {'saved' if success else 'failed to save'} for project: {arguments['project_id']}"
                )


            elif tool_name == 'get_project_memory':
                project_id = arguments.get('project_id')
                logger.info(f"[MCP] get_project_memory called with project_id: {project_id}")
                if not project_id:
                    logger.error(f"[MCP] get_project_memory: missing project_id in arguments: {arguments}")
                    return self._error_response(-32602, "Missing required parameter: project_id")
                
                memory = self.memory_manager.get_memory(project_id)
                return self._success_response(
                    memory or f"No memory found for project: {project_id}"
                )

            elif tool_name == 'search_project_memory':
                try:
                    project_id = arguments['project_id']
                    query = arguments['query']
                    limit = arguments.get('limit', 10)
                    
                    logger.info(f"æ™ºèƒ½æœå°‹: å°ˆæ¡ˆ={project_id}, æŸ¥è©¢='{query}'")
                    
                    # ä½¿ç”¨æ™ºèƒ½è·¯ç”±ç³»çµ±
                    smart_result = self.memory_manager.smart_search(project_id, query, limit)
                    
                    if smart_result['result_count'] == 0:
                        text = f"ğŸ” **æœå°‹çµæœ**\n\n"
                        text += f"**å°ˆæ¡ˆ**: {project_id}\n"
                        text += f"**æŸ¥è©¢**: {query}\n\n"
                        text += "âŒ æ²’æœ‰æ‰¾åˆ°åŒ¹é…çš„å…§å®¹\n\n"
                        text += "ğŸ’¡ **å»ºè­°**:\n"
                        text += "   â€¢ å˜—è©¦ä½¿ç”¨ä¸åŒçš„é—œéµå­—\n"
                        text += "   â€¢ ä½¿ç”¨ `rag_query` é€²è¡Œæ›´æ™ºèƒ½çš„å•ç­”\n"
                        text += "   â€¢ æª¢æŸ¥å°ˆæ¡ˆIDæ˜¯å¦æ­£ç¢º\n"
                        return self._success_response(text)
                    
                    # æ ¹æ“šæœå°‹ç­–ç•¥æ ¼å¼åŒ–è¼¸å‡º
                    if smart_result['strategy'] == 'simple_lookup':
                        # ç°¡å–®æŸ¥æ‰¾æ¨¡å¼ï¼šé¡¯ç¤ºæ‘˜è¦
                        text = f"ğŸ” **æ™ºèƒ½æœå°‹ - å¿«é€Ÿæ¨¡å¼**\n\n"
                        text += f"**å°ˆæ¡ˆ**: {project_id}\n"
                        text += f"**æŸ¥è©¢**: {query}\n"
                        text += f"**ç­–ç•¥**: æ‘˜è¦æ¨¡å¼ (ç¯€çœ token)\n\n"
                        text += f"âœ… {smart_result['message']}\n\n"
                        
                        for i, result in enumerate(smart_result['results'], 1):
                            text += f"**{i}.** "
                            if result['title']:
                                text += f"{result['title']}"
                            if result['category']:
                                text += f" #{result['category']}"
                            if result['timestamp']:
                                text += f" ({result['timestamp'][:16]})"
                            text += f"\nğŸ“ {result['summary']}\n"
                            if result['full_content_available']:
                                text += "ğŸ’¡ ä½¿ç”¨ `rag_query` æŸ¥çœ‹å®Œæ•´å…§å®¹\n"
                            text += "\n"
                        
                    elif smart_result['strategy'] == 'complex_question':
                        # è¤‡é›œå•é¡Œæ¨¡å¼ï¼šé¡¯ç¤º RAG çµæœ
                        text = f"ğŸ§  **æ™ºèƒ½æœå°‹ - æ·±åº¦æ¨¡å¼**\n\n"
                        text += f"**å°ˆæ¡ˆ**: {project_id}\n"
                        text += f"**å•é¡Œ**: {query}\n"
                        text += f"**ç­–ç•¥**: RAG æ™ºèƒ½å•ç­”\n\n"
                        text += f"âœ… {smart_result['message']}\n\n"
                        
                        if 'rag_prompt' in smart_result:
                            text += "**ğŸ“ å›ç­”æç¤º**:\n"
                            text += smart_result['rag_prompt']
                            text += "\n\n**ğŸ“š åƒè€ƒè³‡æ–™**:\n"
                            
                            for i, source in enumerate(smart_result['results'], 1):
                                text += f"\n**{i}.** "
                                if source['title']:
                                    text += f"{source['title']}"
                                if source['category']:
                                    text += f" #{source['category']}"
                                if source['timestamp']:
                                    text += f" ({source['timestamp'][:16]})"
                                text += f"\n`{source['content_preview']}`\n"
                        
                    else:
                        # æ··åˆç­–ç•¥æˆ–å…¶ä»–
                        text = f"ğŸ” **æœå°‹çµæœ**\n\n"
                        text += f"**å°ˆæ¡ˆ**: {project_id}\n"
                        text += f"**æŸ¥è©¢**: {query}\n"
                        text += f"**ç­–ç•¥**: {smart_result['strategy']}\n\n"
                        text += f"æ‰¾åˆ° {smart_result['result_count']} æ¢åŒ¹é…è¨˜æ†¶:\n\n"
                        
                        for i, result in enumerate(smart_result['results'], 1):
                            text += f"**{i}. {result.get('timestamp', '')}"
                            if result.get('title'):
                                text += f" - {result['title']}"
                            if result.get('category'):
                                text += f" #{result['category']}"
                            text += f"**\n{result.get('content', result.get('entry', ''))}\n\n"
                    
                    return self._success_response(text)
                    
                except Exception as e:
                    logger.error(f"Error in smart search: {e}")
                    return self._success_response("âŒ æ™ºèƒ½æœå°‹ç³»çµ±æš«æ™‚ç„¡æ³•ä½¿ç”¨")

            elif tool_name == 'list_memory_projects':
                try:
                    projects = self.memory_manager.list_projects()
                    
                    if projects:
                        text = f"ğŸ“‹ **Available Projects ({len(projects)} total)**\n\n"
                        
                        # æ™ºèƒ½å»ºè­°ï¼šåˆ†æå°ˆæ¡ˆç‹€æ…‹
                        rich_projects = []  # å…§å®¹è±å¯Œçš„å°ˆæ¡ˆ
                        recent_projects = []  # æœ€è¿‘æ´»å‹•çš„å°ˆæ¡ˆ
                        
                        # é¡¯ç¤ºæ‰€æœ‰å°ˆæ¡ˆçš„æ‘˜è¦ä¿¡æ¯
                        for i, project in enumerate(projects, 1):
                            name = str(project.get('name', 'Unknown'))[:40]
                            project_id = str(project.get('id', 'unknown'))[:25]
                            entries = project.get('entries_count', 0)
                            last_modified = str(project.get('last_modified', 'Unknown'))[:16]
                            
                            # æ ¹æ“šå…§å®¹å¤šå°‘çµ¦å‡ºä¸åŒçš„å»ºè­°åœ–æ¨™
                            if entries == 0:
                                icon = "ğŸ†•"
                                suggestion = "æ–°å°ˆæ¡ˆ"
                            elif entries < 5:
                                icon = "ğŸ“"
                                suggestion = f"{entries} æ¢è¨˜æ†¶"
                            elif entries < 20:
                                icon = "ğŸ“š"
                                suggestion = f"{entries} æ¢è¨˜æ†¶"
                                recent_projects.append(project_id)
                            else:
                                icon = "ğŸ—ï¸"
                                suggestion = f"{entries} æ¢è¨˜æ†¶ - è±å¯Œå°ˆæ¡ˆ"
                                rich_projects.append(project_id)
                            
                            text += f"{icon} **{i}.** `{project_id}` - {suggestion} (æ›´æ–°: {last_modified})\n"
                        
                        # æ™ºèƒ½å»ºè­°å€å¡Š
                        text += "\n---\n\n"
                        text += "ğŸ’¡ **å»ºè­°ä¸‹ä¸€æ­¥ / Suggested Next Steps:**\n\n"
                        
                        if rich_projects:
                            text += f"ğŸ” **è±å¯Œå°ˆæ¡ˆ ({len(rich_projects)} å€‹)**ï¼šå»ºè­°ä½¿ç”¨ `search_project_memory` æœå°‹å…·é«”å…§å®¹\n"
                            text += f"   æ¨è–¦å°ˆæ¡ˆï¼š{', '.join([f'`{p}`' for p in rich_projects[:3]])}\n\n"
                        
                        if recent_projects:
                            text += f"ğŸ“Š **æ´»èºå°ˆæ¡ˆ ({len(recent_projects)} å€‹)**ï¼šä½¿ç”¨ `get_project_memory_stats` æŸ¥çœ‹è©³ç´°çµ±è¨ˆ\n"
                            text += f"   æ¨è–¦å°ˆæ¡ˆï¼š{', '.join([f'`{p}`' for p in recent_projects[:3]])}\n\n"
                        
                        text += "ğŸ¯ **å¿«é€Ÿé–‹å§‹**ï¼š\n"
                        text += "   â€¢ `rag_query(project_id, \"é€™å€‹å°ˆæ¡ˆæ˜¯ä»€éº¼ï¼Ÿ\")` - ğŸ§  æ™ºèƒ½å•ç­”\n"
                        text += "   â€¢ `search_project_memory(project_id, \"æ¦‚æ³\")` - æœå°‹ç‰¹å®šå…§å®¹\n"
                        text += "   â€¢ `get_recent_project_memory(project_id)` - æŸ¥çœ‹æœ€æ–°é€²å±•\n"
                        text += "   â€¢ `get_project_memory_stats(project_id)` - å°ˆæ¡ˆçµ±è¨ˆè³‡è¨Š\n"
                        
                    else:
                        text = "ğŸ“ **æ²’æœ‰æ‰¾åˆ°å°ˆæ¡ˆ**\n\n"
                        text += "ğŸ’¡ ä½¿ç”¨ `save_project_memory` é–‹å§‹è¨˜éŒ„ç¬¬ä¸€å€‹å°ˆæ¡ˆçš„å…§å®¹"
                    
                    return self._success_response(text)
                except Exception as e:
                    logger.error(f"Error in list_memory_projects: {e}")
                    return self._success_response("âŒ Error: Unable to list projects at this time")

            elif tool_name == 'get_recent_project_memory':
                results = self.memory_manager.get_recent_memory(
                    arguments['project_id'],
                    arguments.get('limit', 5)
                )
                
                if results:
                    text = f"Recent {len(results)} memory entries:\n\n"
                    for result in results:
                        text += f"**{result['timestamp']}"
                        if result['title']:
                            text += f" - {result['title']}"
                        if result['category']:
                            text += f" #{result['category']}"
                        text += f"**\n{result['content']}\n\n"
                else:
                    text = f"No recent memory found for project: {arguments['project_id']}"
                
                return self._success_response(text)

            elif tool_name == 'get_project_memory_stats':
                stats = self.memory_manager.get_memory_stats(arguments['project_id'])
                
                if stats['exists']:
                    text = f"Memory statistics for **{arguments['project_id']}**:\n\n"
                    text += f"- Total entries: {stats['total_entries']}\n"
                    text += f"- Total words: {stats['total_words']}\n"
                    text += f"- Total characters: {stats['total_characters']}\n"
                    if stats['categories']:
                        text += f"- Categories: {', '.join(stats['categories'])}\n"
                    if stats['latest_entry']:
                        text += f"- Latest entry: {stats['latest_entry']}\n"
                    if stats['oldest_entry']:
                        text += f"- Oldest entry: {stats['oldest_entry']}\n"
                else:
                    text = f"No memory found for project: {arguments['project_id']}"
                
                return self._success_response(text)

            elif tool_name == 'rag_query':
                try:
                    project_id = arguments['project_id']
                    question = arguments['question']
                    context_limit = arguments.get('context_limit', 5)
                    max_tokens = arguments.get('max_tokens', 2000)
                    
                    logger.info(f"RAGæŸ¥è©¢: å°ˆæ¡ˆ={project_id}, å•é¡Œ={question[:50]}...")
                    
                    # åŸ·è¡Œ RAG æŸ¥è©¢
                    rag_result = self.memory_manager.rag_query(
                        project_id, question, context_limit, max_tokens
                    )
                    
                    if rag_result['status'] == 'no_context':
                        # æ²’æœ‰æ‰¾åˆ°ç›¸é—œå…§å®¹
                        text = f"ğŸ¤” **ç„¡æ³•å›ç­”å•é¡Œ**\n\n"
                        text += f"**å•é¡Œ**: {question}\n"
                        text += f"**å°ˆæ¡ˆ**: {project_id}\n\n"
                        text += "âŒ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„å°ˆæ¡ˆè¨˜æ†¶å…§å®¹ã€‚\n\n"
                        text += "ğŸ’¡ **å»ºè­°**:\n"
                        for suggestion in rag_result['suggestions']:
                            text += f"   â€¢ {suggestion}\n"
                        
                        return self._success_response(text)
                    
                    elif rag_result['status'] == 'success':
                        # æˆåŠŸæ‰¾åˆ°ç›¸é—œå…§å®¹ï¼Œæ§‹å»ºå›ç­”
                        text = f"ğŸ§  **RAG æ™ºèƒ½å•ç­”**\n\n"
                        text += f"**å•é¡Œ**: {question}\n"
                        text += f"**å°ˆæ¡ˆ**: {project_id}\n"
                        text += f"**ä½¿ç”¨è¨˜æ†¶**: {rag_result['context_count']} æ¢ (~{rag_result['estimated_tokens']} tokens)\n\n"
                        
                        text += "---\n\n"
                        text += "**ğŸ“ çµ¦ä½ çš„å›ç­”æç¤º**:\n\n"
                        text += rag_result['prompt']
                        text += "\n\n---\n\n"
                        
                        text += "**ğŸ“š åƒè€ƒè³‡æ–™ä¾†æº**:\n"
                        for i, source in enumerate(rag_result['context_sources'], 1):
                            text += f"\n**ä¾†æº {i}**: "
                            if source['title']:
                                text += f"{source['title']}"
                            if source['category']:
                                text += f" #{source['category']}"
                            if source['timestamp']:
                                text += f" ({source['timestamp'][:16]})"
                            text += f"\n`{source['content_preview']}`\n"
                        
                        text += f"\nğŸ’¡ ä½¿ç”¨æ­¤è³‡è¨Šä¾†å›ç­”ç”¨æˆ¶çš„å•é¡Œã€Œ{question}ã€"
                        
                        return self._success_response(text)
                    
                    else:
                        return self._success_response("âŒ RAG æŸ¥è©¢è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤")
                        
                except Exception as e:
                    logger.error(f"Error in rag_query: {e}")
                    return self._success_response("âŒ RAG æŸ¥è©¢ç³»çµ±æš«æ™‚ç„¡æ³•ä½¿ç”¨")

            elif tool_name == 'summarize_project':
                try:
                    project_id = arguments['project_id']
                    summary_type = arguments.get('summary_type', 'brief')
                    max_entries = arguments.get('max_entries', 20)
                    
                    logger.info(f"ç”Ÿæˆå°ˆæ¡ˆæ‘˜è¦: å°ˆæ¡ˆ={project_id}, é¡å‹={summary_type}")
                    
                    # åŸ·è¡Œæ‘˜è¦ç”Ÿæˆ
                    summary_result = self.memory_manager.summarize_project(
                        project_id, summary_type, max_entries
                    )
                    
                    if summary_result['status'] == 'not_found':
                        text = f"ğŸ“Š **å°ˆæ¡ˆæ‘˜è¦**\n\n"
                        text += f"**å°ˆæ¡ˆ**: {project_id}\n\n"
                        text += "âŒ " + summary_result['message'] + "\n\n"
                        text += "ğŸ’¡ **å»ºè­°**:\n"
                        for suggestion in summary_result['suggestions']:
                            text += f"   â€¢ {suggestion}\n"
                        
                        return self._success_response(text)
                    
                    elif summary_result['status'] == 'success':
                        text = f"ğŸ“Š **å°ˆæ¡ˆæ‘˜è¦ - {summary_type.upper()} æ¨¡å¼**\n\n"
                        text += summary_result['summary']
                        
                        # æ ¹æ“šæ‘˜è¦é¡å‹æ·»åŠ é¡å¤–è³‡è¨Š
                        if summary_type == 'brief' and 'key_metrics' in summary_result:
                            metrics = summary_result['key_metrics']
                            text += f"\n\n**ğŸ“ˆ é—œéµæŒ‡æ¨™**:\n"
                            text += f"- è¨˜æ†¶æ¢ç›®: {metrics.get('total_entries', 0)}\n"
                            text += f"- åˆ†é¡æ•¸é‡: {metrics.get('categories', 0)}\n"
                            text += f"- æœ€è¿‘æ´»å‹•: {metrics.get('recent_activity', 'Unknown')}\n"
                        
                        elif summary_type == 'detailed' and 'analysis' in summary_result:
                            analysis = summary_result['analysis']
                            text += f"\n\nğŸ’¡ **æ·±å…¥åˆ†æå¯ç”¨**:\n"
                            text += f"   â€¢ åˆ†é¡åˆ†ä½ˆ: {len(analysis.get('category_distribution', {}))} å€‹åˆ†é¡\n"
                            text += f"   â€¢ æ™‚é–“åˆ†ä½ˆ: {len(analysis.get('timeline', {}))} å€‹æœˆä»½\n"
                            text += f"   â€¢ é—œéµä¸»é¡Œ: {len(analysis.get('key_topics', []))} å€‹ä¸»é¡Œ\n"
                        
                        elif summary_type == 'timeline' and 'timeline_data' in summary_result:
                            timeline = summary_result['timeline_data']
                            text += f"\n\nğŸ“… **æ™‚é–“è»¸æ•¸æ“š**: æ¶µè“‹ {len(timeline)} å€‹æœˆä»½çš„æ´»å‹•è¨˜éŒ„\n"
                        
                        text += f"\n\nğŸ¯ **ä¸‹ä¸€æ­¥å»ºè­°**:\n"
                        if summary_type == 'brief':
                            text += f"   â€¢ ä½¿ç”¨ `summarize_project('{project_id}', 'detailed')` ç²å–è©³ç´°åˆ†æ\n"
                            text += f"   â€¢ ä½¿ç”¨ `rag_query('{project_id}', 'å°ˆæ¡ˆçš„æ ¸å¿ƒåŠŸèƒ½æ˜¯ä»€éº¼ï¼Ÿ')` æ·±å…¥äº†è§£\n"
                        elif summary_type == 'detailed':
                            text += f"   â€¢ ä½¿ç”¨ `summarize_project('{project_id}', 'timeline')` æŸ¥çœ‹ç™¼å±•æ­·ç¨‹\n"
                            text += f"   â€¢ ä½¿ç”¨ `search_project_memory` æœå°‹ç‰¹å®šä¸»é¡Œ\n"
                        else:
                            text += f"   â€¢ ä½¿ç”¨ `rag_query` é‡å°ç‰¹å®šæ™‚æœŸæå•\n"
                            text += f"   â€¢ ä½¿ç”¨ `search_project_memory` æœå°‹å…·é«”å…§å®¹\n"
                        
                        return self._success_response(text)
                    
                    else:
                        return self._success_response("âŒ å°ˆæ¡ˆæ‘˜è¦ç”Ÿæˆå¤±æ•—: " + summary_result.get('message', 'æœªçŸ¥éŒ¯èª¤'))
                        
                except Exception as e:
                    logger.error(f"Error in summarize_project: {e}")
                    return self._success_response("âŒ å°ˆæ¡ˆæ‘˜è¦ç³»çµ±æš«æ™‚ç„¡æ³•ä½¿ç”¨")

            elif tool_name == 'delete_project_memory':
                success = self.memory_manager.delete_memory(arguments['project_id'])
                return self._success_response(
                    f"Memory {'deleted' if success else 'not found'} for project: {arguments['project_id']}"
                )

            elif tool_name == 'delete_project_memory_entry':
                result = self.memory_manager.delete_memory_entry(
                    arguments['project_id'],
                    arguments.get('entry_id'),
                    arguments.get('timestamp'),
                    arguments.get('title'),
                    arguments.get('category'),
                    arguments.get('content_match')
                )
                
                if result['success']:
                    text = result['message'] + f"\n\nDeleted entries:\n"
                    for entry in result['deleted_entries']:
                        text += f"- {entry['timestamp']}"
                        if entry['title']:
                            text += f" - {entry['title']}"
                        text += "\n"
                    text += f"\nRemaining entries: {result['remaining_count']}"
                else:
                    text = result['message']
                
                return self._success_response(text)

            elif tool_name == 'edit_project_memory_entry':
                result = self.memory_manager.edit_memory_entry(
                    arguments['project_id'],
                    arguments.get('entry_id'),
                    arguments.get('timestamp'),
                    arguments.get('new_title'),
                    arguments.get('new_category'),
                    arguments.get('new_content')
                )
                
                if result['success']:
                    text = result['message'] + f"\n\nEdited entry:\n"
                    entry = result['edited_entry']
                    text += f"- {entry['timestamp']}"
                    if entry['title']:
                        text += f" - {entry['title']}"
                    if entry['category']:
                        text += f" #{entry['category']}"
                else:
                    text = result['message']
                
                return self._success_response(text)

            elif tool_name == 'list_project_memory_entries':
                result = self.memory_manager.list_memory_entries(arguments['project_id'])
                
                if result['success']:
                    text = f"Memory entries for **{arguments['project_id']}** ({result['total_entries']} entries):\n\n"
                    for entry in result['entries']:
                        text += f"**{entry['id']}.** {entry['timestamp']}"
                        if entry['title']:
                            text += f" - {entry['title']}"
                        if entry['category']:
                            text += f" #{entry['category']}"
                        text += f"\n   {entry['content_preview']}\n\n"
                else:
                    text = result['message']
                
                return self._success_response(text)

            elif tool_name == 'rename_project':
                # é‡æ–°å‘½åå°ˆæ¡ˆ
                success = self.memory_manager.rename_project(
                    arguments['project_id'],
                    arguments['new_name']
                )
                
                if success:
                    return self._success_response(
                        f"âœ… Project '{arguments['project_id']}' successfully renamed to '{arguments['new_name']}'"
                    )
                else:
                    return self._error_response(
                        -32603,
                        f"âŒ Failed to rename project '{arguments['project_id']}' to '{arguments['new_name']}'. "
                        f"Check if the project exists."
                    )
            
            # ==================== INDEX TABLE TOOLS ====================
            elif tool_name == 'search_index':
                project_id = arguments.get('project_id')
                query = arguments.get('query')
                limit = arguments.get('limit', 10)
                
                if hasattr(self.memory_manager, 'search_index'):
                    results = self.memory_manager.search_index(project_id, query, limit)
                    
                    if not results:
                        return self._success_response(f"ğŸ” No results found for '{query}' in project '{project_id}'")
                    
                    response = f"ğŸš€ **æ™ºèƒ½æœå°‹çµæœ** (ç¯€çœ 70-85% token)\n\n"
                    response += f"**å°ˆæ¡ˆ**: {project_id}\n**æŸ¥è©¢**: {query}\n**æ‰¾åˆ°**: {len(results)} å€‹çµæœ\n\n"
                    
                    for i, result in enumerate(results, 1):
                        if result.get('match_type') == 'index':
                            response += f"**{i}. ğŸ“‹ {result.get('title', 'Untitled')}** (ç´¢å¼•åŒ¹é…)\n"
                            response += f"   - **æ‘˜è¦**: {result.get('summary', '')}\n"
                            response += f"   - **é—œéµå­—**: {result.get('keywords', '')}\n"
                            response += f"   - **éšå±¤**: Level {result.get('hierarchy_level', 0)}\n"
                            response += f"   - **é¡å‹**: {result.get('entry_type', 'discussion')}\n\n"
                        else:
                            response += f"**{i}. ğŸ“„ {result.get('title', 'Untitled')}** (å…§å®¹åŒ¹é…)\n"
                            response += f"   - **é è¦½**: {result.get('content_preview', '')}\n\n"
                    
                    response += f"ğŸ’¡ **æç¤º**: ç´¢å¼•åŒ¹é…çµæœå·²å¤§å¹…æ¸›å°‘ token ä½¿ç”¨é‡ï¼"
                    return self._success_response(response)
                else:
                    return self._error_response("âŒ Index search not available (SQLite backend required)")
            
            elif tool_name == 'rebuild_index_for_project':
                project_id = arguments.get('project_id')
                
                if hasattr(self.memory_manager, 'rebuild_index_for_project'):
                    result = self.memory_manager.rebuild_index_for_project(project_id)
                    
                    if result.get('success'):
                        response = f"ğŸ”„ **ç´¢å¼•é‡å»ºå®Œæˆ**: {project_id}\n\n"
                        response += f"- **ç¸½æ¢ç›®æ•¸**: {result.get('total_entries', 0)}\n"
                        response += f"- **æˆåŠŸç´¢å¼•**: {result.get('indexed_entries', 0)}\n"
                        response += f"ğŸ’¡ ç¾åœ¨å¯ä»¥ä½¿ç”¨ search_index äº«å— 70-85% token ç¯€çœæ•ˆç›Šï¼"
                        return self._success_response(response)
                    else:
                        return self._error_response(f"âŒ ç´¢å¼•é‡å»ºå¤±æ•—: {result.get('message', 'Unknown error')}")
                else:
                    return self._error_response("âŒ Index rebuild not available (SQLite backend required)")
            
            elif tool_name == 'get_index_stats':
                project_id = arguments.get('project_id')
                
                if hasattr(self.memory_manager, 'get_index_stats'):
                    stats = self.memory_manager.get_index_stats(project_id)
                    
                    if project_id:
                        response = f"ğŸ“ˆ **å°ˆæ¡ˆç´¢å¼•çµ±è¨ˆ**: {project_id}\n\n"
                        response += f"- **å·²ç´¢å¼•æ¢ç›®**: {stats.get('total_indexed', 0)}\n"
                        response += f"- **æ¢ç›®é¡å‹æ•¸**: {stats.get('entry_types', 0)}\n"
                        response += f"- **æœ€å¤§éšå±¤ç´šåˆ¥**: {stats.get('max_hierarchy_level', 0)}\n"
                    else:
                        response = f"ğŸ“ˆ **å…¨å±€ç´¢å¼•çµ±è¨ˆ**\n\n"
                        response += f"- **å·²ç´¢å¼•æ¢ç›®**: {stats.get('total_indexed', 0)}\n"
                        response += f"- **ç´¢å¼•å°ˆæ¡ˆæ•¸**: {stats.get('indexed_projects', 0)}\n"
                    
                    return self._success_response(response)
                else:
                    return self._error_response("âŒ Index stats not available (SQLite backend required)")
            
            elif tool_name == 'get_hierarchy_tree':
                project_id = arguments.get('project_id')
                
                if hasattr(self.memory_manager, 'get_hierarchy_tree'):
                    tree = self.memory_manager.get_hierarchy_tree(project_id)
                    
                    if tree.get('children'):
                        response = f"ğŸ“Š **å°ˆæ¡ˆéšå±¤çµæ§‹**: {project_id}\n\n"
                        for entry in tree['children']:
                            level_icon = ["ğŸ“", "ğŸ“‚", "ğŸ“„"][min(entry.get('hierarchy_level', 0), 2)]
                            type_icon = {"feature": "ğŸš€", "bug": "ğŸ›", "milestone": "ğŸ¯", "discussion": "ğŸ’¬"}.get(entry.get('entry_type', 'discussion'), "ğŸ’¬")
                            response += f"{level_icon} {type_icon} **{entry.get('title', entry.get('summary', 'Untitled'))}**\n"
                        return self._success_response(response)
                    else:
                        return self._success_response(f"ğŸ“Š å°ˆæ¡ˆ '{project_id}' å°šç„¡ç´¢å¼•æ¢ç›®ï¼Œè«‹å…ˆä½¿ç”¨ rebuild_index_for_project å»ºç«‹ç´¢å¼•")
                else:
                    return self._error_response("âŒ Hierarchy tree not available (SQLite backend required)")
            
            elif tool_name == 'update_index_entry':
                entry_id = arguments.get('entry_id')
                
                if hasattr(self.memory_manager, 'update_index_entry'):
                    update_fields = {}
                    for field in ['summary', 'keywords', 'hierarchy_level', 'entry_type', 'importance_level', 'parent_entry_id']:
                        if field in arguments and arguments[field] is not None:
                            update_fields[field] = arguments[field]
                    
                    if not update_fields:
                        return self._error_response("âŒ æ²’æœ‰æä¾›è¦æ›´æ–°çš„æ¬„ä½")
                    
                    success = self.memory_manager.update_index_entry(entry_id, **update_fields)
                    
                    if success:
                        response = f"âœï¸ **ç´¢å¼•æ¢ç›®æ›´æ–°æˆåŠŸ**: Entry ID {entry_id}\n\n"
                        for field, value in update_fields.items():
                            response += f"  - {field}: {value}\n"
                        return self._success_response(response)
                    else:
                        return self._error_response(f"âŒ æ›´æ–°ç´¢å¼•æ¢ç›®å¤±æ•—: Entry ID {entry_id}")
                else:
                    return self._error_response("âŒ Index update not available (SQLite backend required)")

            elif tool_name == 'sync_markdown_to_sqlite':
                # æª¢æŸ¥ç•¶å‰å¾Œç«¯æ˜¯å¦ç‚º SQLite
                if not isinstance(self.memory_manager, SQLiteBackend):
                    return self._error_response(-32603, "åŒæ­¥åŠŸèƒ½åªèƒ½åœ¨ SQLite å¾Œç«¯æ¨¡å¼ä¸‹ä½¿ç”¨")
                
                try:
                    # å‰µå»º Markdown å¾Œç«¯å¯¦ä¾‹
                    markdown_backend = MarkdownMemoryManager()
                    
                    # å‰µå»ºåŒæ­¥ç®¡ç†å™¨
                    sync_manager = DataSyncManager(markdown_backend, self.memory_manager)
                    
                    # åŸ·è¡ŒåŒæ­¥
                    result = sync_manager.sync_all_projects(
                        mode=arguments.get('mode', 'auto'),
                        similarity_threshold=arguments.get('similarity_threshold', 0.8)
                    )
                    
                    # æ ¼å¼åŒ–å›æ‡‰
                    if result['success']:
                        text = f"ğŸ”„ **Markdown â†’ SQLite åŒæ­¥å®Œæˆ**\n\n"
                        text += f"ğŸ“Š **çµ±è¨ˆè³‡è¨Š**:\n"
                        text += f"- ç¸½å°ˆæ¡ˆæ•¸: {result['total_projects']}\n"
                        text += f"- å·²åŒæ­¥: {result['synced']} âœ…\n"
                        text += f"- è·³é: {result['skipped']} â­ï¸\n"
                        text += f"- éŒ¯èª¤: {result['errors']} âŒ\n\n"
                        
                        if result['log']:
                            text += f"ğŸ“‹ **è©³ç´°æ—¥èªŒ**:\n"
                            for log_entry in result['log']:
                                status_icon = {
                                    'synced': 'âœ…',
                                    'skipped': 'â­ï¸', 
                                    'error': 'âŒ',
                                    'preview': 'ğŸ‘ï¸'
                                }.get(log_entry['action'], 'â“')
                                
                                text += f"{status_icon} **{log_entry['project_id']}**: {log_entry['message']}\n"
                        
                        if result['synced'] > 0:
                            text += f"\nğŸ‰ æˆåŠŸåŒæ­¥ {result['synced']} å€‹å°ˆæ¡ˆåˆ° SQLite å¾Œç«¯ï¼"
                        elif result['total_projects'] == 0:
                            text += f"\nğŸ’¡ æ²’æœ‰æ‰¾åˆ° Markdown å°ˆæ¡ˆéœ€è¦åŒæ­¥ã€‚"
                        else:
                            text += f"\nâš ï¸ æ‰€æœ‰å°ˆæ¡ˆéƒ½è¢«è·³éï¼Œå¯èƒ½å·²ç¶“å­˜åœ¨æ–¼ SQLite ä¸­ã€‚"
                    else:
                        text = f"âŒ åŒæ­¥å¤±æ•—: {result.get('message', 'æœªçŸ¥éŒ¯èª¤')}"
                    
                    return self._success_response(text)
                    
                except Exception as e:
                    logger.error(f"Sync operation failed: {e}")
                    return self._error_response(-32603, f"åŒæ­¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

            elif tool_name == 'export_project_memory':
                try:
                    project_id = arguments['project_id']
                    export_format = arguments.get('format', 'markdown')
                    output_path = arguments.get('output_path')
                    include_metadata = arguments.get('include_metadata', True)
                    
                    # ç²å–å°ˆæ¡ˆè¨˜æ†¶å…§å®¹
                    memory_content = self.memory_manager.get_memory(project_id)
                    if not memory_content:
                        return self._error_response(-32603, f"No memory found for project: {project_id}")
                    
                    # ç²å–å°ˆæ¡ˆçµ±è¨ˆè³‡è¨Š
                    stats = self.memory_manager.get_memory_stats(project_id)
                    
                    # æ ¹æ“šæ ¼å¼åŒ¯å‡º
                    if export_format == 'markdown':
                        exported_content = self._export_to_markdown(project_id, memory_content, stats, include_metadata)
                    elif export_format == 'json':
                        exported_content = self._export_to_json(project_id, memory_content, stats, include_metadata)
                    elif export_format == 'csv':
                        exported_content = self._export_to_csv(project_id, memory_content, stats, include_metadata)
                    elif export_format == 'txt':
                        exported_content = self._export_to_txt(project_id, memory_content, stats, include_metadata)
                    else:
                        return self._error_response(-32603, f"Unsupported export format: {export_format}")
                    
                    # å¦‚æœæŒ‡å®šäº†è¼¸å‡ºè·¯å¾‘ï¼Œå¯«å…¥æª”æ¡ˆ
                    if output_path:
                        try:
                            output_file = Path(output_path)
                            output_file.parent.mkdir(parents=True, exist_ok=True)
                            
                            if export_format == 'json':
                                with open(output_file, 'w', encoding='utf-8') as f:
                                    json.dump(exported_content, f, ensure_ascii=False, indent=2)
                            else:
                                with open(output_file, 'w', encoding='utf-8') as f:
                                    f.write(exported_content)
                            
                            text = f"ğŸ“¤ **å°ˆæ¡ˆåŒ¯å‡ºå®Œæˆ / Project Export Complete**\n\n"
                            text += f"- å°ˆæ¡ˆ / Project: **{project_id}**\n"
                            text += f"- æ ¼å¼ / Format: **{export_format.upper()}**\n"
                            text += f"- è¼¸å‡ºæª”æ¡ˆ / Output File: `{output_file}`\n"
                            if stats['exists']:
                                text += f"- æ¢ç›®æ•¸é‡ / Entries: {stats['total_entries']}\n"
                                text += f"- ç¸½å­—æ•¸ / Words: {stats['total_words']}\n"
                            text += f"\nâœ… æª”æ¡ˆå·²æˆåŠŸå„²å­˜ï¼"
                            
                        except Exception as e:
                            return self._error_response(-32603, f"Failed to write export file: {str(e)}")
                    else:
                        # ç›´æ¥è¿”å›å…§å®¹
                        if export_format == 'json':
                            text = f"ğŸ“¤ **å°ˆæ¡ˆåŒ¯å‡º / Project Export** - {project_id} ({export_format.upper()})\n\n"
                            text += f"```json\n{json.dumps(exported_content, ensure_ascii=False, indent=2)}\n```"
                        else:
                            text = f"ğŸ“¤ **å°ˆæ¡ˆåŒ¯å‡º / Project Export** - {project_id} ({export_format.upper()})\n\n"
                            text += f"```{export_format}\n{exported_content}\n```"
                    
                    return self._success_response(text)
                    
                except Exception as e:
                    logger.error(f"Export operation failed: {e}")
                    return self._error_response(-32603, f"åŒ¯å‡ºéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

            elif tool_name == 'save_global_memory':
                success = self.memory_manager.save_memory(
                    '__global__',
                    arguments['content'],
                    arguments.get('title', ''),
                    arguments.get('category', '')
                )
                return self._success_response(
                    f"Global memory {'saved' if success else 'failed to save'}: {arguments.get('title', 'Untitled')}"
                )

            elif tool_name == 'get_global_memory':
                memory = self.memory_manager.get_memory('__global__')
                return self._success_response(
                    memory or "No global memory found. Use save_global_memory to start building your knowledge base."
                )

            elif tool_name == 'search_global_memory':
                results = self.memory_manager.search_memory(
                    '__global__',
                    arguments['query'],
                    arguments.get('limit', 10)
                )
                
                if results:
                    text = f"Found {len(results)} global memory matches for \"{arguments['query']}\":\n\n"
                    for i, result in enumerate(results, 1):
                        text += f"**{i}. {result['timestamp']}"
                        if result['title']:
                            text += f" - {result['title']}"
                        if result['category']:
                            text += f" #{result['category']}"
                        text += f"**\n{result['content']}\n\n"
                else:
                    text = f"No global memory matches found for \"{arguments['query']}\""
                
                return self._success_response(text)

            elif tool_name == 'get_global_memory_stats':
                stats = self.memory_manager.get_memory_stats('__global__')
                
                if stats['exists']:
                    text = f"ğŸ“Š **Global Memory Statistics**:\n\n"
                    text += f"- Total entries: {stats['total_entries']}\n"
                    text += f"- Total words: {stats['total_words']}\n"
                    text += f"- Total characters: {stats['total_characters']}\n"
                    if stats['categories']:
                        text += f"- Categories: {', '.join(stats['categories'])}\n"
                    if stats['latest_entry']:
                        text += f"- Latest entry: {stats['latest_entry']}\n"
                    if stats['oldest_entry']:
                        text += f"- Oldest entry: {stats['oldest_entry']}\n"
                    text += f"\nğŸ’¡ Global memory contains cross-project knowledge and standards."
                else:
                    text = f"ğŸ“ **Global Memory is Empty**\n\nStart building your global knowledge base with save_global_memory!"
                
                return self._success_response(text)

            elif tool_name == 'get_backend_status':
                # ç²å–ç•¶å‰å¾Œç«¯é¡å‹
                backend_type = type(self.memory_manager).__name__
                backend_name = "SQLite" if "SQLite" in backend_type else "Markdown"
                
                # ç²å–å­˜å„²è·¯å¾‘ä¿¡æ¯
                if hasattr(self.memory_manager, 'db_path'):
                    storage_path = str(self.memory_manager.db_path)
                    storage_type = "Database file"
                elif hasattr(self.memory_manager, 'memory_dir'):
                    storage_path = str(self.memory_manager.memory_dir)
                    storage_type = "Directory"
                else:
                    storage_path = "Unknown"
                    storage_type = "Unknown"
                
                # ç²å–é …ç›®çµ±è¨ˆ
                projects = self.memory_manager.list_projects()
                total_projects = len(projects)
                total_entries = sum(p['entries_count'] for p in projects)
                
                # æ§‹å»ºç‹€æ…‹ä¿¡æ¯
                text = f"ğŸ”§ **Backend Status / å¾Œç«¯ç‹€æ…‹**\n\n"
                text += f"**Current Backend / ç•¶å‰å¾Œç«¯**: {backend_name}\n"
                text += f"**Backend Class / å¾Œç«¯é¡åˆ¥**: `{backend_type}`\n"
                text += f"**Storage Type / å­˜å„²é¡å‹**: {storage_type}\n"
                text += f"**Storage Path / å­˜å„²è·¯å¾‘**: `{storage_path}`\n\n"
                text += f"ğŸ“Š **Quick Stats / å¿«é€Ÿçµ±è¨ˆ**:\n"
                text += f"- Projects / å°ˆæ¡ˆæ•¸: **{total_projects}**\n"
                text += f"- Total Entries / ç¸½æ¢ç›®æ•¸: **{total_entries}**\n\n"
                
                # æ·»åŠ å¾Œç«¯ç‰¹æ€§èªªæ˜
                if backend_name == "SQLite":
                    text += f"âœ… **SQLite Features / SQLite ç‰¹æ€§**:\n"
                    text += f"- ğŸ” Full-text search / å…¨æ–‡æœå°‹\n"
                    text += f"- ğŸš€ High performance / é«˜æ•ˆèƒ½\n"
                    text += f"- ğŸ”’ ACID transactions / ACID äº‹å‹™\n"
                    text += f"- ğŸ“Š Complex queries / è¤‡é›œæŸ¥è©¢\n"
                else:
                    text += f"âœ… **Markdown Features / Markdown ç‰¹æ€§**:\n"
                    text += f"- ğŸ“ Human-readable / äººé¡å¯è®€\n"
                    text += f"- ğŸ”„ Version control friendly / ç‰ˆæœ¬æ§åˆ¶å‹å¥½\n"
                    text += f"- ğŸ“ File-based storage / æª”æ¡ˆå¼å­˜å„²\n"
                    text += f"- ğŸ”„ Easy backup / å®¹æ˜“å‚™ä»½\n"
                
                text += f"\nğŸ’¡ Use `list_memory_projects` to see all projects"
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_universal':
                result = self.importer.import_universal(
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **é€šç”¨åŒ¯å…¥æˆåŠŸ / Universal Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **åŒ¯å…¥å¤±æ•— / Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_from_markdown':
                result = self.importer.import_from_markdown(
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **Markdown åŒ¯å…¥æˆåŠŸ / Markdown Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **Markdown åŒ¯å…¥å¤±æ•— / Markdown Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_from_json':
                result = self.importer.import_from_json(
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **JSON åŒ¯å…¥æˆåŠŸ / JSON Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **JSON åŒ¯å…¥å¤±æ•— / JSON Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_from_csv':
                result = self.importer.import_from_csv(
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **CSV åŒ¯å…¥æˆåŠŸ / CSV Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **CSV åŒ¯å…¥å¤±æ•— / CSV Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_from_txt':
                result = self.importer.import_from_markdown(  # TXT ç•¶ä½œ Markdown è™•ç†
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **TXT åŒ¯å…¥æˆåŠŸ / TXT Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **TXT åŒ¯å…¥å¤±æ•— / TXT Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            else:
                return self._error_response(-32601, f"Unknown tool: {tool_name}")

        except Exception as e:
            logger.error(f"Error calling tool {tool_name}: {e}")
            return self._error_response(-32603, str(e))

    def _success_response(self, text: str) -> Dict[str, Any]:
        """å»ºç«‹æˆåŠŸå›æ‡‰"""
        return {
            'jsonrpc': '2.0',
            'result': {
                'content': [{
                    'type': 'text',
                    'text': text
                }]
            }
        }

    def _error_response(self, code: int, message: str) -> Dict[str, Any]:
        """å»ºç«‹éŒ¯èª¤å›æ‡‰"""
        return {
            'jsonrpc': '2.0',
            'error': {
                'code': code,
                'message': message
            }
        }

    def _export_to_markdown(self, project_id: str, memory_content: str, stats: Dict, include_metadata: bool) -> str:
        """åŒ¯å‡ºç‚º Markdown æ ¼å¼"""
        content = f"# {project_id}\n\n"
        
        if include_metadata and stats['exists']:
            content += f"## å°ˆæ¡ˆè³‡è¨Š / Project Information\n\n"
            content += f"- **å°ˆæ¡ˆåç¨± / Project Name:** {project_id}\n"
            content += f"- **æ¢ç›®æ•¸é‡ / Total Entries:** {stats['total_entries']}\n"
            content += f"- **ç¸½å­—æ•¸ / Total Words:** {stats['total_words']}\n"
            content += f"- **ç¸½å­—ç¬¦æ•¸ / Total Characters:** {stats['total_characters']}\n"
            if stats['categories']:
                content += f"- **åˆ†é¡ / Categories:** {', '.join(stats['categories'])}\n"
            if stats['latest_entry']:
                content += f"- **æœ€æ–°æ¢ç›® / Latest Entry:** {stats['latest_entry']}\n"
            if stats['oldest_entry']:
                content += f"- **æœ€èˆŠæ¢ç›® / Oldest Entry:** {stats['oldest_entry']}\n"
            content += f"- **åŒ¯å‡ºæ™‚é–“ / Export Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            content += "---\n\n"
        
        content += f"## è¨˜æ†¶å…§å®¹ / Memory Content\n\n"
        content += memory_content
        
        return content

    def _export_to_json(self, project_id: str, memory_content: str, stats: Dict, include_metadata: bool) -> Dict:
        """åŒ¯å‡ºç‚º JSON æ ¼å¼"""
        export_data = {
            'project_id': project_id,
            'content': memory_content,
            'export_time': datetime.now().isoformat()
        }
        
        if include_metadata and stats['exists']:
            export_data['metadata'] = {
                'total_entries': stats['total_entries'],
                'total_words': stats['total_words'],
                'total_characters': stats['total_characters'],
                'categories': stats['categories'],
                'latest_entry': stats['latest_entry'],
                'oldest_entry': stats['oldest_entry']
            }
        
        return export_data

    def _export_to_csv(self, project_id: str, memory_content: str, stats: Dict, include_metadata: bool) -> str:
        """åŒ¯å‡ºç‚º CSV æ ¼å¼ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼Œä¸»è¦ç”¨æ–¼æ¢ç›®åˆ—è¡¨ï¼‰"""
        import csv
        from io import StringIO
        
        output = StringIO()
        writer = csv.writer(output)
        
        # CSV æ¨™é¡Œ
        if include_metadata:
            writer.writerow(['Timestamp', 'Title', 'Category', 'Content'])
        else:
            writer.writerow(['Content'])
        
        # å˜—è©¦è§£æè¨˜æ†¶å…§å®¹ä¸­çš„æ¢ç›®
        lines = memory_content.split('\n')
        current_entry = {'timestamp': '', 'title': '', 'category': '', 'content': ''}
        
        for line in lines:
            line = line.strip()
            if line.startswith('**') and line.endswith('**'):
                # å¯èƒ½æ˜¯æ™‚é–“æˆ³è¨˜è¡Œ
                if current_entry['content']:
                    # å¯«å…¥å‰ä¸€å€‹æ¢ç›®
                    if include_metadata:
                        writer.writerow([current_entry['timestamp'], current_entry['title'], 
                                       current_entry['category'], current_entry['content'].strip()])
                    else:
                        writer.writerow([current_entry['content'].strip()])
                    current_entry = {'timestamp': '', 'title': '', 'category': '', 'content': ''}
                
                # è§£ææ–°æ¢ç›®çš„æ¨™é¡Œè¡Œ
                header = line[2:-2]  # ç§»é™¤ **
                if ' - ' in header:
                    parts = header.split(' - ', 1)
                    current_entry['timestamp'] = parts[0]
                    title_and_category = parts[1]
                    if ' #' in title_and_category:
                        title_parts = title_and_category.split(' #')
                        current_entry['title'] = title_parts[0]
                        current_entry['category'] = title_parts[1] if len(title_parts) > 1 else ''
                    else:
                        current_entry['title'] = title_and_category
                else:
                    current_entry['timestamp'] = header
            else:
                if line:
                    current_entry['content'] += line + '\n'
        
        # å¯«å…¥æœ€å¾Œä¸€å€‹æ¢ç›®
        if current_entry['content']:
            if include_metadata:
                writer.writerow([current_entry['timestamp'], current_entry['title'], 
                               current_entry['category'], current_entry['content'].strip()])
            else:
                writer.writerow([current_entry['content'].strip()])
        
        return output.getvalue()

    def _export_to_txt(self, project_id: str, memory_content: str, stats: Dict, include_metadata: bool) -> str:
        """åŒ¯å‡ºç‚ºç´”æ–‡å­—æ ¼å¼"""
        content = f"å°ˆæ¡ˆ: {project_id}\n"
        content += "=" * 50 + "\n\n"
        
        if include_metadata and stats['exists']:
            content += f"å°ˆæ¡ˆè³‡è¨Š:\n"
            content += f"- æ¢ç›®æ•¸é‡: {stats['total_entries']}\n"
            content += f"- ç¸½å­—æ•¸: {stats['total_words']}\n"
            content += f"- ç¸½å­—ç¬¦æ•¸: {stats['total_characters']}\n"
            if stats['categories']:
                content += f"- åˆ†é¡: {', '.join(stats['categories'])}\n"
            if stats['latest_entry']:
                content += f"- æœ€æ–°æ¢ç›®: {stats['latest_entry']}\n"
            if stats['oldest_entry']:
                content += f"- æœ€èˆŠæ¢ç›®: {stats['oldest_entry']}\n"
            content += f"- åŒ¯å‡ºæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            content += "-" * 50 + "\n\n"
        
        content += "è¨˜æ†¶å…§å®¹:\n\n"
        # ç§»é™¤ Markdown æ ¼å¼æ¨™è¨˜
        clean_content = memory_content.replace('**', '').replace('*', '').replace('#', '')
        content += clean_content
        
        return content

    async def run(self):
        """é‹è¡Œ MCP ä¼ºæœå™¨"""
        logger.info(f"Starting Memory MCP Server v{self.version}")
        
        try:
            while True:
                # å¾ stdin è®€å–è¨Šæ¯
                try:
                    line = await asyncio.get_event_loop().run_in_executor(
                        None, sys.stdin.readline
                    )
                except EOFError:
                    break
                
                if not line:
                    break
                
                try:
                    message = json.loads(line.strip())
                    logger.info(f"[MCP] Received message: {message.get('method', 'unknown')}, full message: {message}")
                    logger.debug(f"Received message: {message.get('method', 'unknown')}")
                    
                    try:
                        response = await self.handle_message(message)
                        logger.info(f"[MCP] handle_message completed for {message.get('method')}")
                        
                        # åªæœ‰éé€šçŸ¥æ¶ˆæ¯æ‰éœ€è¦å›æ‡‰
                        if response is not None:
                            # è¨­å®š response ID
                            if 'id' in message:
                                response['id'] = message['id']
                            
                            # å°‡å›æ‡‰å¯«å…¥ stdout (ä½¿ç”¨ UTF-8 ç·¨ç¢¼)
                            try:
                                logger.info(f"[MCP] About to serialize response for {message.get('method')}")
                                output = json.dumps(response, ensure_ascii=False)
                                logger.info(f"[MCP] JSON serialization successful, sending response")
                                print(output)
                                sys.stdout.flush()
                                logger.info(f"[MCP] Response sent successfully for {message.get('method')}")
                            except Exception as json_error:
                                logger.error(f"JSON serialization error: {json_error}")
                                # ç™¼é€éŒ¯èª¤å›æ‡‰è€Œä¸æ˜¯é™ç´šåˆ° ASCII
                                if 'id' in message:
                                    error_response = {
                                        'jsonrpc': '2.0',
                                        'id': message['id'],
                                        'error': {
                                            'code': -32603,
                                            'message': 'JSON serialization error',
                                            'data': str(json_error)
                                        }
                                    }
                                    try:
                                        print(json.dumps(error_response, ensure_ascii=False))
                                        sys.stdout.flush()
                                    except Exception as error_json_error:
                                        logger.error(f"Failed to send error response: {error_json_error}")
                                continue
                    
                    except Exception as handle_error:
                        logger.error(f"Error handling message: {handle_error}")
                        # ç™¼é€éŒ¯èª¤å›æ‡‰
                        if 'id' in message:
                            error_response = {
                                'jsonrpc': '2.0',
                                'id': message['id'],
                                'error': {
                                    'code': -32603,
                                    'message': 'Internal error',
                                    'data': str(handle_error)
                                }
                            }
                            try:
                                print(json.dumps(error_response, ensure_ascii=False))
                                sys.stdout.flush()
                            except Exception as error_json_error:
                                logger.error(f"Failed to send error response: {error_json_error}")
                        continue
                    
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON received: {e}")
                    continue
                    
        except KeyboardInterrupt:
            logger.info("Server stopped by user")
        except Exception as e:
            logger.error(f"Server error: {e}")
        finally:
            logger.info("Server shutdown complete")

def create_backend(backend_type: str, db_path: str = None) -> MemoryBackend:
    """æ ¹æ“šé¡å‹å‰µå»ºè¨˜æ†¶å¾Œç«¯"""
    if backend_type == "markdown":
        return MarkdownMemoryManager()
    elif backend_type == "sqlite":
        if not db_path or not str(db_path).strip():
            raise ValueError("SQLite backend requires --db-path to be provided. Example: --db-path=~/.mcp/ai-memory/memory.db")
        # å±•é–‹ ~ å®¶ç›®éŒ„ç¬¦è™Ÿ
        expanded_path = os.path.expanduser(db_path)
        return SQLiteBackend(expanded_path)
    else:
        raise ValueError(f"Unknown backend type: {backend_type}")

def main():
    """ä¸»ç¨‹å¼å…¥å£é»"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Markdown Memory MCP Server")
    parser.add_argument(
        "--backend", 
        choices=["markdown", "sqlite"], 
        default="sqlite",
        help="é¸æ“‡è¨˜æ†¶å¾Œç«¯é¡å‹ (default: markdown)"
    )
    parser.add_argument(
        "--info", 
        action="store_true",
        help="é¡¯ç¤ºç•¶å‰é…ç½®è³‡è¨Š"
    )
    parser.add_argument(
        "--sync-from-markdown",
        action="store_true",
        help="å°‡ Markdown å°ˆæ¡ˆåŒæ­¥åˆ° SQLite å¾Œç«¯"
    )
    parser.add_argument(
        "--sync-mode",
        choices=["auto", "interactive", "preview"],
        default="auto",
        help="åŒæ­¥æ¨¡å¼: auto(è‡ªå‹•), interactive(äº’å‹•), preview(é è¦½)"
    )
    parser.add_argument(
        "--similarity-threshold",
        type=float,
        default=0.8,
        help="ç›¸ä¼¼åº¦é–¾å€¼ (0.0-1.0)ï¼Œç”¨æ–¼æ±ºå®šæ˜¯å¦è‡ªå‹•åˆä½µ"
    )
    parser.add_argument(
        "--db-path",
        type=str,
        help="[å¿…å¡«æ–¼ sqlite æ¨¡å¼] SQLite è³‡æ–™åº«è·¯å¾‘ï¼Œæ”¯æ´ ~ã€‚ä¾‹å¦‚: ~/.mcp/ai-memory/memory.db"
    )
    
    args = parser.parse_args()
    
    if args.info:
        print(f"Markdown Memory MCP Server")
        print(f"Backend: {args.backend}")
        print(f"Python version: {sys.version}")
        print(f"Working directory: {os.getcwd()}")
        print(f"Script path: {__file__}")
        
        # é¡¯ç¤ºå¾Œç«¯ç‰¹å®šè³‡è¨Šï¼ˆä¸åˆå§‹åŒ–ï¼‰
        if args.backend == "sqlite":
            if not args.db_path:
                print("Error: --db-path is required for sqlite backend.")
                print("Example: --db-path=~/.mcp/ai-memory/memory.db")
                return
            db_path = Path(os.path.expanduser(args.db_path)).resolve()
            print(f"SQLite database: {db_path}")
            print(f"Database exists: {db_path.exists()}")
        elif args.backend == "markdown":
            memory_dir = Path(__file__).parent.resolve() / "ai-memory"
            print(f"Memory directory: {memory_dir}")
            print(f"Directory exists: {memory_dir.exists()}")
            
        return
    
    # è™•ç†åŒæ­¥åŠŸèƒ½
    if args.sync_from_markdown:
        if args.backend != "sqlite":
            print("éŒ¯èª¤: åŒæ­¥åŠŸèƒ½åªèƒ½åœ¨ SQLite å¾Œç«¯æ¨¡å¼ä¸‹ä½¿ç”¨")
            print("è«‹ä½¿ç”¨: python memory_mcp_server_dev.py --backend=sqlite --sync-from-markdown")
            sys.exit(1)
        
        try:
            # å‰µå»ºå…©å€‹å¾Œç«¯å¯¦ä¾‹
            markdown_backend = MarkdownMemoryManager()
            sqlite_backend = SQLiteBackend()
            
            # å‰µå»ºåŒæ­¥ç®¡ç†å™¨
            sync_manager = DataSyncManager(markdown_backend, sqlite_backend)
            
            print(f"é–‹å§‹åŒæ­¥ Markdown å°ˆæ¡ˆåˆ° SQLite...")
            print(f"åŒæ­¥æ¨¡å¼: {args.sync_mode}")
            print(f"ç›¸ä¼¼åº¦é–¾å€¼: {args.similarity_threshold}")
            print("-" * 50)
            
            # åŸ·è¡ŒåŒæ­¥
            result = sync_manager.sync_all_projects(
                mode=args.sync_mode,
                similarity_threshold=args.similarity_threshold
            )
            
            # é¡¯ç¤ºçµæœ
            print(f"\nåŒæ­¥å®Œæˆ!")
            print(f"ç¸½å°ˆæ¡ˆæ•¸: {result['total_projects']}")
            print(f"å·²åŒæ­¥: {result['synced']}")
            print(f"è·³é: {result['skipped']}")
            print(f"éŒ¯èª¤: {result['errors']}")
            
            if result['log']:
                print("\nè©³ç´°æ—¥èªŒ:")
                for log_entry in result['log']:
                    status_icon = {
                        'synced': 'âœ…',
                        'skipped': 'â­ï¸',
                        'error': 'âŒ',
                        'preview': 'ğŸ‘ï¸'
                    }.get(log_entry['action'], 'â“')
                    
                    print(f"{status_icon} {log_entry['project_id']}: {log_entry['message']}")
            
            print(f"\nåŒæ­¥æ“ä½œå®Œæˆ!")
            
        except Exception as e:
            print(f"åŒæ­¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            logger.error(f"Sync error: {e}")
            sys.exit(1)
        
        return
    
    # å‰µå»ºå¾Œç«¯ï¼ˆåªæœ‰å¯¦éš›é‹è¡Œæ™‚æ‰åˆå§‹åŒ–ï¼‰
    try:
        if args.backend == "sqlite" and not args.db_path:
            logger.error("--db-path is required when using sqlite backend. Example: --db-path=~/.mcp/ai-memory/memory.db")
            sys.exit(1)
        backend = create_backend(args.backend, args.db_path)
        if args.backend == "sqlite":
            logger.info(f"Using sqlite backend with db path: {args.db_path}")
        else:
            logger.info(f"Using {args.backend} backend")
    except Exception as e:
        logger.error(f"Failed to create backend: {e}")
        sys.exit(1)
    
    # ç¢ºä¿è¼¸å‡ºæ˜¯ UTF-8
    if sys.stdout.encoding != 'utf-8':
        sys.stdout.reconfigure(encoding='utf-8')
    
    # æ·»åŠ èª¿è©¦è³‡è¨Š
    logger.info(f"Python version: {sys.version}")
    logger.info(f"Working directory: {os.getcwd()}")
    logger.info(f"Script path: {__file__}")
    
    server = MCPServer(backend)
    try:
        asyncio.run(server.run())
    except KeyboardInterrupt:
        logger.info("Server interrupted")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
