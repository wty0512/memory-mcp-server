#!/usr/bin/env python3
"""
Python Memory MCP Server
ä¸€å€‹åŸºæ–¼ Python çš„ Model Context Protocol ä¼ºæœå™¨ï¼Œæä¾›è¨˜æ†¶ç®¡ç†åŠŸèƒ½ï¼Œæ”¯æ´ SQLite å’Œ Markdown é›™å¾Œç«¯å„²å­˜

A Python-based Model Context Protocol server providing memory management 
with SQLite and Markdown dual backend storage support.

Features / åŠŸèƒ½ç‰¹è‰²:
- SQLite Backend (Default): High-performance database with full-text search
  SQLite å¾Œç«¯ï¼ˆé è¨­ï¼‰ï¼šé«˜æ•ˆèƒ½è³‡æ–™åº«ï¼Œæ”¯æ´å…¨æ–‡æœå°‹
- Markdown Backend: Human-readable format for version control
  Markdown å¾Œç«¯ï¼šäººé¡å¯è®€æ ¼å¼ï¼Œä¾¿æ–¼ç‰ˆæœ¬æ§åˆ¶
- Auto Sync: Auto-sync Markdown projects to SQLite
  è‡ªå‹•åŒæ­¥ï¼šè‡ªå‹•å°‡ Markdown å°ˆæ¡ˆåŒæ­¥åˆ° SQLite
- Auto Project Display: Show project list on startup
  è‡ªå‹•å°ˆæ¡ˆé¡¯ç¤ºï¼šå•Ÿå‹•æ™‚é¡¯ç¤ºå°ˆæ¡ˆåˆ—è¡¨
"""

import asyncio
import json
import logging
import os
import sys
import sqlite3
import time
import csv
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from abc import ABC, abstractmethod
from contextlib import contextmanager
import difflib
import re
import time

# è·¨å¹³å°æª”æ¡ˆé–å®šæ”¯æ´
try:
    import fcntl  # Unix/Linux/macOS
    HAS_FCNTL = True
except ImportError:
    HAS_FCNTL = False

try:
    import msvcrt  # Windows
    HAS_MSVCRT = True
except ImportError:
    HAS_MSVCRT = False

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FileLock:
    """è·¨å¹³å°æª”æ¡ˆé–å®šå·¥å…·é¡"""
    
    def __init__(self, file_path: Path, timeout: float = 30.0, retry_delay: float = 0.1):
        self.file_path = file_path
        self.timeout = timeout
        self.retry_delay = retry_delay
        self.lock_file = None
        self.locked = False
    
    def __enter__(self):
        self.acquire()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()
    
    def acquire(self):
        """å–å¾—æª”æ¡ˆé–å®š"""
        if self.locked:
            return
        
        # å‰µå»ºé–å®šæª”æ¡ˆè·¯å¾‘
        lock_file_path = self.file_path.with_suffix(self.file_path.suffix + '.lock')
        
        start_time = time.time()
        while time.time() - start_time < self.timeout:
            try:
                # å˜—è©¦å‰µå»ºé–å®šæª”æ¡ˆ
                self.lock_file = open(lock_file_path, 'w')
                
                if HAS_FCNTL:
                    # Unix/Linux/macOS ä½¿ç”¨ fcntl
                    fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
                elif HAS_MSVCRT:
                    # Windows ä½¿ç”¨ msvcrt
                    msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_NBLCK, 1)
                else:
                    # é™ç´šæ–¹æ¡ˆï¼šä½¿ç”¨æª”æ¡ˆå­˜åœ¨æ€§æª¢æŸ¥
                    if lock_file_path.exists():
                        raise OSError("Lock file exists")
                
                # å¯«å…¥é–å®šè³‡è¨Š
                self.lock_file.write(f"Locked by PID {os.getpid()} at {time.time()}\n")
                self.lock_file.flush()
                
                self.locked = True
                logger.debug(f"Acquired lock for {self.file_path}")
                return
                
            except (OSError, IOError) as e:
                # é–å®šå¤±æ•—ï¼Œæ¸…ç†ä¸¦é‡è©¦
                if self.lock_file:
                    try:
                        self.lock_file.close()
                    except:
                        pass
                    self.lock_file = None
                
                logger.debug(f"Lock acquisition failed for {self.file_path}: {e}")
                time.sleep(self.retry_delay)
                continue
        
        # è¶…æ™‚å¤±æ•—
        raise TimeoutError(f"Failed to acquire lock for {self.file_path} within {self.timeout} seconds")
    
    def release(self):
        """é‡‹æ”¾æª”æ¡ˆé–å®š"""
        if not self.locked or not self.lock_file:
            return
        
        try:
            if HAS_FCNTL:
                fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)
            elif HAS_MSVCRT:
                msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_UNLCK, 1)
            
            self.lock_file.close()
            
            # åˆªé™¤é–å®šæª”æ¡ˆ
            lock_file_path = self.file_path.with_suffix(self.file_path.suffix + '.lock')
            try:
                lock_file_path.unlink()
            except FileNotFoundError:
                pass
            
            logger.debug(f"Released lock for {self.file_path}")
            
        except Exception as e:
            logger.warning(f"Error releasing lock for {self.file_path}: {e}")
        finally:
            self.lock_file = None
            self.locked = False

class AtomicFileWriter:
    """åŸå­æª”æ¡ˆå¯«å…¥å·¥å…·é¡"""
    
    def __init__(self, file_path: Path):
        self.file_path = file_path
        self.temp_path = file_path.with_suffix(file_path.suffix + '.tmp')
        self.temp_file = None
    
    def __enter__(self):
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self.file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # é–‹å•Ÿè‡¨æ™‚æª”æ¡ˆ
        self.temp_file = open(self.temp_path, 'w', encoding='utf-8')
        return self.temp_file
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.temp_file:
            self.temp_file.close()
        
        if exc_type is None:
            # æˆåŠŸï¼šåŸå­æ€§é‡å‘½å
            try:
                self.temp_path.replace(self.file_path)
                logger.debug(f"Atomic write completed for {self.file_path}")
            except Exception as e:
                logger.error(f"Failed to complete atomic write for {self.file_path}: {e}")
                # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                try:
                    self.temp_path.unlink()
                except FileNotFoundError:
                    pass
                raise
        else:
            # å¤±æ•—ï¼šæ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            try:
                self.temp_path.unlink()
            except FileNotFoundError:
                pass


class MemoryBackend(ABC):
    """
    è¨˜æ†¶å¾Œç«¯æŠ½è±¡åŸºé¡
    Abstract base class for memory backends
    
    å®šç¾©æ‰€æœ‰è¨˜æ†¶å¾Œç«¯å¿…é ˆå¯¦ä½œçš„ä»‹é¢æ–¹æ³•
    Defines interface methods that all memory backends must implement
    """
    
    @abstractmethod
    def save_memory(self, project_id: str, content: str, title: str = "", category: str = "") -> bool:
        """å„²å­˜è¨˜æ†¶åˆ°å¾Œç«¯"""
        pass
    
    @abstractmethod
    def get_memory(self, project_id: str) -> Optional[str]:
        """è®€å–å®Œæ•´å°ˆæ¡ˆè¨˜æ†¶"""
        pass
    
    @abstractmethod
    def search_memory(self, project_id: str, query: str, limit: int = 10) -> List[Dict[str, str]]:
        """æœå°‹è¨˜æ†¶å…§å®¹"""
        pass
    
    @abstractmethod
    def list_projects(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆåŠå…¶çµ±è¨ˆè³‡è¨Š"""
        pass
    
    @abstractmethod
    def get_recent_memory(self, project_id: str, limit: int = 5) -> List[Dict[str, str]]:
        """å–å¾—æœ€è¿‘çš„è¨˜æ†¶æ¢ç›®"""
        pass
    
    @abstractmethod
    def delete_memory(self, project_id: str) -> bool:
        """åˆªé™¤å°ˆæ¡ˆè¨˜æ†¶æª”æ¡ˆ"""
        pass
    
    @abstractmethod
    def delete_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None, 
                           title: str = None, category: str = None, content_match: str = None) -> Dict[str, Any]:
        """åˆªé™¤ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        pass
    
    @abstractmethod
    def edit_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None,
                         new_title: str = None, new_category: str = None, new_content: str = None) -> Dict[str, Any]:
        """ç·¨è¼¯ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        pass
    
    @abstractmethod
    def list_memory_entries(self, project_id: str) -> Dict[str, Any]:
        """åˆ—å‡ºå°ˆæ¡ˆä¸­çš„æ‰€æœ‰è¨˜æ†¶æ¢ç›®ï¼Œå¸¶æœ‰ç´¢å¼•"""
        pass
    
    @abstractmethod
    def get_memory_stats(self, project_id: str) -> Dict[str, Any]:
        """å–å¾—è¨˜æ†¶çµ±è¨ˆè³‡è¨Š"""
        pass

    @abstractmethod
    def rename_project(self, old_project_id: str, new_project_id: str) -> bool:
        """é‡æ–°å‘½åå°ˆæ¡ˆ"""
        raise NotImplementedError("Subclasses must implement rename_project method")



class MarkdownMemoryManager(MemoryBackend):
    """
    Markdown è¨˜æ†¶ç®¡ç†å™¨
    Markdown Memory Manager
    
    ä½¿ç”¨ Markdown æª”æ¡ˆæ ¼å¼å„²å­˜å’Œç®¡ç† AI è¨˜æ†¶ï¼Œæ”¯æ´æª”æ¡ˆé–å®šå’ŒåŸå­å¯«å…¥
    Stores and manages AI memory using Markdown file format with file locking and atomic writes
    """
    
    def __init__(self, memory_dir: str = "ai-memory"):
        # ç¸½æ˜¯ä½¿ç”¨è…³æœ¬æ‰€åœ¨ç›®éŒ„ä½œç‚ºåŸºæº–ï¼Œç¢ºä¿è·¯å¾‘ç©©å®šæ€§
        script_dir = Path(__file__).parent.resolve()  # ä½¿ç”¨ resolve() ç²å–çµ•å°è·¯å¾‘
        
        if Path(memory_dir).is_absolute():
            # å¦‚æœæä¾›çµ•å°è·¯å¾‘ï¼Œç›´æ¥ä½¿ç”¨
            self.memory_dir = Path(memory_dir)
        else:
            # ç›¸å°è·¯å¾‘ç¸½æ˜¯ç›¸å°æ–¼è…³æœ¬ç›®éŒ„
            self.memory_dir = script_dir / memory_dir
        
        # ç¢ºä¿ä½¿ç”¨çµ•å°è·¯å¾‘
        self.memory_dir = self.memory_dir.resolve()
        
        # è¨˜éŒ„è·¯å¾‘ä¿¡æ¯ç”¨æ–¼èª¿è©¦
        logger.info(f"Script directory: {script_dir}")
        logger.info(f"Target memory directory: {self.memory_dir}")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        try:
            self.memory_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Memory directory successfully initialized: {self.memory_dir}")
            
            # é©—è­‰ç›®éŒ„æ˜¯å¦å¯å¯«
            test_file = self.memory_dir / ".write_test"
            try:
                test_file.write_text("test")
                test_file.unlink()  # åˆªé™¤æ¸¬è©¦æ–‡ä»¶
                logger.info("Memory directory write test passed")
            except Exception as write_error:
                logger.error(f"Memory directory is not writable: {write_error}")
                raise OSError(f"Memory directory not writable: {self.memory_dir}")
                
        except OSError as e:
            logger.error(f"Failed to create memory directory at {self.memory_dir}: {e}")
            # å¦‚æœç„¡æ³•å‰µå»ºï¼Œå˜—è©¦åœ¨ç”¨æˆ¶ä¸»ç›®éŒ„å‰µå»º
            import tempfile
            fallback_dir = Path.home() / ".ai-memory"
            try:
                fallback_dir.mkdir(parents=True, exist_ok=True)
                self.memory_dir = fallback_dir
                logger.warning(f"Using fallback directory in user home: {self.memory_dir}")
            except OSError:
                # æœ€å¾Œçš„å‚™é¸æ–¹æ¡ˆï¼šä½¿ç”¨è‡¨æ™‚ç›®éŒ„
                self.memory_dir = Path(tempfile.mkdtemp(prefix="ai-memory-"))
                logger.warning(f"Using temporary directory: {self.memory_dir}")

    def get_memory_file(self, project_id: str) -> Path:
        """å–å¾—å°ˆæ¡ˆè¨˜æ†¶æª”æ¡ˆè·¯å¾‘"""
        # æ¸…ç†å°ˆæ¡ˆ IDï¼Œç§»é™¤ä¸å®‰å…¨å­—ç¬¦
        clean_id = "".join(c for c in project_id if c.isalnum() or c in ('-', '_'))
        return self.memory_dir / f"{clean_id}.md"

    def get_memory(self, project_id: str) -> Optional[str]:
        """è®€å–å®Œæ•´å°ˆæ¡ˆè¨˜æ†¶"""
        try:
            memory_file = self.get_memory_file(project_id)
            if memory_file.exists():
                return memory_file.read_text(encoding='utf-8')
            return None
        except Exception as e:
            logger.error(f"Error reading memory for {project_id}: {e}")
            return None

    def save_memory(self, project_id: str, content: str, title: str = "", category: str = "") -> bool:
        """å„²å­˜è¨˜æ†¶åˆ° markdown æª”æ¡ˆï¼ˆä½¿ç”¨æª”æ¡ˆé–å®šï¼‰"""
        try:
            memory_file = self.get_memory_file(project_id)
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # å»ºç«‹è¨˜æ†¶æ¢ç›®
            entry_parts = [f"\n## {timestamp}"]
            if title:
                entry_parts.append(f" - {title}")
            if category:
                entry_parts.append(f" #{category}")
            
            entry = "".join(entry_parts) + f"\n\n{content}\n\n---\n"
            
            # ä½¿ç”¨æª”æ¡ˆé–å®šé€²è¡Œå®‰å…¨å¯«å…¥
            with FileLock(memory_file):
                if memory_file.exists():
                    # è¿½åŠ åˆ°ç¾æœ‰æª”æ¡ˆ
                    with open(memory_file, 'a', encoding='utf-8') as f:
                        f.write(entry)
                else:
                    # å‰µå»ºæ–°æª”æ¡ˆï¼ˆä½¿ç”¨åŸå­å¯«å…¥ï¼‰
                    header = f"# AI Memory for {project_id}\n\n"
                    header += f"Created: {timestamp}\n\n"
                    with AtomicFileWriter(memory_file) as f:
                        f.write(header + entry)
            
            logger.info(f"Memory saved for project: {project_id}")
            return True
            
        except TimeoutError as e:
            logger.error(f"Timeout acquiring lock for {project_id}: {e}")
            return False
        except Exception as e:
            logger.error(f"Error saving memory for {project_id}: {e}")
            return False

    def search_memory(self, project_id: str, query: str, limit: int = 10) -> List[Dict[str, str]]:
        """æœå°‹è¨˜æ†¶å…§å®¹"""
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return []

            results = []
            sections = self._parse_memory_sections(memory)
            
            for section in sections:
                if query.lower() in section['content'].lower():
                    results.append({
                        'timestamp': section['timestamp'],
                        'title': section['title'],
                        'category': section['category'],
                        'content': section['content'][:500] + "..." if len(section['content']) > 500 else section['content'],
                        'relevance': section['content'].lower().count(query.lower())
                    })
            
            # æŒ‰ç›¸é—œæ€§æ’åº
            results.sort(key=lambda x: x['relevance'], reverse=True)
            return results[:limit]
            
        except Exception as e:
            logger.error(f"Error searching memory for {project_id}: {e}")
            return []

    def _parse_memory_sections(self, memory: str) -> List[Dict[str, str]]:
        """è§£æè¨˜æ†¶æª”æ¡ˆçš„å„å€‹å€æ®µ"""
        sections = []
        lines = memory.split('\n')
        current_section = None
        current_content = []

        for line in lines:
            if line.startswith('## '):
                # å„²å­˜ä¸Šä¸€å€‹å€æ®µ
                if current_section:
                    sections.append({
                        'timestamp': current_section['timestamp'],
                        'title': current_section['title'],
                        'category': current_section['category'],
                        'content': '\n'.join(current_content).strip()
                    })
                
                # è§£ææ–°å€æ®µæ¨™é¡Œ
                header = line[3:].strip()
                timestamp, title, category = self._parse_section_header(header)
                current_section = {
                    'timestamp': timestamp,
                    'title': title,
                    'category': category
                }
                current_content = []
                
            elif line.strip() == '---':
                # å€æ®µçµæŸ
                continue
            else:
                current_content.append(line)

        # è™•ç†æœ€å¾Œä¸€å€‹å€æ®µ
        if current_section:
            sections.append({
                'timestamp': current_section['timestamp'],
                'title': current_section['title'],
                'category': current_section['category'],
                'content': '\n'.join(current_content).strip()
            })

        return sections

    def _parse_section_header(self, header: str) -> Tuple[str, str, str]:
        """è§£æå€æ®µæ¨™é¡Œï¼Œæå–æ™‚é–“æˆ³ã€æ¨™é¡Œå’Œåˆ†é¡"""
        timestamp = ""
        title = ""
        category = ""
        
        # æå–åˆ†é¡ (hashtag)
        if '#' in header:
            header, category = header.split('#', 1)
            category = category.strip()
        
        # æå–æ™‚é–“æˆ³å’Œæ¨™é¡Œ
        if ' - ' in header:
            timestamp, title = header.split(' - ', 1)
            timestamp = timestamp.strip()
            title = title.strip()
        else:
            timestamp = header.strip()
        
        return timestamp, title, category

    def list_projects(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆåŠå…¶çµ±è¨ˆè³‡è¨Šï¼ˆæ’é™¤å…¨å±€è¨˜æ†¶ï¼‰"""
        projects = []
        for file in self.memory_dir.glob("*.md"):
            # éæ¿¾æ‰å…¨å±€è¨˜æ†¶æª”æ¡ˆ
            if file.stem == "__global__":
                continue
                
            try:
                content = file.read_text(encoding='utf-8')
                sections = self._parse_memory_sections(content)
                projects.append({
                    'id': file.stem,
                    'name': file.stem.replace('-', ' ').title(),
                    'file_path': str(file),
                    'entries_count': len(sections),
                    'last_modified': datetime.fromtimestamp(file.stat().st_mtime).strftime("%Y-%m-%d %H:%M:%S"),
                    'categories': list(set(s['category'] for s in sections if s['category']))
                })
            except Exception as e:
                logger.error(f"Error reading project {file.stem}: {e}")
                continue
        
        return sorted(projects, key=lambda x: x['last_modified'], reverse=True)

    def get_recent_memory(self, project_id: str, limit: int = 5) -> List[Dict[str, str]]:
        """å–å¾—æœ€è¿‘çš„è¨˜æ†¶æ¢ç›®"""
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return []

            sections = self._parse_memory_sections(memory)
            
            # è¿”å›æœ€è¿‘çš„æ¢ç›®
            return sections[-limit:] if len(sections) > limit else sections
            
        except Exception as e:
            logger.error(f"Error getting recent memory for {project_id}: {e}")
            return []

    def delete_memory(self, project_id: str) -> bool:
        """åˆªé™¤å°ˆæ¡ˆè¨˜æ†¶æª”æ¡ˆï¼ˆä½¿ç”¨æª”æ¡ˆé–å®šï¼‰"""
        try:
            memory_file = self.get_memory_file(project_id)
            
            # ä½¿ç”¨æª”æ¡ˆé–å®šé€²è¡Œå®‰å…¨åˆªé™¤
            with FileLock(memory_file):
                if memory_file.exists():
                    memory_file.unlink()
                    logger.info(f"Memory deleted for project: {project_id}")
                    return True
                return False
                
        except TimeoutError as e:
            logger.error(f"Timeout acquiring lock for deleting {project_id}: {e}")
            return False
        except Exception as e:
            logger.error(f"Error deleting memory for {project_id}: {e}")
            return False

    def delete_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None, 
                           title: str = None, category: str = None, content_match: str = None) -> Dict[str, Any]:
        """åˆªé™¤ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return {'success': False, 'message': f'No memory found for project: {project_id}'}

            sections = self._parse_memory_sections(memory)
            original_count = len(sections)
            
            # æ ¹æ“šä¸åŒæ¢ä»¶ç¯©é¸è¦åˆªé™¤çš„æ¢ç›®
            sections_to_keep = []
            deleted_entries = []
            
            for i, section in enumerate(sections):
                should_delete = False
                
                # æ ¹æ“šç´¢å¼•åˆªé™¤ (entry_id æ˜¯å¾1é–‹å§‹çš„ç´¢å¼•)
                if entry_id is not None:
                    try:
                        entry_index = int(entry_id) - 1  # è½‰æ›ç‚º0åŸºç´¢å¼•
                        if i == entry_index:
                            should_delete = True
                    except ValueError:
                        pass
                
                # æ ¹æ“šæ™‚é–“æˆ³åˆªé™¤
                elif timestamp and timestamp in section['timestamp']:
                    should_delete = True
                
                # æ ¹æ“šæ¨™é¡Œåˆªé™¤
                elif title and title.lower() in section['title'].lower():
                    should_delete = True
                
                # æ ¹æ“šåˆ†é¡åˆªé™¤
                elif category and category.lower() in section['category'].lower():
                    should_delete = True
                
                # æ ¹æ“šå…§å®¹åŒ¹é…åˆªé™¤
                elif content_match and content_match.lower() in section['content'].lower():
                    should_delete = True
                
                if should_delete:
                    deleted_entries.append(section)
                else:
                    sections_to_keep.append(section)
            
            if len(deleted_entries) == 0:
                return {'success': False, 'message': 'No matching entries found to delete'}
            
            # é‡å»ºè¨˜æ†¶æª”æ¡ˆ
            success = self._rebuild_memory_file(project_id, sections_to_keep)
            
            if success:
                message = f"Deleted {len(deleted_entries)} entries from project {project_id}"
                return {
                    'success': True, 
                    'message': message,
                    'deleted_count': len(deleted_entries),
                    'remaining_count': len(sections_to_keep),
                    'deleted_entries': [{'timestamp': e['timestamp'], 'title': e['title']} for e in deleted_entries]
                }
            else:
                return {'success': False, 'message': 'Failed to update memory file'}
                
        except Exception as e:
            logger.error(f"Error deleting memory entry for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}

    def edit_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None,
                         new_title: str = None, new_category: str = None, new_content: str = None) -> Dict[str, Any]:
        """ç·¨è¼¯ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return {'success': False, 'message': f'No memory found for project: {project_id}'}

            sections = self._parse_memory_sections(memory)
            
            # æ‰¾åˆ°è¦ç·¨è¼¯çš„æ¢ç›®
            target_section = None
            target_index = -1
            
            for i, section in enumerate(sections):
                # æ ¹æ“šç´¢å¼•æŸ¥æ‰¾
                if entry_id is not None:
                    try:
                        entry_index = int(entry_id) - 1
                        if i == entry_index:
                            target_section = section
                            target_index = i
                            break
                    except ValueError:
                        pass
                
                # æ ¹æ“šæ™‚é–“æˆ³æŸ¥æ‰¾
                elif timestamp and timestamp in section['timestamp']:
                    target_section = section
                    target_index = i
                    break
            
            if target_section is None:
                return {'success': False, 'message': 'No matching entry found to edit'}
            
            # æ›´æ–°æ¢ç›®å…§å®¹
            if new_title is not None:
                sections[target_index]['title'] = new_title
            if new_category is not None:
                sections[target_index]['category'] = new_category
            if new_content is not None:
                sections[target_index]['content'] = new_content
            
            # é‡å»ºè¨˜æ†¶æª”æ¡ˆ
            success = self._rebuild_memory_file(project_id, sections)
            
            if success:
                return {
                    'success': True,
                    'message': f"Successfully edited entry in project {project_id}",
                    'edited_entry': {
                        'timestamp': sections[target_index]['timestamp'],
                        'title': sections[target_index]['title'],
                        'category': sections[target_index]['category']
                    }
                }
            else:
                return {'success': False, 'message': 'Failed to update memory file'}
                
        except Exception as e:
            logger.error(f"Error editing memory entry for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}

    def list_memory_entries(self, project_id: str) -> Dict[str, Any]:
        """åˆ—å‡ºå°ˆæ¡ˆä¸­çš„æ‰€æœ‰è¨˜æ†¶æ¢ç›®ï¼Œå¸¶æœ‰ç´¢å¼•"""
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return {'success': False, 'message': f'No memory found for project: {project_id}'}

            sections = self._parse_memory_sections(memory)
            
            entries = []
            for i, section in enumerate(sections):
                entries.append({
                    'id': i + 1,  # 1-based index for user convenience
                    'timestamp': section['timestamp'],
                    'title': section['title'],
                    'category': section['category'],
                    'content_preview': section['content'][:100] + "..." if len(section['content']) > 100 else section['content']
                })
            
            return {
                'success': True,
                'total_entries': len(entries),
                'entries': entries
            }
            
        except Exception as e:
            logger.error(f"Error listing memory entries for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}

    def _rebuild_memory_file(self, project_id: str, sections: List[Dict[str, str]]) -> bool:
        """é‡å»ºè¨˜æ†¶æª”æ¡ˆï¼ˆä½¿ç”¨æª”æ¡ˆé–å®šï¼‰"""
        try:
            memory_file = self.get_memory_file(project_id)
            
            # ä½¿ç”¨æª”æ¡ˆé–å®šé€²è¡Œå®‰å…¨é‡å»º
            with FileLock(memory_file):
                if not sections:
                    # å¦‚æœæ²’æœ‰æ¢ç›®ï¼Œåˆªé™¤æª”æ¡ˆ
                    if memory_file.exists():
                        memory_file.unlink()
                    return True
                
                # é‡å»ºæª”æ¡ˆå…§å®¹
                content_parts = [f"# AI Memory for {project_id}\n\n"]
                content_parts.append(f"Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                for section in sections:
                    # é‡å»ºæ¢ç›®
                    entry_parts = [f"## {section['timestamp']}"]
                    if section['title']:
                        entry_parts.append(f" - {section['title']}")
                    if section['category']:
                        entry_parts.append(f" #{section['category']}")
                    
                    entry = "".join(entry_parts) + f"\n\n{section['content']}\n\n---\n\n"
                    content_parts.append(entry)
                
                # ä½¿ç”¨åŸå­å¯«å…¥
                with AtomicFileWriter(memory_file) as f:
                    f.write("".join(content_parts))
            
            logger.info(f"Memory file rebuilt for project: {project_id}")
            return True
            
        except TimeoutError as e:
            logger.error(f"Timeout acquiring lock for rebuilding {project_id}: {e}")
            return False
        except Exception as e:
            logger.error(f"Error rebuilding memory file for {project_id}: {e}")
            return False

    def get_memory_stats(self, project_id: str) -> Dict[str, Any]:
        """å–å¾—è¨˜æ†¶çµ±è¨ˆè³‡è¨Š"""
        try:
            memory = self.get_memory(project_id)
            if not memory:
                return {'exists': False}

            sections = self._parse_memory_sections(memory)
            categories = [s['category'] for s in sections if s['category']]
            
            return {
                'exists': True,
                'total_entries': len(sections),
                'total_words': len(memory.split()),
                'total_characters': len(memory),
                'categories': list(set(categories)),
                'latest_entry': sections[-1]['timestamp'] if sections else None,
                'oldest_entry': sections[0]['timestamp'] if sections else None
            }
            
        except Exception as e:
            logger.error(f"Error getting memory stats for {project_id}: {e}")
            return {'exists': False, 'error': str(e)}

    def rename_project(self, old_project_id: str, new_project_id: str) -> bool:
        """é‡æ–°å‘½åå°ˆæ¡ˆï¼ˆç§»å‹•æª”æ¡ˆä¸¦æ›´æ–°æ¨™é¡Œï¼‰"""
        try:
            old_file = self.get_memory_file(old_project_id)
            new_file = self.get_memory_file(new_project_id)
            
            # æª¢æŸ¥èˆŠæª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if not old_file.exists():
                logger.error(f"Project {old_project_id} does not exist")
                return False
            
            # æª¢æŸ¥æ–°æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
            if new_file.exists():
                logger.error(f"Project {new_project_id} already exists")
                return False
            
            # ä½¿ç”¨æª”æ¡ˆé–å®šé€²è¡Œå®‰å…¨é‡å‘½å
            with FileLock(old_file):
                # è®€å–åŸå§‹å…§å®¹
                content = old_file.read_text(encoding='utf-8')
                
                # æ›´æ–°æª”æ¡ˆæ¨™é¡Œ
                lines = content.split('\n')
                if lines and lines[0].startswith('# AI Memory for '):
                    lines[0] = f"# AI Memory for {new_project_id}"
                
                updated_content = '\n'.join(lines)
                
                # å¯«å…¥æ–°æª”æ¡ˆ
                with AtomicFileWriter(new_file) as f:
                    f.write(updated_content)
                
                # åˆªé™¤èˆŠæª”æ¡ˆ
                old_file.unlink()
                
                logger.info(f"Project renamed from {old_project_id} to {new_project_id}")
                return True
                
        except TimeoutError as e:
            logger.error(f"Timeout acquiring lock for renaming {old_project_id}: {e}")
            return False
        except Exception as e:
            logger.error(f"Error renaming project from {old_project_id} to {new_project_id}: {e}")
            return False

class SQLiteBackend(MemoryBackend):
    """
    SQLite è¨˜æ†¶å¾Œç«¯
    SQLite Memory Backend
    
    ä½¿ç”¨ SQLite è³‡æ–™åº«å„²å­˜å’Œç®¡ç† AI è¨˜æ†¶ï¼Œæ”¯æ´å…¨æ–‡æœå°‹å’Œé«˜æ•ˆèƒ½æŸ¥è©¢
    Stores and manages AI memory using SQLite database with full-text search and high-performance queries
    
    Features / åŠŸèƒ½:
    - FTS5 å…¨æ–‡æœå°‹ / FTS5 full-text search
    - äº‹å‹™å®‰å…¨ / Transaction safety  
    - ç´¢å¼•å„ªåŒ– / Index optimization
    - è‡ªå‹•å‚™ä»½ / Automatic backup
    """
    
    def __init__(self, db_path: str = "ai-memory/memory.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_database()
    
    @contextmanager
    def get_connection(self):
        """å–å¾—è³‡æ–™åº«é€£æ¥"""
        conn = sqlite3.connect(
            self.db_path,
            timeout=30.0,
            check_same_thread=False
        )
        conn.row_factory = sqlite3.Row
        self._configure_connection(conn)
        try:
            yield conn
        finally:
            conn.close()
    
    def _configure_connection(self, conn):
        """é…ç½®é€£æ¥æœ€ä½³åŒ–åƒæ•¸"""
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")
        conn.execute("PRAGMA cache_size=10000")
        conn.execute("PRAGMA temp_store=MEMORY")
        conn.execute("PRAGMA foreign_keys=ON")
    
    def _init_database(self):
        """åˆå§‹åŒ–è³‡æ–™åº«çµæ§‹"""
        with self.get_connection() as conn:
            # å»ºç«‹å°ˆæ¡ˆè¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS projects (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    description TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # å»ºç«‹è¨˜æ†¶æ¢ç›®è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS memory_entries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    project_id TEXT NOT NULL,
                    title TEXT,
                    category TEXT,
                    content TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE
                )
            """)
            
            # å»ºç«‹ç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_project ON memory_entries(project_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_category ON memory_entries(category)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_created ON memory_entries(created_at)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_updated ON memory_entries(updated_at)")
            
            # å»ºç«‹çœŸæ­£çš„çµæ§‹åŒ– Index Table - è§£æ±º token æµªè²»å•é¡Œ
            conn.execute("""
                CREATE TABLE IF NOT EXISTS memory_index_v3 (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    project_id INTEGER NOT NULL,
                    
                    -- åŸºæœ¬è³‡è¨Š (ä¸éœ€è¦è®€å– content)
                    title TEXT NOT NULL,
                    entry_type TEXT NOT NULL DEFAULT 'discussion',    -- feature/bug/discussion/milestone/task
                    status TEXT DEFAULT 'active',                     -- active/completed/archived
                    priority INTEGER DEFAULT 1,                       -- 1-5
                    
                    -- éšå±¤çµæ§‹
                    parent_id INTEGER,
                    hierarchy_level INTEGER DEFAULT 0,
                    hierarchy_path TEXT,
                    
                    -- æ ¸å¿ƒå…§å®¹ (çµæ§‹åŒ–ï¼Œä¸éœ€è¦è§£æ)
                    summary TEXT NOT NULL,                            -- ç°¡çŸ­æ‘˜è¦ (50-100å­—)
                    description TEXT,                                 -- è©³ç´°æè¿° (å¯é¸)
                    keywords TEXT,                                    -- é—œéµå­—
                    tags TEXT,                                        -- æ¨™ç±¤
                    
                    -- é—œè¯è³‡è¨Š
                    related_entries TEXT,                             -- ç›¸é—œæ¢ç›®IDåˆ—è¡¨
                    "references" TEXT,                                -- å¤–éƒ¨åƒè€ƒ
                    
                    -- æ™‚é–“è³‡è¨Š
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    due_date TIMESTAMP,                               -- æˆªæ­¢æ—¥æœŸ (å¯é¸)
                    
                    -- åŸå§‹å…§å®¹ (åªåœ¨éœ€è¦æ™‚è¼‰å…¥)
                    full_content TEXT,                                -- å®Œæ•´åŸå§‹å…§å®¹
                    content_hash TEXT,                                -- å…§å®¹é›œæ¹Š
                    
                    FOREIGN KEY (project_id) REFERENCES projects_v2(id) ON DELETE CASCADE,
                    FOREIGN KEY (parent_id) REFERENCES memory_index_v3(id) ON DELETE SET NULL
                )
            """)
            
            # çµæ§‹åŒ– Index Table ç´¢å¼• - çœŸæ­£è§£æ±º token æµªè²»
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_index_v3_project ON memory_index_v3(project_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_index_v3_type ON memory_index_v3(entry_type)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_index_v3_status ON memory_index_v3(status)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_index_v3_priority ON memory_index_v3(priority)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_index_v3_parent ON memory_index_v3(parent_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_index_v3_hierarchy ON memory_index_v3(hierarchy_level)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_index_v3_keywords ON memory_index_v3(keywords)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_index_v3_title ON memory_index_v3(title)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_index_v3_created ON memory_index_v3(created_at)")
            
            # å»ºç«‹å…¨æ–‡æœå°‹è¡¨
            conn.execute("""
                CREATE VIRTUAL TABLE IF NOT EXISTS memory_fts USING fts5(
                    content,
                    title,
                    category,
                    content='memory_entries',
                    content_rowid='id'
                )
            """)
            
            # å»ºç«‹è§¸ç™¼å™¨ç¶­è­·å…¨æ–‡æœå°‹ç´¢å¼•
            conn.execute("""
                CREATE TRIGGER IF NOT EXISTS memory_fts_insert AFTER INSERT ON memory_entries BEGIN
                    INSERT INTO memory_fts(rowid, content, title, category) 
                    VALUES (new.id, new.content, COALESCE(new.title, ''), COALESCE(new.category, ''));
                END
            """)
            
            conn.execute("""
                CREATE TRIGGER IF NOT EXISTS memory_fts_delete AFTER DELETE ON memory_entries BEGIN
                    DELETE FROM memory_fts WHERE rowid = old.id;
                END
            """)
            
            conn.execute("""
                CREATE TRIGGER IF NOT EXISTS memory_fts_update AFTER UPDATE ON memory_entries BEGIN
                    DELETE FROM memory_fts WHERE rowid = old.id;
                    INSERT INTO memory_fts(rowid, content, title, category) 
                    VALUES (new.id, new.content, COALESCE(new.title, ''), COALESCE(new.category, ''));
                END
            """)
            
            # Phase 1: å»ºç«‹ v2 é«˜æ•ˆèƒ½è¡¨çµæ§‹
            self._init_v2_tables(conn)
            
            # åŸ·è¡Œè³‡æ–™é·ç§»ï¼ˆç¨ç«‹æ–¼è¡¨å‰µå»ºï¼‰
            self._migrate_existing_data(conn)
            
            conn.commit()
            logger.info(f"SQLite database initialized at: {self.db_path}")
    
    def _init_v2_tables(self, conn):
        """åˆå§‹åŒ– v2 é«˜æ•ˆèƒ½è¡¨çµæ§‹"""
        # å»ºç«‹ projects_v2 è¡¨ (INT ä¸»éµ)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS projects_v2 (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_key TEXT UNIQUE NOT NULL,
                name TEXT NOT NULL,
                description TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # å»ºç«‹ memory_entries_v2 è¡¨ (INT å¤–éµ)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS memory_entries_v2 (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER NOT NULL,
                title TEXT,
                category TEXT,
                content TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (project_id) REFERENCES projects_v2(id) ON DELETE CASCADE
            )
        """)
        
        # å»ºç«‹ v2 ç´¢å¼•
        conn.execute("CREATE INDEX IF NOT EXISTS idx_projects_v2_key ON projects_v2(project_key)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_v2_project_id ON memory_entries_v2(project_id)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_v2_category ON memory_entries_v2(category)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_v2_created_at ON memory_entries_v2(created_at)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_memory_v2_updated_at ON memory_entries_v2(updated_at)")
        
        # å»ºç«‹ v2 FTS5 å…¨æ–‡æœå°‹è¡¨
        conn.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS memory_entries_v2_fts USING fts5(
                title, category, content,
                content='memory_entries_v2',
                content_rowid='id'
            )
        """)
        
        # å»ºç«‹ v2 FTS5 è§¸ç™¼å™¨
        conn.execute("""
            CREATE TRIGGER IF NOT EXISTS memory_v2_fts_insert AFTER INSERT ON memory_entries_v2 BEGIN
                INSERT INTO memory_entries_v2_fts(rowid, title, category, content) 
                VALUES (new.id, COALESCE(new.title, ''), COALESCE(new.category, ''), new.content);
            END
        """)
        
        conn.execute("""
            CREATE TRIGGER IF NOT EXISTS memory_v2_fts_delete AFTER DELETE ON memory_entries_v2 BEGIN
                DELETE FROM memory_entries_v2_fts WHERE rowid = old.id;
            END
        """)
        
        conn.execute("""
            CREATE TRIGGER IF NOT EXISTS memory_v2_fts_update AFTER UPDATE ON memory_entries_v2 BEGIN
                DELETE FROM memory_entries_v2_fts WHERE rowid = old.id;
                INSERT INTO memory_entries_v2_fts(rowid, title, category, content) 
                VALUES (new.id, COALESCE(new.title, ''), COALESCE(new.category, ''), new.content);
            END
        """)
        
        # å»ºç«‹é·ç§»ç‹€æ…‹è¡¨
        conn.execute("""
            CREATE TABLE IF NOT EXISTS migration_state (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
    
    def _migrate_existing_data(self, conn):
        """é·ç§»ç¾æœ‰è³‡æ–™åˆ° v2 è¡¨"""
        logger.info("Starting data migration check...")
        
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“é·ç§»é
        cursor = conn.execute("SELECT value FROM migration_state WHERE key = 'data_migrated'")
        result = cursor.fetchone()
        logger.info(f"Migration check result: {result}")
        logger.info(f"Result type: {type(result)}")
        if result:
            logger.info(f"Result[0]: {result[0]}")
            logger.info(f"Result[0] == 'true': {result[0] == 'true'}")
        
        if result and result[0] == 'true':
            logger.info("Data already migrated, skipping...")
            return  # å·²ç¶“é·ç§»éï¼Œè·³é
        
        logger.info("Starting data migration...")
        
        # é·ç§»å°ˆæ¡ˆè³‡æ–™
        conn.execute("""
            INSERT OR IGNORE INTO projects_v2 (project_key, name, description, created_at, updated_at)
            SELECT id, name, COALESCE(description, ''), created_at, updated_at 
            FROM projects
        """)
        
        # é·ç§»è¨˜æ†¶æ¢ç›®è³‡æ–™
        conn.execute("""
            INSERT OR IGNORE INTO memory_entries_v2 (project_id, title, category, content, created_at, updated_at)
            SELECT p2.id, me.title, me.category, me.content, me.created_at, me.updated_at
            FROM memory_entries me
            JOIN projects p1 ON me.project_id = p1.id
            JOIN projects_v2 p2 ON p1.id = p2.project_key
        """)
        
        # æ¨™è¨˜é·ç§»å®Œæˆ
        conn.execute("""
            INSERT OR REPLACE INTO migration_state (key, value, updated_at)
            VALUES ('data_migrated', 'true', CURRENT_TIMESTAMP)
        """)
        
        logger.info("Data migration to v2 tables completed")
    
    def save_memory(self, project_id: str, content: str, title: str = "", category: str = "") -> bool:
        """å„²å­˜è¨˜æ†¶åˆ° SQLite è³‡æ–™åº«ï¼ˆåƒ…ä½¿ç”¨ V2 è¡¨ï¼Œæå‡æ•ˆèƒ½ï¼‰"""
        try:
            with self.get_connection() as conn:
                # === åƒ…ä½¿ç”¨ V2 è¡¨ï¼ˆé«˜æ•ˆèƒ½æ¨¡å¼ï¼‰ ===
                try:
                    # ç¢ºä¿ V2 å°ˆæ¡ˆå­˜åœ¨ä¸¦ç²å– INT project_id
                    v2_project_id = self._ensure_v2_project_exists(conn, project_id)
                    
                    if v2_project_id:
                        
                        # æ’å…¥ V2 è¨˜æ†¶æ¢ç›®
                        v2_cursor = conn.execute("""
                            INSERT INTO memory_entries_v2 (project_id, title, category, content)
                            VALUES (?, ?, ?, ?)
                        """, (v2_project_id, title or None, category or None, content))
                        
                        v2_entry_id = v2_cursor.lastrowid
                        
                        # ğŸš€ å»ºç«‹çœŸæ­£çš„çµæ§‹åŒ– Index Table - ä¸å†æµªè²» tokenï¼
                        try:
                            # æ™ºèƒ½ç”Ÿæˆçµæ§‹åŒ–è³‡è¨Š
                            summary = self._generate_summary(content, title)
                            keywords = self._extract_keywords(content, title, category)
                            
                            # æª¢æ¸¬æ¢ç›®é¡å‹
                            entry_type = 'discussion'  # é è¨­
                            if category:
                                    if any(keyword in category.lower() for keyword in ['bug', 'fix', 'error', 'issue']):
                                        entry_type = 'bug'
                                    elif any(keyword in category.lower() for keyword in ['feature', 'implement', 'design']):
                                        entry_type = 'feature'
                                    elif any(keyword in category.lower() for keyword in ['milestone', 'progress', 'complete']):
                                        entry_type = 'milestone'
                                    elif any(keyword in category.lower() for keyword in ['task', 'todo', 'action']):
                                        entry_type = 'task'
                            
                            # è¨­å®šé è¨­å€¼
                            status = 'active'
                            priority = 1
                            hierarchy_level = self._detect_hierarchy_level(content, title)
                            content_hash = self._generate_content_hash(content)
                            
                            # æ’å…¥çµæ§‹åŒ– Index Table (ä¸»è¦è³‡æ–™çµæ§‹)
                            conn.execute("""
                                INSERT INTO memory_index_v3 (
                                    project_id, title, entry_type, status, priority,
                                    hierarchy_level, summary, keywords, tags,
                                    full_content, content_hash, created_at, updated_at
                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                            """, (v2_project_id, title or "Untitled", entry_type, status, priority,
                                 hierarchy_level, summary, keywords, category, content, content_hash))
                            
                            logger.info(f"âœ… Created structured index entry for {project_id}: type={entry_type}, level={hierarchy_level}, priority={priority}")
                            
                        except Exception as index_error:
                            logger.warning(f"âš ï¸ Failed to create structured index entry for {project_id}: {index_error}")
                        
                        logger.info(f"Successfully saved to both v1 and v2 tables for project {project_id}")
                    else:
                        logger.warning(f"Failed to get V2 project ID for {project_id}")
                        
                except Exception as v2_error:
                    logger.warning(f"V2 save failed for project {project_id}, but v1 save succeeded: {v2_error}")
                
                # æ›´æ–°å°ˆæ¡ˆçš„ updated_at
                conn.execute("""
                    UPDATE projects SET updated_at = CURRENT_TIMESTAMP 
                    WHERE id = ?
                """, (project_id,))
                
                # === V2 è¡¨æ“ä½œï¼ˆé›™å¯«ï¼‰ ===
                try:
                    # ç¢ºä¿ v2 å°ˆæ¡ˆå­˜åœ¨ï¼Œç²å–æˆ–å‰µå»º INT ä¸»éµ
                    cursor = conn.execute("""
                        INSERT OR IGNORE INTO projects_v2 (project_key, name) 
                        VALUES (?, ?)
                    """, (project_id, project_id.replace('-', ' ').title()))
                    
                    # ä¿®æ­£ï¼šä½¿ç”¨æ–°çš„è¼”åŠ©æ–¹æ³•ç¢ºä¿ V2 å°ˆæ¡ˆå­˜åœ¨
                    v2_project_id = self._ensure_v2_project_exists(conn, project_id)
                    if v2_project_id is None:
                        raise Exception(f"Failed to create/get V2 project for {project_id}")
                    
                    # æ’å…¥ v2 è¨˜æ†¶æ¢ç›®
                    conn.execute("""
                        INSERT INTO memory_entries_v2 (project_id, title, category, content)
                        VALUES (?, ?, ?, ?)
                    """, (v2_project_id, title or None, category or None, content))
                    
                    # æ›´æ–° v2 å°ˆæ¡ˆçš„ updated_at
                    conn.execute("""
                        UPDATE projects_v2 SET updated_at = CURRENT_TIMESTAMP 
                        WHERE id = ?
                    """, (v2_project_id,))
                    
                    logger.info(f"V2 write successful for project: {project_id}")
                    
                except Exception as v2_error:
                    logger.error(f"V2 write failed for {project_id}: {v2_error}")
                    raise v2_error
                
                conn.commit()
                logger.info(f"Memory saved for project: {project_id}")
                return True
                
        except Exception as e:
            logger.error(f"Error saving memory for {project_id}: {e}")
            return False
    
    def get_memory(self, project_id: str) -> Optional[str]:
        """è®€å–å®Œæ•´å°ˆæ¡ˆè¨˜æ†¶ï¼Œè½‰æ›ç‚º Markdown æ ¼å¼ï¼ˆä½¿ç”¨ V2 è¡¨ï¼‰"""
        try:
            with self.get_connection() as conn:
                # ç²å– V2 å°ˆæ¡ˆ ID
                v2_project_id = self._get_v2_project_id(conn, project_id)
                if v2_project_id is None:
                    return None
                
                # å–å¾—æ‰€æœ‰è¨˜æ†¶æ¢ç›®ï¼ˆå¾ V2 è¡¨ï¼‰
                cursor = conn.execute("""
                    SELECT title, category, content, created_at
                    FROM memory_entries_v2 
                    WHERE project_id = ?
                    ORDER BY created_at ASC
                """, (v2_project_id,))
                
                entries = cursor.fetchall()
                if not entries:
                    return None
                
                # è½‰æ›ç‚º Markdown æ ¼å¼
                markdown_parts = [f"# AI Memory for {project_id}\n\n"]
                
                for entry in entries:
                    # æ ¼å¼åŒ–æ™‚é–“æˆ³
                    timestamp = entry['created_at']
                    if isinstance(timestamp, str):
                        # SQLite è¿”å›çš„å¯èƒ½æ˜¯å­—ä¸²æ ¼å¼
                        try:
                            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                            formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            formatted_time = timestamp
                    else:
                        formatted_time = timestamp
                    
                    # å»ºç«‹æ¢ç›®æ¨™é¡Œ
                    header_parts = [f"## {formatted_time}"]
                    if entry['title']:
                        header_parts.append(f" - {entry['title']}")
                    if entry['category']:
                        header_parts.append(f" #{entry['category']}")
                    
                    # çµ„åˆæ¢ç›®
                    entry_md = "".join(header_parts) + f"\n\n{entry['content']}\n\n---\n\n"
                    markdown_parts.append(entry_md)
                
                return "".join(markdown_parts)
                
        except Exception as e:
            logger.error(f"Error reading memory for {project_id}: {e}")
            return None
    
    def search_memory(self, project_id: str, query: str, limit: int = 10) -> List[Dict[str, str]]:
        """ä½¿ç”¨ FTS5 æœå°‹è¨˜æ†¶å…§å®¹ï¼Œå¦‚æœå¤±æ•—å‰‡ä½¿ç”¨ LIKE å‚™ç”¨æœå°‹"""
        try:
            # é¦–å…ˆå˜—è©¦ FTS5 å…¨æ–‡æœå°‹
            fts_results = self._fts_search(project_id, query, limit)
            
            # å¦‚æœ FTS5 æœå°‹çµæœç‚ºç©ºï¼Œä½¿ç”¨ LIKE å‚™ç”¨æœå°‹
            if not fts_results:
                logger.info(f"FTS5 search returned no results for '{query}', trying LIKE search")
                like_results = self._like_search(project_id, query, limit)
                if like_results:
                    logger.info(f"LIKE search found {len(like_results)} results for '{query}'")
                return like_results
            
            return fts_results
                
        except Exception as e:
            logger.error(f"Error searching memory for {project_id}: {e}")
            # å¦‚æœ FTS5 æœå°‹å‡ºç¾ç•°å¸¸ï¼Œå˜—è©¦ LIKE å‚™ç”¨æœå°‹
            try:
                logger.info(f"FTS5 search failed, trying LIKE search as fallback")
                return self._like_search(project_id, query, limit)
            except Exception as fallback_error:
                logger.error(f"Fallback LIKE search also failed: {fallback_error}")
                return []
    
    def _fts_search(self, project_id: str, query: str, limit: int) -> List[Dict[str, str]]:
        """ä½¿ç”¨ FTS5 å…¨æ–‡æœå°‹"""
        with self.get_connection() as conn:
            cursor = conn.execute("""
                SELECT me.title, me.category, me.content, me.created_at,
                       rank
                FROM memory_fts 
                JOIN memory_entries me ON memory_fts.rowid = me.id
                WHERE memory_fts MATCH ? AND me.project_id = ?
                ORDER BY rank
                LIMIT ?
            """, (query, project_id, limit))
            
            results = []
            for row in cursor.fetchall():
                results.append(self._format_search_result(row, 1))
            
            return results
    
    def _like_search(self, project_id: str, query: str, limit: int) -> List[Dict[str, str]]:
        """ä½¿ç”¨ LIKE æŸ¥è©¢ä½œç‚ºå‚™ç”¨æœå°‹æ–¹æ¡ˆ"""
        with self.get_connection() as conn:
            # ä½¿ç”¨ LIKE æŸ¥è©¢æœå°‹æ¨™é¡Œã€åˆ†é¡å’Œå…§å®¹
            like_pattern = f"%{query}%"
            cursor = conn.execute("""
                SELECT title, category, content, created_at
                FROM memory_entries 
                WHERE project_id = ? AND (
                    content LIKE ? OR 
                    title LIKE ? OR 
                    category LIKE ?
                )
                ORDER BY created_at DESC
                LIMIT ?
            """, (project_id, like_pattern, like_pattern, like_pattern, limit))
            
            results = []
            for row in cursor.fetchall():
                # è¨ˆç®—ç›¸é—œæ€§ï¼ˆå‡ºç¾æ¬¡æ•¸ï¼‰
                content = row['content'] or ''
                title = row['title'] or ''
                category = row['category'] or ''
                
                relevance = (
                    content.lower().count(query.lower()) +
                    title.lower().count(query.lower()) * 2 +  # æ¨™é¡Œæ¬Šé‡æ›´é«˜
                    category.lower().count(query.lower()) * 1.5  # åˆ†é¡æ¬Šé‡ä¸­ç­‰
                )
                
                results.append(self._format_search_result(row, relevance))
            
            # æŒ‰ç›¸é—œæ€§æ’åº
            results.sort(key=lambda x: x['relevance'], reverse=True)
            return results
    
    def _format_search_result(self, row, relevance: float) -> Dict[str, str]:
        """æ ¼å¼åŒ–æœå°‹çµæœ"""
        # æ ¼å¼åŒ–æ™‚é–“æˆ³
        timestamp = row['created_at']
        if isinstance(timestamp, str):
            try:
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
            except:
                formatted_time = timestamp
        else:
            formatted_time = str(timestamp)
        
        content = row['content'] or ''
        return {
            'timestamp': formatted_time,
            'title': row['title'] or '',
            'category': row['category'] or '',
            'content': content[:500] + "..." if len(content) > 500 else content,
            'relevance': relevance
        }
    
    def list_projects(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆåŠå…¶çµ±è¨ˆè³‡è¨Šï¼ˆæ’é™¤å…¨å±€è¨˜æ†¶ï¼‰"""
        try:
            with self.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT p.id, p.name, p.created_at, p.updated_at,
                           COUNT(me.id) as entries_count,
                           GROUP_CONCAT(DISTINCT me.category) as categories
                    FROM projects p
                    LEFT JOIN memory_entries me ON p.id = me.project_id
                    WHERE p.id != '__global__'
                    GROUP BY p.id, p.name, p.created_at, p.updated_at
                    ORDER BY p.updated_at DESC
                """)
                
                projects = []
                for row in cursor.fetchall():
                    # è™•ç†åˆ†é¡
                    categories = []
                    if row['categories']:
                        categories = [cat.strip() for cat in row['categories'].split(',') if cat.strip()]
                    
                    # æ ¼å¼åŒ–æ™‚é–“
                    updated_at = row['updated_at']
                    if isinstance(updated_at, str):
                        try:
                            dt = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
                            formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            formatted_time = updated_at
                    else:
                        formatted_time = str(updated_at)
                    
                    projects.append({
                        'id': row['id'],
                        'name': row['name'],
                        'file_path': f"SQLite: {self.db_path}",
                        'entries_count': row['entries_count'],
                        'last_modified': formatted_time,
                        'categories': categories
                    })
                
                return projects
                
        except Exception as e:
            logger.error(f"Error listing projects: {e}")
            return []
    
    def get_recent_memory(self, project_id: str, limit: int = 5) -> List[Dict[str, str]]:
        """å–å¾—æœ€è¿‘çš„è¨˜æ†¶æ¢ç›®"""
        try:
            with self.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT title, category, content, created_at
                    FROM memory_entries 
                    WHERE project_id = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """, (project_id, limit))
                
                results = []
                for row in cursor.fetchall():
                    # æ ¼å¼åŒ–æ™‚é–“æˆ³
                    timestamp = row['created_at']
                    if isinstance(timestamp, str):
                        try:
                            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                            formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            formatted_time = timestamp
                    else:
                        formatted_time = str(timestamp)
                    
                    results.append({
                        'timestamp': formatted_time,
                        'title': row['title'] or '',
                        'category': row['category'] or '',
                        'content': row['content']
                    })
                
                return list(reversed(results))  # è¿”å›æ™‚é–“é †åºï¼ˆèˆŠåˆ°æ–°ï¼‰
                
        except Exception as e:
            logger.error(f"Error getting recent memory for {project_id}: {e}")
            return []
    
    def delete_memory(self, project_id: str) -> bool:
        """åˆªé™¤å°ˆæ¡ˆè¨˜æ†¶ï¼ˆé›™åˆªé™¤ v1 å’Œ v2 è¡¨ï¼‰"""
        try:
            with self.get_connection() as conn:
                # === V1 è¡¨æ“ä½œï¼ˆä¸»è¦ï¼‰ ===
                # åˆªé™¤è¨˜æ†¶æ¢ç›®ï¼ˆè§¸ç™¼å™¨æœƒè‡ªå‹•æ¸…ç† FTSï¼‰
                cursor = conn.execute("DELETE FROM memory_entries WHERE project_id = ?", (project_id,))
                deleted_entries = cursor.rowcount
                
                # åˆªé™¤å°ˆæ¡ˆ
                cursor = conn.execute("DELETE FROM projects WHERE id = ?", (project_id,))
                deleted_project = cursor.rowcount
                
                # === V2 è¡¨æ“ä½œï¼ˆé›™åˆªé™¤ï¼‰ ===
                v2_deleted_entries = 0
                v2_deleted_project = 0
                try:
                    # ç²å– v2 å°ˆæ¡ˆçš„ INT ä¸»éµ
                    cursor = conn.execute("SELECT id FROM projects_v2 WHERE project_key = ?", (project_id,))
                    result = cursor.fetchone()
                    
                    if result:
                        v2_project_id = result[0]
                        
                        # åˆªé™¤ v2 è¨˜æ†¶æ¢ç›®ï¼ˆè§¸ç™¼å™¨æœƒè‡ªå‹•æ¸…ç† FTSï¼‰
                        cursor = conn.execute("DELETE FROM memory_entries_v2 WHERE project_id = ?", (v2_project_id,))
                        v2_deleted_entries = cursor.rowcount
                        
                        # åˆªé™¤ v2 å°ˆæ¡ˆ
                        cursor = conn.execute("DELETE FROM projects_v2 WHERE id = ?", (v2_project_id,))
                        v2_deleted_project = cursor.rowcount
                        
                        logger.info(f"Dual-delete successful for project: {project_id} (v2: {v2_deleted_entries} entries, {v2_deleted_project} project)")
                    else:
                        logger.warning(f"Project {project_id} not found in v2 tables")
                        
                except Exception as v2_error:
                    logger.warning(f"V2 delete failed for {project_id}, but V1 succeeded: {v2_error}")
                    # V2 å¤±æ•—ä¸å½±éŸ¿ V1 æ“ä½œçš„æˆåŠŸ
                
                conn.commit()
                
                if deleted_entries > 0 or deleted_project > 0:
                    logger.info(f"Memory deleted for project: {project_id} (v1: {deleted_entries} entries, {deleted_project} project)")
                    return True
                return False
                
        except Exception as e:
            logger.error(f"Error deleting memory for {project_id}: {e}")
            return False
    
    def delete_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None, 
                           title: str = None, category: str = None, content_match: str = None) -> Dict[str, Any]:
        """åˆªé™¤ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        try:
            with self.get_connection() as conn:
                # å»ºç«‹æŸ¥è©¢æ¢ä»¶
                where_conditions = ["project_id = ?"]
                params = [project_id]
                
                if entry_id is not None:
                    try:
                        where_conditions.append("id = ?")
                        params.append(int(entry_id))
                    except ValueError:
                        return {'success': False, 'message': 'Invalid entry_id format'}
                
                if timestamp:
                    where_conditions.append("created_at LIKE ?")
                    params.append(f"%{timestamp}%")
                
                if title:
                    where_conditions.append("title LIKE ?")
                    params.append(f"%{title}%")
                
                if category:
                    where_conditions.append("category LIKE ?")
                    params.append(f"%{category}%")
                
                if content_match:
                    where_conditions.append("content LIKE ?")
                    params.append(f"%{content_match}%")
                
                # å…ˆæŸ¥è©¢è¦åˆªé™¤çš„æ¢ç›®
                select_sql = f"""
                    SELECT id, title, created_at FROM memory_entries 
                    WHERE {' AND '.join(where_conditions)}
                """
                cursor = conn.execute(select_sql, params)
                entries_to_delete = cursor.fetchall()
                
                if not entries_to_delete:
                    return {'success': False, 'message': 'No matching entries found to delete'}
                
                # åŸ·è¡Œ v1 è¡¨åˆªé™¤
                delete_sql = f"DELETE FROM memory_entries WHERE {' AND '.join(where_conditions)}"
                cursor = conn.execute(delete_sql, params)
                deleted_count = cursor.rowcount
                
                # åŸ·è¡Œ v2 è¡¨é›™åˆªé™¤ï¼ˆå®¹éŒ¯è™•ç†ï¼‰
                try:
                    # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¢ºçš„ V2 å°ˆæ¡ˆ ID æ˜ å°„
                    v2_project_id = self._get_v2_project_id(conn, project_id)
                    
                    if v2_project_id is not None:
                        
                        # å»ºç«‹ v2 æŸ¥è©¢æ¢ä»¶
                        v2_where_conditions = ["project_id = ?"]
                        v2_params = [v2_project_id]
                        
                        # æ ¹æ“šåˆªé™¤çš„æ¢ç›®è³‡è¨Šåœ¨ v2 è¡¨ä¸­åŒ¹é…
                        for entry in entries_to_delete:
                            v2_where_conditions.append("created_at = ?")
                            v2_params.append(entry['created_at'])
                            break  # åªè™•ç†ç¬¬ä¸€å€‹åŒ¹é…æ¢ç›®ï¼Œé¿å…è¤‡é›œé‚è¼¯
                        
                        if len(entries_to_delete) == 1:  # ç²¾ç¢ºåŒ¹é…å–®å€‹æ¢ç›®
                            v2_delete_sql = f"DELETE FROM memory_entries_v2 WHERE {' AND '.join(v2_where_conditions)}"
                            conn.execute(v2_delete_sql, v2_params)
                        else:  # å¤šå€‹æ¢ç›®ï¼Œä½¿ç”¨æ™‚é–“ç¯„åœåŒ¹é…
                            for entry in entries_to_delete:
                                conn.execute("DELETE FROM memory_entries_v2 WHERE project_id = ? AND created_at = ?", 
                                           (v2_project_id, entry['created_at']))
                        
                        logger.info(f"Successfully deleted entries from both v1 and v2 tables for project {project_id}")
                    else:
                        logger.warning(f"Project {project_id} not found in v2 table, only deleted from v1")
                        
                except Exception as v2_error:
                    logger.warning(f"V2 delete failed for project {project_id}, but v1 delete succeeded: {v2_error}")
                
                conn.commit()
                
                # æ ¼å¼åŒ–å›æ‡‰
                deleted_entries = []
                for entry in entries_to_delete:
                    timestamp = entry['created_at']
                    if isinstance(timestamp, str):
                        try:
                            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                            formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            formatted_time = timestamp
                    else:
                        formatted_time = str(timestamp)
                    
                    deleted_entries.append({
                        'timestamp': formatted_time,
                        'title': entry['title'] or ''
                    })
                
                return {
                    'success': True,
                    'message': f"Deleted {deleted_count} entries from project {project_id}",
                    'deleted_count': deleted_count,
                    'remaining_count': self._count_entries(conn, project_id),
                    'deleted_entries': deleted_entries
                }
                
        except Exception as e:
            logger.error(f"Error deleting memory entry for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}
    
    def edit_memory_entry(self, project_id: str, entry_id: str = None, timestamp: str = None,
                         new_title: str = None, new_category: str = None, new_content: str = None) -> Dict[str, Any]:
        """ç·¨è¼¯ç‰¹å®šçš„è¨˜æ†¶æ¢ç›®"""
        try:
            with self.get_connection() as conn:
                # å»ºç«‹æŸ¥è©¢æ¢ä»¶
                where_conditions = ["project_id = ?"]
                params = [project_id]
                
                if entry_id is not None:
                    try:
                        where_conditions.append("id = ?")
                        params.append(int(entry_id))
                    except ValueError:
                        return {'success': False, 'message': 'Invalid entry_id format'}
                
                if timestamp:
                    where_conditions.append("created_at LIKE ?")
                    params.append(f"%{timestamp}%")
                
                # å…ˆæŸ¥è©¢æ¢ç›®æ˜¯å¦å­˜åœ¨
                select_sql = f"""
                    SELECT id, title, category, created_at FROM memory_entries 
                    WHERE {' AND '.join(where_conditions)}
                    LIMIT 1
                """
                cursor = conn.execute(select_sql, params)
                entry = cursor.fetchone()
                
                if not entry:
                    return {'success': False, 'message': 'No matching entry found to edit'}
                
                # å»ºç«‹æ›´æ–°èªå¥
                update_fields = []
                update_params = []
                
                if new_title is not None:
                    update_fields.append("title = ?")
                    update_params.append(new_title or None)
                
                if new_category is not None:
                    update_fields.append("category = ?")
                    update_params.append(new_category or None)
                
                if new_content is not None:
                    update_fields.append("content = ?")
                    update_params.append(new_content)
                
                if not update_fields:
                    return {'success': False, 'message': 'No fields to update'}
                
                update_fields.append("updated_at = CURRENT_TIMESTAMP")
                
                # åŸ·è¡Œ v1 è¡¨æ›´æ–°
                update_sql = f"""
                    UPDATE memory_entries 
                    SET {', '.join(update_fields)}
                    WHERE {' AND '.join(where_conditions)}
                """
                update_params.extend(params)
                conn.execute(update_sql, update_params)
                
                # åŸ·è¡Œ v2 è¡¨é›™ç·¨è¼¯ï¼ˆå®¹éŒ¯è™•ç†ï¼‰
                try:
                    # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¢ºçš„ V2 å°ˆæ¡ˆ ID æ˜ å°„
                    v2_project_id = self._get_v2_project_id(conn, project_id)
                    
                    if v2_project_id is not None:
                        
                        # å»ºç«‹ v2 æ›´æ–°æ¢ä»¶ï¼ˆä½¿ç”¨ created_at åŒ¹é…ï¼‰
                        v2_update_fields = []
                        v2_update_params = []
                        
                        if new_title is not None:
                            v2_update_fields.append("title = ?")
                            v2_update_params.append(new_title or None)
                        
                        if new_category is not None:
                            v2_update_fields.append("category = ?")
                            v2_update_params.append(new_category or None)
                        
                        if new_content is not None:
                            v2_update_fields.append("content = ?")
                            v2_update_params.append(new_content)
                        
                        if v2_update_fields:
                            v2_update_fields.append("updated_at = CURRENT_TIMESTAMP")
                            
                            # ä½¿ç”¨ created_at åŒ¹é… v2 è¡¨ä¸­çš„æ¢ç›®
                            v2_update_sql = f"""
                                UPDATE memory_entries_v2 
                                SET {', '.join(v2_update_fields)}
                                WHERE project_id = ? AND created_at = ?
                            """
                            v2_update_params.extend([v2_project_id, entry['created_at']])
                            conn.execute(v2_update_sql, v2_update_params)
                            
                            logger.info(f"Successfully edited entry in both v1 and v2 tables for project {project_id}")
                    else:
                        logger.warning(f"Project {project_id} not found in v2 table, only edited in v1")
                        
                except Exception as v2_error:
                    logger.warning(f"V2 edit failed for project {project_id}, but v1 edit succeeded: {v2_error}")
                
                conn.commit()
                
                # å–å¾—æ›´æ–°å¾Œçš„æ¢ç›®
                cursor = conn.execute(select_sql, params)
                updated_entry = cursor.fetchone()
                
                # æ ¼å¼åŒ–æ™‚é–“æˆ³
                timestamp = updated_entry['created_at']
                if isinstance(timestamp, str):
                    try:
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        formatted_time = timestamp
                else:
                    formatted_time = str(timestamp)
                
                return {
                    'success': True,
                    'message': f"Successfully edited entry in project {project_id}",
                    'edited_entry': {
                        'timestamp': formatted_time,
                        'title': updated_entry['title'] or '',
                        'category': updated_entry['category'] or ''
                    }
                }
                
        except Exception as e:
            logger.error(f"Error editing memory entry for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}
    
    def list_memory_entries(self, project_id: str) -> Dict[str, Any]:
        """åˆ—å‡ºå°ˆæ¡ˆä¸­çš„æ‰€æœ‰è¨˜æ†¶æ¢ç›®ï¼Œå¸¶æœ‰ç´¢å¼•"""
        try:
            with self.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT id, title, category, content, created_at
                    FROM memory_entries 
                    WHERE project_id = ?
                    ORDER BY created_at ASC
                """, (project_id,))
                
                entries = []
                for row in cursor.fetchall():
                    # æ ¼å¼åŒ–æ™‚é–“æˆ³
                    timestamp = row['created_at']
                    if isinstance(timestamp, str):
                        try:
                            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                            formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            formatted_time = timestamp
                    else:
                        formatted_time = str(timestamp)
                    
                    content = row['content']
                    entries.append({
                        'id': row['id'],
                        'timestamp': formatted_time,
                        'title': row['title'] or '',
                        'category': row['category'] or '',
                        'content_preview': content[:100] + "..." if len(content) > 100 else content
                    })
                
                return {
                    'success': True,
                    'total_entries': len(entries),
                    'entries': entries
                }
                
        except Exception as e:
            logger.error(f"Error listing memory entries for {project_id}: {e}")
            return {'success': False, 'message': f'Error: {str(e)}'}
    
    def get_memory_stats(self, project_id: str) -> Dict[str, Any]:
        """å–å¾—è¨˜æ†¶çµ±è¨ˆè³‡è¨Š"""
        try:
            with self.get_connection() as conn:
                # æª¢æŸ¥å°ˆæ¡ˆæ˜¯å¦å­˜åœ¨
                cursor = conn.execute("SELECT COUNT(*) FROM projects WHERE id = ?", (project_id,))
                if cursor.fetchone()[0] == 0:
                    return {'exists': False}
                
                # å–å¾—çµ±è¨ˆè³‡è¨Š
                cursor = conn.execute("""
                    SELECT 
                        COUNT(*) as total_entries,
                        SUM(LENGTH(content)) as total_characters,
                        MIN(created_at) as oldest_entry,
                        MAX(created_at) as latest_entry,
                        GROUP_CONCAT(DISTINCT category) as categories
                    FROM memory_entries 
                    WHERE project_id = ?
                """, (project_id,))
                
                stats = cursor.fetchone()
                
                # è™•ç†åˆ†é¡
                categories = []
                if stats['categories']:
                    categories = [cat.strip() for cat in stats['categories'].split(',') if cat.strip()]
                
                # æ ¼å¼åŒ–æ™‚é–“
                def format_timestamp(ts):
                    if not ts:
                        return None
                    if isinstance(ts, str):
                        try:
                            dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                            return dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            return ts
                    return str(ts)
                
                # è¨ˆç®—å­—æ•¸ï¼ˆç°¡å–®ä¼°ç®—ï¼‰
                total_words = (stats['total_characters'] or 0) // 5
                
                return {
                    'exists': True,
                    'total_entries': stats['total_entries'],
                    'total_words': total_words,
                    'total_characters': stats['total_characters'] or 0,
                    'categories': categories,
                    'latest_entry': format_timestamp(stats['latest_entry']),
                    'oldest_entry': format_timestamp(stats['oldest_entry'])
                }
                
        except Exception as e:
            logger.error(f"Error getting memory stats for {project_id}: {e}")
            return {'exists': False, 'error': str(e)}

    def rename_project(self, old_project_id: str, new_project_id: str) -> bool:
        """é‡æ–°å‘½åå°ˆæ¡ˆï¼ˆæ›´æ–°è³‡æ–™åº«ä¸­çš„å°ˆæ¡ˆ IDï¼‰"""
        try:
            with self.get_connection() as conn:
                # æª¢æŸ¥èˆŠå°ˆæ¡ˆæ˜¯å¦å­˜åœ¨
                cursor = conn.execute("SELECT COUNT(*) FROM projects WHERE id = ?", (old_project_id,))
                if cursor.fetchone()[0] == 0:
                    logger.error(f"Project {old_project_id} does not exist")
                    return False
                
                # æª¢æŸ¥æ–°å°ˆæ¡ˆ ID æ˜¯å¦å·²å­˜åœ¨
                cursor = conn.execute("SELECT COUNT(*) FROM projects WHERE id = ?", (new_project_id,))
                if cursor.fetchone()[0] > 0:
                    logger.error(f"Project {new_project_id} already exists")
                    return False
                
                # é–‹å§‹äº‹å‹™
                conn.execute("BEGIN TRANSACTION")
                
                try:
                    # å…ˆæ›´æ–°è¨˜æ†¶æ¢ç›®è¡¨ï¼ˆå­è¡¨ï¼‰
                    conn.execute("""
                        UPDATE memory_entries 
                        SET project_id = ? 
                        WHERE project_id = ?
                    """, (new_project_id, old_project_id))
                    
                    # å†æ›´æ–°å°ˆæ¡ˆè¡¨ï¼ˆçˆ¶è¡¨ï¼‰- ä½¿ç”¨ INSERT + DELETE æ–¹å¼é¿å…å¤–éµç´„æŸ
                    # å…ˆç²å–åŸå°ˆæ¡ˆè³‡æ–™
                    cursor = conn.execute("""
                        SELECT name, created_at FROM projects WHERE id = ?
                    """, (old_project_id,))
                    project_data = cursor.fetchone()
                    
                    if project_data:
                        # æ’å…¥æ–°å°ˆæ¡ˆè¨˜éŒ„
                        conn.execute("""
                            INSERT INTO projects (id, name, created_at, updated_at)
                            VALUES (?, ?, ?, CURRENT_TIMESTAMP)
                        """, (new_project_id, project_data[0], project_data[1]))
                        
                        # åˆªé™¤èˆŠå°ˆæ¡ˆè¨˜éŒ„
                        conn.execute("DELETE FROM projects WHERE id = ?", (old_project_id,))
                    
                    # v1 é‡å‘½åå®Œæˆï¼Œç¾åœ¨è™•ç† v2 è¡¨é›™é‡å‘½åï¼ˆå®¹éŒ¯è™•ç†ï¼‰
                    try:
                        # æª¢æŸ¥ v2 è¡¨ä¸­æ˜¯å¦å­˜åœ¨è©²å°ˆæ¡ˆ
                        v2_cursor = conn.execute("SELECT id FROM projects_v2 WHERE project_key = ?", (old_project_id,))
                        v2_project_result = v2_cursor.fetchone()
                        
                        if v2_project_result:
                            # v2 é‡å‘½åéå¸¸ç°¡å–®ï¼šåªéœ€è¦ UPDATE project_key
                            conn.execute("""
                                UPDATE projects_v2 
                                SET project_key = ?, updated_at = CURRENT_TIMESTAMP 
                                WHERE project_key = ?
                            """, (new_project_id, old_project_id))
                            
                            logger.info(f"Successfully renamed project in both v1 and v2 tables: {old_project_id} -> {new_project_id}")
                        else:
                            logger.warning(f"Project {old_project_id} not found in v2 table, only renamed in v1")
                            
                    except Exception as v2_error:
                        logger.warning(f"V2 rename failed for project {old_project_id}, but v1 rename succeeded: {v2_error}")
                    
                    # æäº¤äº‹å‹™
                    conn.execute("COMMIT")
                    
                    logger.info(f"Project renamed from {old_project_id} to {new_project_id}")
                    return True
                    
                except Exception as e:
                    # å›æ»¾äº‹å‹™
                    conn.execute("ROLLBACK")
                    logger.error(f"Error during rename transaction: {e}")
                    return False
                    
        except Exception as e:
            logger.error(f"Error renaming project from {old_project_id} to {new_project_id}: {e}")
            return False
    
    def _count_entries(self, conn, project_id: str) -> int:
        """è¨ˆç®—å°ˆæ¡ˆçš„è¨˜æ†¶æ¢ç›®æ•¸é‡"""
        cursor = conn.execute("SELECT COUNT(*) FROM memory_entries WHERE project_id = ?", (project_id,))
        return cursor.fetchone()[0]
    
    def _generate_summary(self, content: str, title: str = None) -> str:
        """ç”Ÿæˆå…§å®¹æ‘˜è¦ï¼Œå¤§å¹…æ¸›å°‘ token ä½¿ç”¨"""
        if not content:
            return title or "Empty content"
        
        # æ¸…ç†å…§å®¹
        content = content.strip()
        
        # å¦‚æœæœ‰æ¨™é¡Œï¼Œå„ªå…ˆä½¿ç”¨æ¨™é¡Œä½œç‚ºæ‘˜è¦åŸºç¤
        if title and title.strip():
            title_clean = title.strip()
            # å¦‚æœæ¨™é¡Œå·²ç¶“å¾ˆå¥½åœ°æ¦‚æ‹¬äº†å…§å®¹ï¼Œç›´æ¥ä½¿ç”¨
            if len(title_clean) > 10 and len(title_clean) < 100:
                return title_clean
        
        # æå–å‰200å­—å…ƒï¼Œä½†åœ¨å¥å­é‚Šç•Œæˆªæ–·
        if len(content) <= 200:
            return content
        
        summary = content[:200]
        
        # åœ¨å¥å­é‚Šç•Œæˆªæ–·
        sentence_endings = ['. ', 'ã€‚', 'ï¼', '!', 'ï¼Ÿ', '?', '\n\n']
        best_cut = 0
        
        for ending in sentence_endings:
            pos = summary.rfind(ending)
            if pos > 50:  # ç¢ºä¿æ‘˜è¦æœ‰è¶³å¤ é•·åº¦
                best_cut = max(best_cut, pos + len(ending))
        
        if best_cut > 0:
            return summary[:best_cut].strip()
        
        # å¦‚æœæ‰¾ä¸åˆ°å¥½çš„æˆªæ–·é»ï¼Œåœ¨è©é‚Šç•Œæˆªæ–·
        words = summary.split()
        if len(words) > 1:
            return ' '.join(words[:-1]) + '...'
        
        return summary + '...'
    
    def _extract_keywords(self, content: str, title: str = None, category: str = None) -> str:
        """æå–é—œéµå­—ï¼Œæå‡æœå°‹ç²¾åº¦"""
        keywords = set()
        
        # å¾æ¨™é¡Œæå–
        if title:
            title_words = title.replace('-', ' ').replace('_', ' ').split()
            keywords.update([w.lower() for w in title_words if len(w) > 2])
        
        # å¾åˆ†é¡æå–
        if category:
            category_words = category.replace('-', ' ').replace('_', ' ').split()
            keywords.update([w.lower() for w in category_words if len(w) > 2])
        
        # å¾å…§å®¹æå–æŠ€è¡“é—œéµå­—
        if content:
            # å¸¸è¦‹æŠ€è¡“è¡“èª
            tech_terms = ['api', 'sql', 'database', 'index', 'table', 'function', 'class', 'method', 
                         'bug', 'fix', 'error', 'issue', 'feature', 'implement', 'design', 'architecture',
                         'mcp', 'server', 'client', 'backend', 'frontend', 'token', 'memory', 'search',
                         'sqlite', 'markdown', 'hierarchy', 'optimization', 'performance']
            
            content_lower = content.lower()
            for term in tech_terms:
                if term in content_lower:
                    keywords.add(term)
        
        # é™åˆ¶é—œéµå­—æ•¸é‡ï¼Œé¿å…éé•·
        keywords_list = list(keywords)[:10]
        return ', '.join(sorted(keywords_list))
    
    def _detect_hierarchy_level(self, content: str, title: str = None) -> int:
        """æª¢æ¸¬éšå±¤ç´šåˆ¥ï¼Œæ”¯æ´éšå±¤çµæ§‹"""
        if not content:
            return 0
        
        # æª¢æŸ¥æ¨™é¡Œä¸­çš„ Markdown æ¨™è¨˜
        if title:
            if title.startswith('###'):
                return 2  # å°æ¨™é¡Œ
            elif title.startswith('##'):
                return 1  # ä¸­æ¨™é¡Œ
            elif title.startswith('#'):
                return 0  # å¤§æ¨™é¡Œ
        
        # æª¢æŸ¥å…§å®¹ä¸­çš„æ¨™è¨˜
        lines = content.split('\n')
        for line in lines[:5]:  # åªæª¢æŸ¥å‰5è¡Œ
            line = line.strip()
            if line.startswith('###'):
                return 2
            elif line.startswith('##'):
                return 1
            elif line.startswith('#'):
                return 0
        
        # æ ¹æ“šå…§å®¹ç‰¹å¾µåˆ¤æ–·
        if any(keyword in content.lower() for keyword in ['å¯¦ä½œ', 'implementation', 'æ­¥é©Ÿ', 'step', 'ç´°ç¯€', 'detail']):
            return 2  # å¯¦ä½œç´°ç¯€
        elif any(keyword in content.lower() for keyword in ['åŠŸèƒ½', 'feature', 'æ¨¡çµ„', 'module', 'éšæ®µ', 'phase']):
            return 1  # åŠŸèƒ½æ¨¡çµ„
        
        return 0  # é è¨­ç‚ºæ ¹ç´šåˆ¥
    
    def _generate_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå…§å®¹é›œæ¹Šï¼Œç”¨æ–¼æª¢æ¸¬è®Šæ›´"""
        import hashlib
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
    
    def _get_v2_project_id(self, conn, project_key: str) -> Optional[int]:
        """æ­£ç¢ºçš„æ˜ å°„ï¼šproject_key â†’ project_idï¼Œæ”¯æ´é‡å‘½å"""
        try:
            cursor = conn.execute("SELECT id FROM projects_v2 WHERE project_key = ?", (project_key,))
            result = cursor.fetchone()
            return result[0] if result else None
        except Exception as e:
            logger.error(f"Error getting V2 project ID for {project_key}: {e}")
            return None
    
    def _ensure_v2_project_exists(self, conn, project_key: str) -> Optional[int]:
        """ç¢ºä¿ V2 å°ˆæ¡ˆå­˜åœ¨ï¼Œè¿”å› project_id"""
        # å…ˆå˜—è©¦ç²å–ç¾æœ‰çš„ project_id
        project_id = self._get_v2_project_id(conn, project_key)
        if project_id is not None:
            return project_id
        
        # å¦‚æœä¸å­˜åœ¨ï¼Œå‰µå»ºæ–°çš„ V2 å°ˆæ¡ˆ
        try:
            cursor = conn.execute("""
                INSERT INTO projects_v2 (project_key, name, description, created_at, updated_at)
                VALUES (?, ?, '', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            """, (project_key, project_key.replace('-', ' ').title()))
            
            return cursor.lastrowid
        except Exception as e:
            logger.error(f"Error creating v2 project for {project_key}: {e}")
            # å†æ¬¡å˜—è©¦ç²å–ï¼Œå¯èƒ½æ˜¯ä¸¦ç™¼å‰µå»º
            return self._get_v2_project_id(conn, project_key)
    
    # ==================== INDEX TABLE METHODS ====================
    
    def search_index(self, project_id: str, query: str, limit: int = 10, 
                    entry_type: str = None, status: str = None, need_full_content: bool = False) -> List[Dict]:
        """çœŸæ­£çš„çµæ§‹åŒ–æœå°‹ - 90-95% token ç¯€çœï¼Œä¸å†æµªè²»ï¼"""
        try:
            with self.get_connection() as conn:
                # æ­£ç¢ºçš„åšæ³•ï¼šå»ºç«‹ project_key â†’ project_id æ˜ å°„
                v2_project_id = self._get_v2_project_id(conn, project_id)
                if not v2_project_id:
                    logger.warning(f"Project {project_id} not found in V2 table")
                    return []
                
                # å»ºç«‹æœå°‹æ¢ä»¶
                where_conditions = ["project_id = ?"]
                params = [v2_project_id]
                
                # æ–‡å­—æœå°‹æ¢ä»¶
                if query:
                    where_conditions.append("(title LIKE ? OR summary LIKE ? OR keywords LIKE ?)")
                    params.extend([f"%{query}%", f"%{query}%", f"%{query}%"])
                
                # é¡å‹ç¯©é¸
                if entry_type:
                    where_conditions.append("entry_type = ?")
                    params.append(entry_type)
                
                # ç‹€æ…‹ç¯©é¸
                if status:
                    where_conditions.append("status = ?")
                    params.append(status)
                
                # é¸æ“‡è¦è¼‰å…¥çš„æ¬„ä½ (é—œéµï¼šä¸è¼‰å…¥ full_content é™¤ééœ€è¦)
                if need_full_content:
                    select_fields = "*"
                else:
                    select_fields = """
                        id, project_id, title, entry_type, status, priority,
                        hierarchy_level, summary, keywords, tags, 
                        created_at, updated_at, due_date
                    """
                
                # åŸ·è¡Œçµæ§‹åŒ–æœå°‹ (è¶…çœ tokenï¼)
                cursor = conn.execute(f"""
                    SELECT {select_fields}
                    FROM memory_index_v3
                    WHERE {' AND '.join(where_conditions)}
                    ORDER BY priority DESC, hierarchy_level ASC, created_at DESC
                    LIMIT ?
                """, params + [limit])
                
                # è½‰æ›çµæœç‚ºå­—å…¸æ ¼å¼
                results = []
                for row in cursor.fetchall():
                    result = dict(row)
                    result['match_type'] = 'structured'
                    results.append(result)
                
                logger.info(f"âœ… Structured search found {len(results)} results for '{query}' in {project_id}")
                return results
                
        except Exception as e:
            logger.error(f"Error searching index for {project_id}: {e}")
            return []
    
    def get_index_entry(self, entry_id: int) -> Dict:
        """ç²å–ç‰¹å®šæ¢ç›®çš„ç´¢å¼•è³‡è¨Š - åŸºæ–¼ V2 è¡¨"""
        try:
            with self.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT mi.*, me.title, me.category, me.content, me.created_at as entry_created_at
                    FROM memory_index mi
                    JOIN memory_entries_v2 me ON mi.entry_id = me.id
                    WHERE mi.entry_id = ?
                """, (entry_id,))
                
                row = cursor.fetchone()
                if row:
                    return dict(row)
                return {}
                
        except Exception as e:
            logger.error(f"Error getting index entry {entry_id}: {e}")
            return {}
    
    def update_index_entry(self, entry_id: int, **kwargs) -> bool:
        """æ›´æ–°ç´¢å¼•æ¢ç›®"""
        try:
            with self.get_connection() as conn:
                # å…è¨±æ›´æ–°çš„æ¬„ä½
                allowed_fields = ['summary', 'keywords', 'hierarchy_level', 'entry_type', 
                                'importance_level', 'parent_entry_id', 'hierarchy_path']
                
                update_fields = []
                params = []
                
                for field, value in kwargs.items():
                    if field in allowed_fields:
                        update_fields.append(f"{field} = ?")
                        params.append(value)
                
                if not update_fields:
                    return False
                
                update_fields.append("updated_at = CURRENT_TIMESTAMP")
                params.append(entry_id)
                
                conn.execute(f"""
                    UPDATE memory_index 
                    SET {', '.join(update_fields)}
                    WHERE entry_id = ?
                """, params)
                
                conn.commit()
                logger.info(f"Updated index entry {entry_id}")
                return True
                
        except Exception as e:
            logger.error(f"Error updating index entry {entry_id}: {e}")
            return False
    
    def delete_index_entry(self, entry_id: int) -> bool:
        """åˆªé™¤ç´¢å¼•æ¢ç›®"""
        try:
            with self.get_connection() as conn:
                conn.execute("DELETE FROM memory_index WHERE entry_id = ?", (entry_id,))
                conn.commit()
                logger.info(f"Deleted index entry {entry_id}")
                return True
                
        except Exception as e:
            logger.error(f"Error deleting index entry {entry_id}: {e}")
            return False
    
    def get_hierarchy_tree(self, project_id: str) -> Dict:
        """ç²å–å°ˆæ¡ˆçš„éšå±¤æ¨¹ç‹€çµæ§‹ - åŸºæ–¼ V2 è¡¨"""
        try:
            with self.get_connection() as conn:
                # æ­£ç¢ºçš„æ˜ å°„
                v2_project_id = self._get_v2_project_id(conn, project_id)
                if not v2_project_id:
                    logger.warning(f"Project {project_id} not found in V2 table")
                    return {'children': []}
                
                cursor = conn.execute("""
                    SELECT mi.entry_id, mi.parent_entry_id, mi.hierarchy_level, 
                           mi.summary, mi.entry_type, me.title, me.created_at
                    FROM memory_index mi
                    JOIN memory_entries_v2 me ON mi.entry_id = me.id
                    WHERE mi.project_id = ?
                    ORDER BY mi.hierarchy_level ASC, me.created_at ASC
                """, (v2_project_id,))
                
                entries = [dict(row) for row in cursor.fetchall()]
                
                # å»ºç«‹éšå±¤æ¨¹
                tree = {'children': []}
                entry_map = {}
                
                for entry in entries:
                    entry['children'] = []
                    entry_map[entry['entry_id']] = entry
                    
                    if entry['parent_entry_id'] is None:
                        tree['children'].append(entry)
                    else:
                        parent = entry_map.get(entry['parent_entry_id'])
                        if parent:
                            parent['children'].append(entry)
                        else:
                            tree['children'].append(entry)  # å­¤å…’ç¯€é»
                
                return tree
                
        except Exception as e:
            logger.error(f"Error getting hierarchy tree for {project_id}: {e}")
            return {'children': []}
    
    def rebuild_index_for_project(self, project_id: str) -> Dict[str, Any]:
        """ç‚ºå°ˆæ¡ˆé‡å»ºæ‰€æœ‰ç´¢å¼•æ¢ç›® - åŸºæ–¼ V2 è¡¨"""
        try:
            with self.get_connection() as conn:
                # ä¿®æ­£ï¼šç²å– V2 å°ˆæ¡ˆ ID
                v2_project_id = self._get_v2_project_id(conn, project_id)
                
                if v2_project_id is None:
                    return {
                        'success': False,
                        'message': f"Project {project_id} not found in V2 table. Please save some memory first."
                    }
                
                # v2_project_id å·²ç¶“åœ¨ä¸Šé¢ç²å–äº†
                
                # å…ˆåˆªé™¤ç¾æœ‰ç´¢å¼•ï¼ˆä½¿ç”¨æ–°çš„ V3 è¡¨ï¼‰
                conn.execute("DELETE FROM memory_index_v3 WHERE project_id = ?", (v2_project_id,))
                
                # ç²å–æ‰€æœ‰ V2 è¨˜æ†¶æ¢ç›®
                cursor = conn.execute("""
                    SELECT id, title, category, content, created_at
                    FROM memory_entries_v2
                    WHERE project_id = ?
                    ORDER BY created_at ASC
                """, (v2_project_id,))
                
                entries = cursor.fetchall()
                created_count = 0
                
                for entry in entries:
                    try:
                        entry_id, title, category, content, created_at = entry
                        
                        # ç”Ÿæˆç´¢å¼•è³‡æ–™
                        summary = self._generate_summary(content, title)
                        keywords = self._extract_keywords(content, title, category)
                        hierarchy_level = self._detect_hierarchy_level(content, title)
                        content_hash = self._generate_content_hash(content)
                        
                        # æª¢æ¸¬æ¢ç›®é¡å‹
                        entry_type = 'discussion'
                        if category:
                            if any(keyword in category.lower() for keyword in ['bug', 'fix', 'error', 'issue']):
                                entry_type = 'bug'
                            elif any(keyword in category.lower() for keyword in ['feature', 'implement', 'design']):
                                entry_type = 'feature'
                            elif any(keyword in category.lower() for keyword in ['milestone', 'progress', 'complete']):
                                entry_type = 'milestone'
                        
                        # æ’å…¥ç´¢å¼•åˆ°æ–°çš„ V3 è¡¨
                        conn.execute("""
                            INSERT INTO memory_index_v3 (
                                project_id, title, entry_type, status, priority,
                                hierarchy_level, summary, keywords, tags,
                                full_content, content_hash, created_at, updated_at
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                        """, (v2_project_id, title or "Untitled", entry_type, 'active', 1,
                             hierarchy_level, summary, keywords, category or '', content, content_hash, created_at))
                        
                        created_count += 1
                        
                    except Exception as entry_error:
                        logger.warning(f"Failed to create index for entry {entry_id}: {entry_error}")
                
                conn.commit()
                
                return {
                    'success': True,
                    'message': f"Rebuilt index for project {project_id}",
                    'total_entries': len(entries),
                    'indexed_entries': created_count
                }
                
        except Exception as e:
            logger.error(f"Error rebuilding index for {project_id}: {e}")
            return {
                'success': False,
                'message': f"Error: {str(e)}"
            }
    
    def get_index_stats(self, project_id: str = None) -> Dict[str, Any]:
        """ç²å–ç´¢å¼•çµ±è¨ˆè³‡è¨Š - åŸºæ–¼ V2 è¡¨"""
        try:
            with self.get_connection() as conn:
                if project_id:
                    # ä¿®æ­£ï¼šç²å– V2 å°ˆæ¡ˆ ID
                    v2_project_id = self._get_v2_project_id(conn, project_id)
                    
                    if v2_project_id is None:
                        return {'total_indexed': 0, 'message': f'Project {project_id} not found in V2 table'}
                    
                    # v2_project_id å·²ç¶“åœ¨ä¸Šé¢ç²å–äº†
                    
                    # ç‰¹å®šå°ˆæ¡ˆçµ±è¨ˆ
                    cursor = conn.execute("""
                        SELECT 
                            COUNT(*) as total_indexed,
                            COUNT(DISTINCT entry_type) as entry_types,
                            MAX(hierarchy_level) as max_hierarchy_level,
                            COUNT(CASE WHEN parent_entry_id IS NOT NULL THEN 1 END) as has_parent
                        FROM memory_index 
                        WHERE project_id = ?
                    """, (v2_project_id,))
                    
                    stats = dict(cursor.fetchone())
                    
                    # æŒ‰é¡å‹çµ±è¨ˆ
                    cursor = conn.execute("""
                        SELECT entry_type, COUNT(*) as count
                        FROM memory_index 
                        WHERE project_id = ?
                        GROUP BY entry_type
                    """, (v2_project_id,))
                    
                    stats['by_type'] = {row[0]: row[1] for row in cursor.fetchall()}
                    
                    # æŒ‰éšå±¤çµ±è¨ˆ
                    cursor = conn.execute("""
                        SELECT hierarchy_level, COUNT(*) as count
                        FROM memory_index 
                        WHERE project_id = ?
                        GROUP BY hierarchy_level
                        ORDER BY hierarchy_level
                    """, (v2_project_id,))
                    
                    stats['by_hierarchy'] = {row[0]: row[1] for row in cursor.fetchall()}
                    
                else:
                    # å…¨å±€çµ±è¨ˆ
                    cursor = conn.execute("""
                        SELECT 
                            COUNT(*) as total_indexed,
                            COUNT(DISTINCT project_id) as indexed_projects,
                            COUNT(DISTINCT entry_type) as entry_types,
                            MAX(hierarchy_level) as max_hierarchy_level
                        FROM memory_index
                    """)
                    
                    stats = dict(cursor.fetchone())
                
                return stats
                
        except Exception as e:
            logger.error(f"Error getting index stats: {e}")
            return {}

class DataSyncManager:
    """
    è³‡æ–™åŒæ­¥ç®¡ç†å™¨ - è² è²¬ Markdown åˆ° SQLite çš„åŒæ­¥
    Data Sync Manager - Handles Markdown to SQLite synchronization
    
    æä¾›è‡ªå‹•åŒæ­¥åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç›¸ä¼¼åº¦æª¢æ¸¬ã€å…§å®¹åˆä½µå’Œè¡çªè§£æ±º
    Provides auto sync features including similarity detection, content merging, and conflict resolution
    
    Sync Modes / åŒæ­¥æ¨¡å¼:
    - auto: è‡ªå‹•åˆä½µ / Automatic merging
    - interactive: äº’å‹•å¼é¸æ“‡ / Interactive selection  
    - preview: é è¦½æ¨¡å¼ / Preview mode
    """
    
    def __init__(self, markdown_backend: MemoryBackend, sqlite_backend: MemoryBackend):
        self.markdown = markdown_backend
        self.sqlite = sqlite_backend
        self.sync_log = []
    
    def sync_all_projects(self, mode='auto', similarity_threshold=0.8):
        """åŒæ­¥æ‰€æœ‰ Markdown å°ˆæ¡ˆåˆ° SQLite
        
        Args:
            mode: 'auto', 'interactive', 'preview'
            similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ (0.0-1.0)
        """
        logger.info("é–‹å§‹åŒæ­¥ Markdown å°ˆæ¡ˆåˆ° SQLite...")
        
        # ç²å–æ‰€æœ‰ Markdown å°ˆæ¡ˆ
        markdown_projects = self.markdown.list_projects()
        
        if not markdown_projects:
            logger.info("æ²’æœ‰æ‰¾åˆ° Markdown å°ˆæ¡ˆ")
            return {'success': True, 'message': 'æ²’æœ‰å°ˆæ¡ˆéœ€è¦åŒæ­¥', 'synced': 0}
        
        logger.info(f"æ‰¾åˆ° {len(markdown_projects)} å€‹ Markdown å°ˆæ¡ˆ")
        
        synced_count = 0
        skipped_count = 0
        error_count = 0
        
        for project in markdown_projects:
            try:
                result = self.sync_project(project['id'], mode, similarity_threshold)
                if result['action'] == 'synced':
                    synced_count += 1
                elif result['action'] == 'skipped':
                    skipped_count += 1
                
                self.sync_log.append({
                    'project_id': project['id'],
                    'action': result['action'],
                    'message': result['message']
                })
                
            except Exception as e:
                error_count += 1
                error_msg = f"åŒæ­¥å°ˆæ¡ˆ {project['id']} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
                logger.error(error_msg)
                self.sync_log.append({
                    'project_id': project['id'],
                    'action': 'error',
                    'message': error_msg
                })
        
        summary = {
            'success': True,
            'total_projects': len(markdown_projects),
            'synced': synced_count,
            'skipped': skipped_count,
            'errors': error_count,
            'log': self.sync_log
        }
        
        logger.info(f"åŒæ­¥å®Œæˆ: {synced_count} å€‹å°ˆæ¡ˆå·²åŒæ­¥, {skipped_count} å€‹è·³é, {error_count} å€‹éŒ¯èª¤")
        return summary
    
    def sync_project(self, project_id, mode='auto', similarity_threshold=0.8):
        """åŒæ­¥å–®å€‹å°ˆæ¡ˆ"""
        logger.info(f"æ­£åœ¨åŒæ­¥å°ˆæ¡ˆ: {project_id}")
        
        # ç²å– Markdown å…§å®¹
        markdown_content = self.markdown.get_memory(project_id)
        if not markdown_content:
            return {'action': 'skipped', 'message': f'å°ˆæ¡ˆ {project_id} æ²’æœ‰å…§å®¹'}
        
        # æª¢æŸ¥ SQLite ä¸­æ˜¯å¦å·²å­˜åœ¨
        sqlite_content = self.sqlite.get_memory(project_id)
        
        if sqlite_content:
            # å°ˆæ¡ˆå·²å­˜åœ¨ï¼Œè™•ç†åˆä½µé‚è¼¯
            return self.handle_existing_project(project_id, markdown_content, sqlite_content, mode, similarity_threshold)
        else:
            # æ–°å°ˆæ¡ˆï¼Œç›´æ¥åŒ¯å…¥
            return self.import_new_project(project_id, markdown_content, mode)
    
    def handle_existing_project(self, project_id, markdown_content, sqlite_content, mode, similarity_threshold):
        """è™•ç†å·²å­˜åœ¨çš„å°ˆæ¡ˆ"""
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarity = self.calculate_similarity(markdown_content, sqlite_content)
        
        logger.info(f"å°ˆæ¡ˆ {project_id} ç›¸ä¼¼åº¦: {similarity:.2f}")
        
        if mode == 'preview':
            return {
                'action': 'preview',
                'message': f'å°ˆæ¡ˆå·²å­˜åœ¨ï¼Œç›¸ä¼¼åº¦: {similarity:.2f}ï¼Œå»ºè­°å‹•ä½œ: {"åˆä½µ" if similarity > similarity_threshold else "å‰µå»ºæ–°å°ˆæ¡ˆ"}'
            }
        
        if similarity > similarity_threshold:
            # é«˜ç›¸ä¼¼åº¦ï¼Œè‡ªå‹•åˆä½µ
            if mode == 'auto':
                merged_content = self.merge_contents(markdown_content, sqlite_content)
                success = self.replace_project_content(project_id, merged_content)
                if success:
                    return {'action': 'synced', 'message': f'å°ˆæ¡ˆ {project_id} å·²åˆä½µ (ç›¸ä¼¼åº¦: {similarity:.2f})'}
                else:
                    return {'action': 'error', 'message': f'åˆä½µå°ˆæ¡ˆ {project_id} å¤±æ•—'}
            elif mode == 'interactive':
                # äº’å‹•æ¨¡å¼ - é€™è£¡ç°¡åŒ–ç‚ºè‡ªå‹•åˆä½µï¼Œå¯¦éš›å¯ä»¥æ·»åŠ ç”¨æˆ¶è¼¸å…¥
                logger.info(f"äº’å‹•æ¨¡å¼: è‡ªå‹•åˆä½µé«˜ç›¸ä¼¼åº¦å°ˆæ¡ˆ {project_id}")
                merged_content = self.merge_contents(markdown_content, sqlite_content)
                success = self.replace_project_content(project_id, merged_content)
                if success:
                    return {'action': 'synced', 'message': f'å°ˆæ¡ˆ {project_id} å·²åˆä½µ (äº’å‹•æ¨¡å¼)'}
                else:
                    return {'action': 'error', 'message': f'åˆä½µå°ˆæ¡ˆ {project_id} å¤±æ•—'}
        else:
            # ä½ç›¸ä¼¼åº¦ï¼Œå‰µå»ºæ–°å°ˆæ¡ˆ
            new_project_id = f"{project_id}-markdown-import"
            success = self.import_new_project(new_project_id, markdown_content, mode)
            if success['action'] == 'synced':
                return {'action': 'synced', 'message': f'å‰µå»ºæ–°å°ˆæ¡ˆ {new_project_id} (åŸå°ˆæ¡ˆç›¸ä¼¼åº¦éä½: {similarity:.2f})'}
            else:
                return success
    
    def import_new_project(self, project_id, markdown_content, mode):
        """åŒ¯å…¥æ–°å°ˆæ¡ˆ"""
        if mode == 'preview':
            return {'action': 'preview', 'message': f'å°‡å‰µå»ºæ–°å°ˆæ¡ˆ: {project_id}'}
        
        # è§£æ Markdown å…§å®¹ä¸¦åŒ¯å…¥åˆ° SQLite
        entries = self.parse_markdown_entries(markdown_content)
        
        success_count = 0
        for entry in entries:
            success = self.sqlite.save_memory(
                project_id,
                entry['content'],
                entry['title'],
                entry['category']
            )
            if success:
                success_count += 1
        
        if success_count > 0:
            return {'action': 'synced', 'message': f'å°ˆæ¡ˆ {project_id} å·²åŒ¯å…¥ ({success_count} å€‹æ¢ç›®)'}
        else:
            return {'action': 'error', 'message': f'åŒ¯å…¥å°ˆæ¡ˆ {project_id} å¤±æ•—'}
    
    def calculate_similarity(self, content1, content2):
        """è¨ˆç®—å…©å€‹å…§å®¹çš„ç›¸ä¼¼åº¦ (ç°¡åŒ–ç‰ˆæœ¬)"""
        # ç°¡åŒ–çš„ç›¸ä¼¼åº¦è¨ˆç®— - åŸºæ–¼é—œéµè©é‡ç–Š
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def merge_contents(self, markdown_content, sqlite_content):
        """åˆä½µå…©å€‹å…§å®¹ (ç°¡åŒ–ç‰ˆæœ¬)"""
        # ç°¡åŒ–çš„åˆä½µé‚è¼¯ - å°‡ Markdown å…§å®¹è¿½åŠ åˆ° SQLite å…§å®¹
        markdown_entries = self.parse_markdown_entries(markdown_content)
        
        # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²æ›´å¥½åœ°å»é‡å’Œæ’åº
        return markdown_entries
    
    def parse_markdown_entries(self, markdown_content):
        """è§£æ Markdown å…§å®¹ç‚ºæ¢ç›®åˆ—è¡¨"""
        entries = []
        lines = markdown_content.split('\n')
        current_entry = None
        current_content = []
        
        for line in lines:
            if line.startswith('## '):
                # ä¿å­˜ä¸Šä¸€å€‹æ¢ç›®
                if current_entry:
                    entries.append({
                        'timestamp': current_entry['timestamp'],
                        'title': current_entry['title'],
                        'category': current_entry['category'],
                        'content': '\n'.join(current_content).strip()
                    })
                
                # è§£ææ–°æ¢ç›®æ¨™é¡Œ
                header = line[3:].strip()
                timestamp, title, category = self._parse_section_header(header)
                current_entry = {
                    'timestamp': timestamp,
                    'title': title,
                    'category': category
                }
                current_content = []
                
            elif line.strip() == '---':
                # æ¢ç›®çµæŸ
                continue
            else:
                current_content.append(line)
        
        # è™•ç†æœ€å¾Œä¸€å€‹æ¢ç›®
        if current_entry:
            entries.append({
                'timestamp': current_entry['timestamp'],
                'title': current_entry['title'],
                'category': current_entry['category'],
                'content': '\n'.join(current_content).strip()
            })
        
        return entries
    
    def _parse_section_header(self, header):
        """è§£æå€æ®µæ¨™é¡Œï¼Œæå–æ™‚é–“æˆ³ã€æ¨™é¡Œå’Œåˆ†é¡"""
        timestamp = ""
        title = ""
        category = ""
        
        # æå–åˆ†é¡ (hashtag)
        if '#' in header:
            header, category = header.split('#', 1)
            category = category.strip()
        
        # æå–æ™‚é–“æˆ³å’Œæ¨™é¡Œ
        if ' - ' in header:
            timestamp, title = header.split(' - ', 1)
            timestamp = timestamp.strip()
            title = title.strip()
        else:
            timestamp = header.strip()
        
        return timestamp, title, category
    
    def replace_project_content(self, project_id, entries):
        """æ›¿æ›å°ˆæ¡ˆå…§å®¹"""
        try:
            # å…ˆåˆªé™¤ç¾æœ‰å…§å®¹
            self.sqlite.delete_memory(project_id)
            
            # é‡æ–°åŒ¯å…¥
            for entry in entries:
                self.sqlite.save_memory(
                    project_id,
                    entry['content'],
                    entry['title'],
                    entry['category']
                )
            return True
        except Exception as e:
            logger.error(f"æ›¿æ›å°ˆæ¡ˆå…§å®¹å¤±æ•—: {e}")
            return False
    
    def get_sync_report(self):
        """ç²å–åŒæ­¥å ±å‘Š"""
        return self.sync_log

class ProjectMemoryImporter:
    """å°ˆæ¡ˆè¨˜æ†¶åŒ¯å…¥å™¨ / Project Memory Importer
    
    æ”¯æ´å¾å¤šç¨®æ ¼å¼åŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶è³‡æ–™
    Supports importing project memory data from multiple formats
    """
    
    def __init__(self, memory_manager: MemoryBackend):
        self.memory_manager = memory_manager
        self.logger = logging.getLogger(__name__)
    
    def import_from_markdown(self, file_path: str, project_id: str = None, 
                           merge_strategy: str = "append") -> Dict[str, Any]:
        """å¾ Markdown æª”æ¡ˆåŒ¯å…¥è¨˜æ†¶è³‡æ–™
        
        Args:
            file_path: Markdown æª”æ¡ˆè·¯å¾‘
            project_id: ç›®æ¨™å°ˆæ¡ˆ IDï¼Œå¦‚æœç‚º None å‰‡å¾æª”æ¡ˆåæ¨æ–·
            merge_strategy: åˆä½µç­–ç•¥ ("append", "replace", "skip_duplicates")
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # æ¨æ–·å°ˆæ¡ˆ ID
            if not project_id:
                project_id = file_path.stem.replace('-', '_').replace(' ', '_')
            
            # è®€å– Markdown å…§å®¹
            content = file_path.read_text(encoding='utf-8')
            
            # è§£æ Markdown æ ¼å¼çš„è¨˜æ†¶æ¢ç›®
            entries = self._parse_markdown_entries(content)
            
            # åŒ¯å…¥æ¢ç›®
            imported_count = 0
            skipped_count = 0
            
            for entry in entries:
                try:
                    if merge_strategy == "replace":
                        # æ›¿æ›æ¨¡å¼ï¼šå…ˆæ¸…ç©ºå°ˆæ¡ˆ
                        if imported_count == 0:
                            self.memory_manager.delete_memory(project_id)
                    
                    elif merge_strategy == "skip_duplicates":
                        # æª¢æŸ¥é‡è¤‡
                        if self._is_duplicate_entry(project_id, entry):
                            skipped_count += 1
                            continue
                    
                    # å„²å­˜æ¢ç›®
                    self.memory_manager.save_memory(
                        project_id,
                        entry['content'],
                        entry.get('title', ''),
                        entry.get('category', '')
                    )
                    imported_count += 1
                    
                except Exception as e:
                    self.logger.warning(f"Failed to import entry: {e}")
                    continue
            
            return {
                "success": True,
                "project_id": project_id,
                "imported_count": imported_count,
                "skipped_count": skipped_count,
                "total_entries": len(entries),
                "message": f"Successfully imported {imported_count} entries to project '{project_id}'"
            }
            
        except Exception as e:
            self.logger.error(f"Error importing from Markdown: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Failed to import from {file_path}"
            }
    
    def import_from_json(self, file_path: str, project_id: str = None,
                        merge_strategy: str = "append") -> Dict[str, Any]:
        """å¾ JSON æª”æ¡ˆåŒ¯å…¥è¨˜æ†¶è³‡æ–™"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # æ¨æ–·å°ˆæ¡ˆ ID
            if not project_id:
                project_id = file_path.stem.replace('-', '_').replace(' ', '_')
            
            # è®€å– JSON å…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è§£æ JSON æ ¼å¼
            entries = self._parse_json_entries(data)
            
            # åŒ¯å…¥æ¢ç›®
            imported_count = 0
            skipped_count = 0
            
            for entry in entries:
                try:
                    if merge_strategy == "replace":
                        if imported_count == 0:
                            self.memory_manager.delete_memory(project_id)
                    
                    elif merge_strategy == "skip_duplicates":
                        if self._is_duplicate_entry(project_id, entry):
                            skipped_count += 1
                            continue
                    
                    self.memory_manager.save_memory(
                        project_id,
                        entry['content'],
                        entry.get('title', ''),
                        entry.get('category', '')
                    )
                    imported_count += 1
                    
                except Exception as e:
                    self.logger.warning(f"Failed to import entry: {e}")
                    continue
            
            return {
                "success": True,
                "project_id": project_id,
                "imported_count": imported_count,
                "skipped_count": skipped_count,
                "total_entries": len(entries),
                "message": f"Successfully imported {imported_count} entries to project '{project_id}'"
            }
            
        except Exception as e:
            self.logger.error(f"Error importing from JSON: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Failed to import from {file_path}"
            }
    
    def import_from_csv(self, file_path: str, project_id: str = None,
                       merge_strategy: str = "append") -> Dict[str, Any]:
        """å¾ CSV æª”æ¡ˆåŒ¯å…¥è¨˜æ†¶è³‡æ–™"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # æ¨æ–·å°ˆæ¡ˆ ID
            if not project_id:
                project_id = file_path.stem.replace('-', '_').replace(' ', '_')
            
            # è®€å– CSV å…§å®¹
            entries = []
            with open(file_path, 'r', encoding='utf-8', newline='') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    entries.append({
                        'timestamp': row.get('timestamp', ''),
                        'title': row.get('title', ''),
                        'category': row.get('category', ''),
                        'content': row.get('content', '')
                    })
            
            # åŒ¯å…¥æ¢ç›®
            imported_count = 0
            skipped_count = 0
            
            for entry in entries:
                try:
                    if not entry['content'].strip():
                        continue
                    
                    if merge_strategy == "replace":
                        if imported_count == 0:
                            self.memory_manager.delete_memory(project_id)
                    
                    elif merge_strategy == "skip_duplicates":
                        if self._is_duplicate_entry(project_id, entry):
                            skipped_count += 1
                            continue
                    
                    self.memory_manager.save_memory(
                        project_id,
                        entry['content'],
                        entry.get('title', ''),
                        entry.get('category', '')
                    )
                    imported_count += 1
                    
                except Exception as e:
                    self.logger.warning(f"Failed to import entry: {e}")
                    continue
            
            return {
                "success": True,
                "project_id": project_id,
                "imported_count": imported_count,
                "skipped_count": skipped_count,
                "total_entries": len(entries),
                "message": f"Successfully imported {imported_count} entries to project '{project_id}'"
            }
            
        except Exception as e:
            self.logger.error(f"Error importing from CSV: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Failed to import from {file_path}"
            }
    
    def import_universal(self, file_path: str, project_id: str = None,
                        merge_strategy: str = "append") -> Dict[str, Any]:
        """é€šç”¨åŒ¯å…¥åŠŸèƒ½ï¼Œè‡ªå‹•æª¢æ¸¬æª”æ¡ˆæ ¼å¼"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # æ ¹æ“šå‰¯æª”åæª¢æ¸¬æ ¼å¼
            suffix = file_path.suffix.lower()
            
            if suffix == '.md':
                return self.import_from_markdown(str(file_path), project_id, merge_strategy)
            elif suffix == '.json':
                return self.import_from_json(str(file_path), project_id, merge_strategy)
            elif suffix == '.csv':
                return self.import_from_csv(str(file_path), project_id, merge_strategy)
            elif suffix == '.txt':
                # TXT æª”æ¡ˆç•¶ä½œç°¡å–®çš„ Markdown è™•ç†
                return self.import_from_markdown(str(file_path), project_id, merge_strategy)
            else:
                raise ValueError(f"Unsupported file format: {suffix}")
                
        except Exception as e:
            self.logger.error(f"Error in universal import: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"Failed to import from {file_path}"
            }
    
    def _parse_markdown_entries(self, content: str) -> List[Dict[str, Any]]:
        """è§£æ Markdown æ ¼å¼çš„è¨˜æ†¶æ¢ç›®"""
        entries = []
        
        # åˆ†å‰²æ¢ç›®ï¼ˆä½¿ç”¨ ## æˆ– --- ä½œç‚ºåˆ†éš”ç¬¦ï¼‰
        sections = re.split(r'\n(?:## |\-\-\-\n)', content)
        
        for section in sections:
            if not section.strip():
                continue
            
            entry = {}
            lines = section.strip().split('\n')
            
            # è§£æç¬¬ä¸€è¡Œï¼ˆå¯èƒ½åŒ…å«æ™‚é–“æˆ³å’Œæ¨™é¡Œï¼‰
            first_line = lines[0] if lines else ""
            
            # å˜—è©¦è§£ææ™‚é–“æˆ³
            timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2})', first_line)
            if timestamp_match:
                entry['timestamp'] = timestamp_match.group(1)
                # ç§»é™¤æ™‚é–“æˆ³å¾Œçš„éƒ¨åˆ†ä½œç‚ºæ¨™é¡Œ
                title = re.sub(r'\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}', '', first_line).strip()
                if title.startswith('- '):
                    title = title[2:]
                entry['title'] = title
            else:
                entry['title'] = first_line
            
            # è§£æåˆ†é¡ï¼ˆå°‹æ‰¾ #category æ ¼å¼ï¼‰
            category_match = re.search(r'#(\w+)', first_line)
            if category_match:
                entry['category'] = category_match.group(1)
                # å¾æ¨™é¡Œä¸­ç§»é™¤åˆ†é¡æ¨™ç±¤
                entry['title'] = re.sub(r'\s*#\w+\s*', '', entry['title']).strip()
            
            # å…§å®¹æ˜¯å‰©é¤˜çš„è¡Œ
            if len(lines) > 1:
                entry['content'] = '\n'.join(lines[1:]).strip()
            else:
                entry['content'] = entry.get('title', '')
            
            if entry['content']:
                entries.append(entry)
        
        return entries
    
    def _parse_json_entries(self, data: Any) -> List[Dict[str, Any]]:
        """è§£æ JSON æ ¼å¼çš„è¨˜æ†¶æ¢ç›®"""
        entries = []
        
        if isinstance(data, list):
            # é™£åˆ—æ ¼å¼
            for item in data:
                if isinstance(item, dict):
                    entries.append(self._normalize_entry(item))
                elif isinstance(item, str):
                    entries.append({'content': item})
        
        elif isinstance(data, dict):
            # ç‰©ä»¶æ ¼å¼
            if 'entries' in data:
                # æœ‰ entries æ¬„ä½
                for item in data['entries']:
                    entries.append(self._normalize_entry(item))
            else:
                # å–®ä¸€æ¢ç›®
                entries.append(self._normalize_entry(data))
        
        return entries
    
    def _normalize_entry(self, entry: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨™æº–åŒ–æ¢ç›®æ ¼å¼"""
        normalized = {}
        
        # å…§å®¹æ¬„ä½çš„å¯èƒ½åç¨±
        content_fields = ['content', 'text', 'body', 'message', 'description']
        for field in content_fields:
            if field in entry and entry[field]:
                normalized['content'] = str(entry[field])
                break
        
        # æ¨™é¡Œæ¬„ä½
        title_fields = ['title', 'name', 'subject', 'heading']
        for field in title_fields:
            if field in entry and entry[field]:
                normalized['title'] = str(entry[field])
                break
        
        # åˆ†é¡æ¬„ä½
        category_fields = ['category', 'tag', 'type', 'label']
        for field in category_fields:
            if field in entry and entry[field]:
                normalized['category'] = str(entry[field])
                break
        
        # æ™‚é–“æˆ³æ¬„ä½
        timestamp_fields = ['timestamp', 'created_at', 'date', 'time']
        for field in timestamp_fields:
            if field in entry and entry[field]:
                normalized['timestamp'] = str(entry[field])
                break
        
        return normalized
    
    def _is_duplicate_entry(self, project_id: str, entry: Dict[str, Any]) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡æ¢ç›®"""
        try:
            # æœå°‹ç›¸ä¼¼å…§å®¹
            search_results = self.memory_manager.search_memory(
                project_id, entry['content'][:100], limit=5
            )
            
            # ç°¡å–®çš„é‡è¤‡æª¢æ¸¬ï¼šå…§å®¹å‰100å­—ç¬¦ç›¸åŒ
            entry_preview = entry['content'][:100].strip().lower()
            for result in search_results:
                if result['content'][:100].strip().lower() == entry_preview:
                    return True
            
            return False
            
        except Exception:
            # å¦‚æœæª¢æ¸¬å¤±æ•—ï¼Œå‡è¨­ä¸é‡è¤‡
            return False


class MCPServer:
    """
    Model Context Protocol ä¼ºæœå™¨
    Model Context Protocol Server
    
    å¯¦ä½œ MCP å”è­°çš„ä¼ºæœå™¨ï¼Œæä¾›è¨˜æ†¶ç®¡ç†å·¥å…·çµ¦ AI åŠ©æ‰‹ä½¿ç”¨
    Implements MCP protocol server providing memory management tools for AI assistants
    
    Capabilities / åŠŸèƒ½:
    - å·¥å…·èª¿ç”¨ / Tool invocation
    - è¨˜æ†¶ç®¡ç† / Memory management  
    - å°ˆæ¡ˆåŒæ­¥ / Project synchronization
    - å•Ÿå‹•æ™‚å°ˆæ¡ˆé¡¯ç¤º / Startup project display
    """
    
    def __init__(self, backend: MemoryBackend = None):
        self.memory_manager = backend or MarkdownMemoryManager()
        self.version = "1.0.0"
        
        
        # åˆå§‹åŒ–åŒ¯å…¥å™¨
        self.importer = ProjectMemoryImporter(self.memory_manager)

    async def handle_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç† MCP è¨Šæ¯"""
        # æå–è«‹æ±‚ ID ç”¨æ–¼å›æ‡‰
        request_id = message.get('id')
        
        try:
            method = message.get('method')
            
            if method == 'initialize':
                response = await self.handle_initialize(message)
            elif method == 'tools/list':
                response = await self.list_tools()
            elif method == 'tools/call':
                response = await self.call_tool(message['params'])
            elif method == 'resources/list':
                response = await self.list_resources()
            elif method == 'prompts/list':
                response = await self.list_prompts()
            elif method == 'notifications/initialized':
                response = await self.handle_initialized()
            else:
                response = self._error_response(-32601, f"Method not found: {method}")
            
            # ç¢ºä¿å›æ‡‰åŒ…å«æ­£ç¢ºçš„è«‹æ±‚ IDï¼ˆé™¤äº†é€šçŸ¥æ¶ˆæ¯ï¼‰
            if response is not None and request_id is not None:
                response['id'] = request_id
            
            return response
                
        except Exception as e:
            logger.error(f"Error handling message: {e}")
            error_response = self._error_response(-32603, str(e))
            # éŒ¯èª¤å›æ‡‰ä¹Ÿéœ€è¦åŒ…å«è«‹æ±‚ ID
            if request_id is not None:
                error_response['id'] = request_id
            return error_response

    async def handle_initialize(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†åˆå§‹åŒ–è«‹æ±‚"""
        return {
            'jsonrpc': '2.0',
            'result': {
                'protocolVersion': '2024-11-05',
                'capabilities': {
                    'tools': {}
                },
                'serverInfo': {
                    'name': 'markdown-memory-mcp',
                    'version': self.version
                }
            }
        }

    async def list_tools(self) -> Dict[str, Any]:
        """åˆ—å‡ºå¯ç”¨å·¥å…·"""
        tools = [
            {
                'name': 'save_project_memory',
                'description': 'å„²å­˜è³‡è¨Šåˆ°å°ˆæ¡ˆè¨˜æ†¶ / Save information to project-specific memory with optional title and category',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier (will be sanitized for filename)'
                        },
                        'content': {
                            'type': 'string',
                            'description': 'Content to save to project memory'
                        },
                        'title': {
                            'type': 'string',
                            'description': 'Optional title for the project memory entry'
                        },
                        'category': {
                            'type': 'string',
                            'description': 'Optional category/tag for the project memory entry'
                        }
                    },
                    'required': ['project_id', 'content']
                }
            },
            {
                'name': 'get_project_memory',
                'description': 'å–å¾—å®Œæ•´å°ˆæ¡ˆè¨˜æ†¶å…§å®¹ / Get full project memory content',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'search_project_memory',
                'description': 'æœå°‹å°ˆæ¡ˆè¨˜æ†¶å…§å®¹ / Search project memory for specific content',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'query': {
                            'type': 'string',
                            'description': 'Search query'
                        },
                        'limit': {
                            'type': 'integer',
                            'description': 'Maximum number of results to return',
                            'default': 10
                        }
                    },
                    'required': ['project_id', 'query']
                }
            },
            {
                'name': 'list_memory_projects',
                'description': 'åˆ—å‡ºæ‰€æœ‰è¨˜æ†¶å°ˆæ¡ˆåŠçµ±è¨ˆè³‡è¨Š / List all projects with memory and their statistics',
                'inputSchema': {
                    'type': 'object',
                    'properties': {}
                }
            },
            {
                'name': 'get_recent_project_memory',
                'description': 'Get recent project memory entries for a project',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'limit': {
                            'type': 'integer',
                            'description': 'Number of recent entries to return',
                            'default': 5
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'get_project_memory_stats',
                'description': 'Get statistics about a project\'s memory',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'delete_project_memory',
                'description': 'Delete all project memory for a project (use with caution)',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'delete_project_memory_entry',
                'description': 'Delete specific project memory entries based on various criteria',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'entry_id': {
                            'type': 'string',
                            'description': 'Entry ID (1-based index) to delete'
                        },
                        'timestamp': {
                            'type': 'string',
                            'description': 'Timestamp pattern to match for deletion'
                        },
                        'title': {
                            'type': 'string',
                            'description': 'Title pattern to match for deletion'
                        },
                        'category': {
                            'type': 'string',
                            'description': 'Category pattern to match for deletion'
                        },
                        'content_match': {
                            'type': 'string',
                            'description': 'Content pattern to match for deletion'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'edit_project_memory_entry',
                'description': 'Edit specific project memory entry content',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        },
                        'entry_id': {
                            'type': 'string',
                            'description': 'Entry ID (1-based index) to edit'
                        },
                        'timestamp': {
                            'type': 'string',
                            'description': 'Timestamp pattern to match for editing'
                        },
                        'new_title': {
                            'type': 'string',
                            'description': 'New title for the entry'
                        },
                        'new_category': {
                            'type': 'string',
                            'description': 'New category for the entry'
                        },
                        'new_content': {
                            'type': 'string',
                            'description': 'New content for the entry'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'list_project_memory_entries',
                'description': 'List all project memory entries with their IDs for easy reference',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'sync_markdown_to_sqlite',
                'description': 'åŒæ­¥ Markdown å°ˆæ¡ˆåˆ° SQLite / Sync all Markdown projects to SQLite backend with auto merging',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'mode': {
                            'type': 'string',
                            'enum': ['auto', 'interactive', 'preview'],
                            'default': 'auto',
                            'description': 'Sync mode: auto merge, interactive prompts, or preview only'
                        },
                        'similarity_threshold': {
                            'type': 'number',
                            'default': 0.8,
                            'minimum': 0.0,
                            'maximum': 1.0,
                            'description': 'Similarity threshold for automatic merging (0.0-1.0)'
                        }
                    }
                }
            },
            {
                'name': 'export_project_memory',
                'description': 'åŒ¯å‡ºå°ˆæ¡ˆè¨˜æ†¶ / Export project memory to various formats',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {
                            'type': 'string',
                            'description': 'Project identifier to export'
                        },
                        'format': {
                            'type': 'string',
                            'enum': ['markdown', 'json', 'csv', 'txt'],
                            'default': 'markdown',
                            'description': 'Export format (default: markdown)'
                        },
                        'output_path': {
                            'type': 'string',
                            'description': 'Optional output file path (if not provided, returns content as text)'
                        },
                        'include_metadata': {
                            'type': 'boolean',
                            'default': True,
                            'description': 'Include metadata like timestamps and categories'
                        }
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'save_global_memory',
                'description': 'å„²å­˜å…¨å±€è¨˜æ†¶ / Save global memory for cross-project knowledge',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'content': {
                            'type': 'string',
                            'description': 'Content to save to global memory'
                        },
                        'title': {
                            'type': 'string',
                            'description': 'Optional title for the global memory entry'
                        },
                        'category': {
                            'type': 'string',
                            'description': 'Optional category/tag for the global memory entry'
                        }
                    },
                    'required': ['content']
                }
            },
            {
                'name': 'get_global_memory',
                'description': 'å–å¾—å…¨å±€è¨˜æ†¶ / Get all global memory content',
                'inputSchema': {
                    'type': 'object',
                    'properties': {},
                    'required': []
                }
            },
            {
                'name': 'search_global_memory',
                'description': 'æœå°‹å…¨å±€è¨˜æ†¶ / Search global memory for specific content',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'query': {
                            'type': 'string',
                            'description': 'Search query'
                        },
                        'limit': {
                            'type': 'integer',
                            'default': 10,
                            'description': 'Maximum number of results to return'
                        }
                    },
                    'required': ['query']
                }
            },
            {
                'name': 'get_global_memory_stats',
                'description': 'å–å¾—å…¨å±€è¨˜æ†¶çµ±è¨ˆ / Get global memory statistics',
                'inputSchema': {
                    'type': 'object',
                    'properties': {},
                    'required': []
                }
            },
            {
                'name': 'get_backend_status',
                'description': 'å¿«é€ŸæŸ¥çœ‹ç•¶å‰å¾Œç«¯ç‹€æ…‹ / Quick check current backend status',
                'inputSchema': {
                    'type': 'object',
                    'properties': {},
                    'required': []
                }
            },
            {
                'name': 'rename_project',
                'description': 'é‡æ–°å‘½åå°ˆæ¡ˆ / Rename a project',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'old_project_id': {
                            'type': 'string',
                            'description': 'Current project identifier'
                        },
                        'new_project_id': {
                            'type': 'string',
                            'description': 'New project identifier'
                        }
                    },
                    'required': ['old_project_id', 'new_project_id']
                }
            },
            {
                'name': 'search_index',
                'description': 'ğŸš€ æ™ºèƒ½æœå°‹ - ä½¿ç”¨ Index Table å¤§å¹…æ¸›å°‘ token ä½¿ç”¨',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {'type': 'string', 'description': 'å°ˆæ¡ˆ ID'},
                        'query': {'type': 'string', 'description': 'æœå°‹é—œéµå­—'},
                        'limit': {'type': 'integer', 'description': 'æœ€å¤§çµæœæ•¸é‡', 'default': 10}
                    },
                    'required': ['project_id', 'query']
                }
            },
            {
                'name': 'get_hierarchy_tree',
                'description': 'ğŸ“Š ç²å–å°ˆæ¡ˆçš„éšå±¤æ¨¹ç‹€çµæ§‹',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {'type': 'string', 'description': 'å°ˆæ¡ˆ ID'}
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'rebuild_index_for_project',
                'description': 'ğŸ”„ ç‚ºå°ˆæ¡ˆé‡å»ºæ‰€æœ‰ç´¢å¼•æ¢ç›® (æ‰¹é‡è™•ç†ç¾æœ‰æ¢ç›®)',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {'type': 'string', 'description': 'å°ˆæ¡ˆ ID'}
                    },
                    'required': ['project_id']
                }
            },
            {
                'name': 'get_index_stats',
                'description': 'ğŸ“ˆ ç²å–ç´¢å¼•çµ±è¨ˆè³‡è¨Š',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'project_id': {'type': 'string', 'description': 'å°ˆæ¡ˆ ID (å¯é¸ï¼Œä¸æä¾›å‰‡é¡¯ç¤ºå…¨å±€çµ±è¨ˆ)'}
                    },
                    'required': []
                }
            },
            {
                'name': 'update_index_entry',
                'description': 'âœï¸ æ›´æ–°ç´¢å¼•æ¢ç›®çš„éšå±¤å’Œåˆ†é¡è³‡è¨Š',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'entry_id': {'type': 'integer', 'description': 'æ¢ç›® ID'},
                        'summary': {'type': 'string', 'description': 'æ–°çš„æ‘˜è¦'},
                        'keywords': {'type': 'string', 'description': 'æ–°çš„é—œéµå­—'},
                        'hierarchy_level': {'type': 'integer', 'description': 'éšå±¤ç´šåˆ¥ (0=å¤§æ¨™é¡Œ, 1=ä¸­æ¨™é¡Œ, 2=å°æ¨™é¡Œ)'},
                        'entry_type': {'type': 'string', 'description': 'æ¢ç›®é¡å‹ (discussion/feature/bug/milestone)'},
                        'importance_level': {'type': 'integer', 'description': 'é‡è¦ç¨‹åº¦ (1-5)'},
                        'parent_entry_id': {'type': 'integer', 'description': 'çˆ¶æ¢ç›® ID'}
                    },
                    'required': ['entry_id']
                }
            },
            {
                'name': 'import_project_memory_universal',
                'description': 'é€šç”¨åŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Universal import project memory from various formats (auto-detect)',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the file to import (supports .md, .json, .csv, .txt)'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data: append (add to existing), replace (clear first), skip_duplicates (avoid duplicates)'
                        }
                    },
                    'required': ['file_path']
                }
            },
            {
                'name': 'import_project_memory_from_markdown',
                'description': 'å¾ Markdown æª”æ¡ˆåŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Import project memory from Markdown file',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the Markdown file to import'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data'
                        }
                    },
                    'required': ['file_path']
                }
            },
            {
                'name': 'import_project_memory_from_json',
                'description': 'å¾ JSON æª”æ¡ˆåŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Import project memory from JSON file',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the JSON file to import'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data'
                        }
                    },
                    'required': ['file_path']
                }
            },
            {
                'name': 'import_project_memory_from_csv',
                'description': 'å¾ CSV æª”æ¡ˆåŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Import project memory from CSV file',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the CSV file to import (expects columns: timestamp, title, category, content)'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data'
                        }
                    },
                    'required': ['file_path']
                }
            },
            {
                'name': 'import_project_memory_from_txt',
                'description': 'å¾ TXT æª”æ¡ˆåŒ¯å…¥å°ˆæ¡ˆè¨˜æ†¶ / Import project memory from TXT file',
                'inputSchema': {
                    'type': 'object',
                    'properties': {
                        'file_path': {
                            'type': 'string',
                            'description': 'Path to the TXT file to import (treated as simple markdown)'
                        },
                        'project_id': {
                            'type': 'string',
                            'description': 'Target project ID (optional, will be inferred from filename if not provided)'
                        },
                        'merge_strategy': {
                            'type': 'string',
                            'enum': ['append', 'replace', 'skip_duplicates'],
                            'default': 'append',
                            'description': 'How to handle existing data'
                        }
                    },
                    'required': ['file_path']
                }
            }
        ]
        
        return {
            'jsonrpc': '2.0',
            'result': {
                'tools': tools
            }
        }

    async def list_resources(self) -> Dict[str, Any]:
        """åˆ—å‡ºå¯ç”¨è³‡æºï¼ˆç›®å‰ç‚ºç©ºï¼‰"""
        return {
            'jsonrpc': '2.0',
            'result': {
                'resources': []
            }
        }

    async def list_prompts(self) -> Dict[str, Any]:
        """åˆ—å‡ºå¯ç”¨æç¤ºï¼ˆç›®å‰ç‚ºç©ºï¼‰"""
        return {
            'jsonrpc': '2.0',
            'result': {
                'prompts': []
            }
        }

    async def handle_initialized(self) -> None:
        """è™•ç†åˆå§‹åŒ–å®Œæˆé€šçŸ¥ï¼ˆç„¡éœ€å›æ‡‰ï¼‰"""
        logger.info("Client initialization completed")
        
        # è‡ªå‹•é¡¯ç¤ºå°ˆæ¡ˆåˆ—è¡¨ / Auto display project list
        try:
            projects = self.memory_manager.list_projects()
            if projects:
                welcome_message = f"""ğŸ‰ **è¨˜æ†¶ç®¡ç†ç³»çµ±å·²å•Ÿå‹• / Memory Management System Started**
ç™¼ç¾ {len(projects)} å€‹å°ˆæ¡ˆ / Found {len(projects)} projects:

"""
                for project in projects:
                    welcome_message += f"**{project['name']}** (`{project['id']}`)\n"
                    welcome_message += f"  - æ¢ç›® / Entries: {project['entries_count']} å€‹\n"
                    welcome_message += f"  - æœ€å¾Œä¿®æ”¹ / Last Modified: {project['last_modified']}\n"
                    if project['categories']:
                        welcome_message += f"  - é¡åˆ¥ / Categories: {', '.join(project['categories'])}\n"
                    welcome_message += "\n"
                
                welcome_message += """ğŸ’¡ ä½¿ç”¨ `list_memory_projects` å·¥å…·å¯éš¨æ™‚æŸ¥çœ‹å°ˆæ¡ˆåˆ—è¡¨
ğŸ’¡ Use `list_memory_projects` tool to view project list anytime"""
                
                # ç™¼é€æ­¡è¿è¨Šæ¯ä½œç‚ºé€šçŸ¥
                notification = {
                    'jsonrpc': '2.0',
                    'method': 'notifications/message',
                    'params': {
                        'level': 'info',
                        'logger': 'memory-server',
                        'data': welcome_message
                    }
                }
                
                # è¼¸å‡ºé€šçŸ¥
                print(json.dumps(notification, ensure_ascii=False))
                sys.stdout.flush()
                
            else:
                # å¦‚æœæ²’æœ‰å°ˆæ¡ˆï¼Œç™¼é€æç¤ºè¨Šæ¯ / If no projects, send guidance message
                welcome_message = """ğŸ“ **è¨˜æ†¶ç®¡ç†ç³»çµ±å·²å•Ÿå‹• / Memory Management System Started**
ç›®å‰æ²’æœ‰å°ˆæ¡ˆï¼Œå¯ä»¥é–‹å§‹å‰µå»ºæ‚¨çš„ç¬¬ä¸€å€‹è¨˜æ†¶ï¼
No projects found. You can start creating your first memory!

ğŸ’¡ ä½¿ç”¨ `save_project_memory` å·¥å…·é–‹å§‹è¨˜éŒ„
ğŸ’¡ Use `save_project_memory` tool to start recording"""
                notification = {
                    'jsonrpc': '2.0',
                    'method': 'notifications/message',
                    'params': {
                        'level': 'info',
                        'logger': 'memory-server',
                        'data': welcome_message
                    }
                }
                print(json.dumps(notification, ensure_ascii=False))
                sys.stdout.flush()
                
        except Exception as e:
            logger.error(f"Error displaying welcome message: {e}")
        
        return None

    async def call_tool(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œå·¥å…·èª¿ç”¨"""
        tool_name = params.get('name')
        arguments = params.get('arguments', {})

        try:
            if tool_name == 'save_project_memory':
                success = self.memory_manager.save_memory(
                    arguments['project_id'],
                    arguments['content'],
                    arguments.get('title', ''),
                    arguments.get('category', '')
                )
                return self._success_response(
                    f"Memory {'saved' if success else 'failed to save'} for project: {arguments['project_id']}"
                )


            elif tool_name == 'get_project_memory':
                memory = self.memory_manager.get_memory(arguments['project_id'])
                return self._success_response(
                    memory or f"No memory found for project: {arguments['project_id']}"
                )

            elif tool_name == 'search_project_memory':
                results = self.memory_manager.search_memory(
                    arguments['project_id'],
                    arguments['query'],
                    arguments.get('limit', 10)
                )
                
                if results:
                    text = f"Found {len(results)} matches for \"{arguments['query']}\":\n\n"
                    for i, result in enumerate(results, 1):
                        text += f"**{i}. {result['timestamp']}"
                        if result['title']:
                            text += f" - {result['title']}"
                        if result['category']:
                            text += f" #{result['category']}"
                        text += f"**\n{result['content']}\n\n"
                else:
                    text = f"No matches found for \"{arguments['query']}\" in project {arguments['project_id']}"
                
                return self._success_response(text)

            elif tool_name == 'list_memory_projects':
                projects = self.memory_manager.list_projects()
                if projects:
                    text = f"Found {len(projects)} projects:\n\n"
                    for project in projects:
                        text += f"**{project['name']}** (`{project['id']}`)\n"
                        text += f"  - Entries: {project['entries_count']}\n"
                        text += f"  - Last modified: {project['last_modified']}\n"
                        if project['categories']:
                            text += f"  - Categories: {', '.join(project['categories'])}\n"
                        text += "\n"
                else:
                    text = "No projects found"
                
                return self._success_response(text)

            elif tool_name == 'get_recent_project_memory':
                results = self.memory_manager.get_recent_memory(
                    arguments['project_id'],
                    arguments.get('limit', 5)
                )
                
                if results:
                    text = f"Recent {len(results)} memory entries:\n\n"
                    for result in results:
                        text += f"**{result['timestamp']}"
                        if result['title']:
                            text += f" - {result['title']}"
                        if result['category']:
                            text += f" #{result['category']}"
                        text += f"**\n{result['content']}\n\n"
                else:
                    text = f"No recent memory found for project: {arguments['project_id']}"
                
                return self._success_response(text)

            elif tool_name == 'get_project_memory_stats':
                stats = self.memory_manager.get_memory_stats(arguments['project_id'])
                
                if stats['exists']:
                    text = f"Memory statistics for **{arguments['project_id']}**:\n\n"
                    text += f"- Total entries: {stats['total_entries']}\n"
                    text += f"- Total words: {stats['total_words']}\n"
                    text += f"- Total characters: {stats['total_characters']}\n"
                    if stats['categories']:
                        text += f"- Categories: {', '.join(stats['categories'])}\n"
                    if stats['latest_entry']:
                        text += f"- Latest entry: {stats['latest_entry']}\n"
                    if stats['oldest_entry']:
                        text += f"- Oldest entry: {stats['oldest_entry']}\n"
                else:
                    text = f"No memory found for project: {arguments['project_id']}"
                
                return self._success_response(text)

            elif tool_name == 'delete_project_memory':
                success = self.memory_manager.delete_memory(arguments['project_id'])
                return self._success_response(
                    f"Memory {'deleted' if success else 'not found'} for project: {arguments['project_id']}"
                )

            elif tool_name == 'delete_project_memory_entry':
                result = self.memory_manager.delete_memory_entry(
                    arguments['project_id'],
                    arguments.get('entry_id'),
                    arguments.get('timestamp'),
                    arguments.get('title'),
                    arguments.get('category'),
                    arguments.get('content_match')
                )
                
                if result['success']:
                    text = result['message'] + f"\n\nDeleted entries:\n"
                    for entry in result['deleted_entries']:
                        text += f"- {entry['timestamp']}"
                        if entry['title']:
                            text += f" - {entry['title']}"
                        text += "\n"
                    text += f"\nRemaining entries: {result['remaining_count']}"
                else:
                    text = result['message']
                
                return self._success_response(text)

            elif tool_name == 'edit_project_memory_entry':
                result = self.memory_manager.edit_memory_entry(
                    arguments['project_id'],
                    arguments.get('entry_id'),
                    arguments.get('timestamp'),
                    arguments.get('new_title'),
                    arguments.get('new_category'),
                    arguments.get('new_content')
                )
                
                if result['success']:
                    text = result['message'] + f"\n\nEdited entry:\n"
                    entry = result['edited_entry']
                    text += f"- {entry['timestamp']}"
                    if entry['title']:
                        text += f" - {entry['title']}"
                    if entry['category']:
                        text += f" #{entry['category']}"
                else:
                    text = result['message']
                
                return self._success_response(text)

            elif tool_name == 'list_project_memory_entries':
                result = self.memory_manager.list_memory_entries(arguments['project_id'])
                
                if result['success']:
                    text = f"Memory entries for **{arguments['project_id']}** ({result['total_entries']} entries):\n\n"
                    for entry in result['entries']:
                        text += f"**{entry['id']}.** {entry['timestamp']}"
                        if entry['title']:
                            text += f" - {entry['title']}"
                        if entry['category']:
                            text += f" #{entry['category']}"
                        text += f"\n   {entry['content_preview']}\n\n"
                else:
                    text = result['message']
                
                return self._success_response(text)

            elif tool_name == 'rename_project':
                # é‡æ–°å‘½åå°ˆæ¡ˆ
                success = self.memory_manager.rename_project(
                    arguments['old_project_id'],
                    arguments['new_project_id']
                )
                
                if success:
                    return self._success_response(
                        f"âœ… Project successfully renamed from '{arguments['old_project_id']}' to '{arguments['new_project_id']}'"
                    )
                else:
                    return self._error_response(
                        f"âŒ Failed to rename project from '{arguments['old_project_id']}' to '{arguments['new_project_id']}'. "
                        f"Check if the old project exists and the new name is not already taken."
                    )
            
            # ==================== INDEX TABLE TOOLS ====================
            elif tool_name == 'search_index':
                project_id = arguments.get('project_id')
                query = arguments.get('query')
                limit = arguments.get('limit', 10)
                
                if hasattr(self.memory_manager, 'search_index'):
                    results = self.memory_manager.search_index(project_id, query, limit)
                    
                    if not results:
                        return self._success_response(f"ğŸ” No results found for '{query}' in project '{project_id}'")
                    
                    response = f"ğŸš€ **æ™ºèƒ½æœå°‹çµæœ** (ç¯€çœ 70-85% token)\n\n"
                    response += f"**å°ˆæ¡ˆ**: {project_id}\n**æŸ¥è©¢**: {query}\n**æ‰¾åˆ°**: {len(results)} å€‹çµæœ\n\n"
                    
                    for i, result in enumerate(results, 1):
                        if result.get('match_type') == 'index':
                            response += f"**{i}. ğŸ“‹ {result.get('title', 'Untitled')}** (ç´¢å¼•åŒ¹é…)\n"
                            response += f"   - **æ‘˜è¦**: {result.get('summary', '')}\n"
                            response += f"   - **é—œéµå­—**: {result.get('keywords', '')}\n"
                            response += f"   - **éšå±¤**: Level {result.get('hierarchy_level', 0)}\n"
                            response += f"   - **é¡å‹**: {result.get('entry_type', 'discussion')}\n\n"
                        else:
                            response += f"**{i}. ğŸ“„ {result.get('title', 'Untitled')}** (å…§å®¹åŒ¹é…)\n"
                            response += f"   - **é è¦½**: {result.get('content_preview', '')}\n\n"
                    
                    response += f"ğŸ’¡ **æç¤º**: ç´¢å¼•åŒ¹é…çµæœå·²å¤§å¹…æ¸›å°‘ token ä½¿ç”¨é‡ï¼"
                    return self._success_response(response)
                else:
                    return self._error_response("âŒ Index search not available (SQLite backend required)")
            
            elif tool_name == 'rebuild_index_for_project':
                project_id = arguments.get('project_id')
                
                if hasattr(self.memory_manager, 'rebuild_index_for_project'):
                    result = self.memory_manager.rebuild_index_for_project(project_id)
                    
                    if result.get('success'):
                        response = f"ğŸ”„ **ç´¢å¼•é‡å»ºå®Œæˆ**: {project_id}\n\n"
                        response += f"- **ç¸½æ¢ç›®æ•¸**: {result.get('total_entries', 0)}\n"
                        response += f"- **æˆåŠŸç´¢å¼•**: {result.get('indexed_entries', 0)}\n"
                        response += f"ğŸ’¡ ç¾åœ¨å¯ä»¥ä½¿ç”¨ search_index äº«å— 70-85% token ç¯€çœæ•ˆç›Šï¼"
                        return self._success_response(response)
                    else:
                        return self._error_response(f"âŒ ç´¢å¼•é‡å»ºå¤±æ•—: {result.get('message', 'Unknown error')}")
                else:
                    return self._error_response("âŒ Index rebuild not available (SQLite backend required)")
            
            elif tool_name == 'get_index_stats':
                project_id = arguments.get('project_id')
                
                if hasattr(self.memory_manager, 'get_index_stats'):
                    stats = self.memory_manager.get_index_stats(project_id)
                    
                    if project_id:
                        response = f"ğŸ“ˆ **å°ˆæ¡ˆç´¢å¼•çµ±è¨ˆ**: {project_id}\n\n"
                        response += f"- **å·²ç´¢å¼•æ¢ç›®**: {stats.get('total_indexed', 0)}\n"
                        response += f"- **æ¢ç›®é¡å‹æ•¸**: {stats.get('entry_types', 0)}\n"
                        response += f"- **æœ€å¤§éšå±¤ç´šåˆ¥**: {stats.get('max_hierarchy_level', 0)}\n"
                    else:
                        response = f"ğŸ“ˆ **å…¨å±€ç´¢å¼•çµ±è¨ˆ**\n\n"
                        response += f"- **å·²ç´¢å¼•æ¢ç›®**: {stats.get('total_indexed', 0)}\n"
                        response += f"- **ç´¢å¼•å°ˆæ¡ˆæ•¸**: {stats.get('indexed_projects', 0)}\n"
                    
                    return self._success_response(response)
                else:
                    return self._error_response("âŒ Index stats not available (SQLite backend required)")
            
            elif tool_name == 'get_hierarchy_tree':
                project_id = arguments.get('project_id')
                
                if hasattr(self.memory_manager, 'get_hierarchy_tree'):
                    tree = self.memory_manager.get_hierarchy_tree(project_id)
                    
                    if tree.get('children'):
                        response = f"ğŸ“Š **å°ˆæ¡ˆéšå±¤çµæ§‹**: {project_id}\n\n"
                        for entry in tree['children']:
                            level_icon = ["ğŸ“", "ğŸ“‚", "ğŸ“„"][min(entry.get('hierarchy_level', 0), 2)]
                            type_icon = {"feature": "ğŸš€", "bug": "ğŸ›", "milestone": "ğŸ¯", "discussion": "ğŸ’¬"}.get(entry.get('entry_type', 'discussion'), "ğŸ’¬")
                            response += f"{level_icon} {type_icon} **{entry.get('title', entry.get('summary', 'Untitled'))}**\n"
                        return self._success_response(response)
                    else:
                        return self._success_response(f"ğŸ“Š å°ˆæ¡ˆ '{project_id}' å°šç„¡ç´¢å¼•æ¢ç›®ï¼Œè«‹å…ˆä½¿ç”¨ rebuild_index_for_project å»ºç«‹ç´¢å¼•")
                else:
                    return self._error_response("âŒ Hierarchy tree not available (SQLite backend required)")
            
            elif tool_name == 'update_index_entry':
                entry_id = arguments.get('entry_id')
                
                if hasattr(self.memory_manager, 'update_index_entry'):
                    update_fields = {}
                    for field in ['summary', 'keywords', 'hierarchy_level', 'entry_type', 'importance_level', 'parent_entry_id']:
                        if field in arguments and arguments[field] is not None:
                            update_fields[field] = arguments[field]
                    
                    if not update_fields:
                        return self._error_response("âŒ æ²’æœ‰æä¾›è¦æ›´æ–°çš„æ¬„ä½")
                    
                    success = self.memory_manager.update_index_entry(entry_id, **update_fields)
                    
                    if success:
                        response = f"âœï¸ **ç´¢å¼•æ¢ç›®æ›´æ–°æˆåŠŸ**: Entry ID {entry_id}\n\n"
                        for field, value in update_fields.items():
                            response += f"  - {field}: {value}\n"
                        return self._success_response(response)
                    else:
                        return self._error_response(f"âŒ æ›´æ–°ç´¢å¼•æ¢ç›®å¤±æ•—: Entry ID {entry_id}")
                else:
                    return self._error_response("âŒ Index update not available (SQLite backend required)")

            elif tool_name == 'sync_markdown_to_sqlite':
                # æª¢æŸ¥ç•¶å‰å¾Œç«¯æ˜¯å¦ç‚º SQLite
                if not isinstance(self.memory_manager, SQLiteBackend):
                    return self._error_response(-32603, "åŒæ­¥åŠŸèƒ½åªèƒ½åœ¨ SQLite å¾Œç«¯æ¨¡å¼ä¸‹ä½¿ç”¨")
                
                try:
                    # å‰µå»º Markdown å¾Œç«¯å¯¦ä¾‹
                    markdown_backend = MarkdownMemoryManager()
                    
                    # å‰µå»ºåŒæ­¥ç®¡ç†å™¨
                    sync_manager = DataSyncManager(markdown_backend, self.memory_manager)
                    
                    # åŸ·è¡ŒåŒæ­¥
                    result = sync_manager.sync_all_projects(
                        mode=arguments.get('mode', 'auto'),
                        similarity_threshold=arguments.get('similarity_threshold', 0.8)
                    )
                    
                    # æ ¼å¼åŒ–å›æ‡‰
                    if result['success']:
                        text = f"ğŸ”„ **Markdown â†’ SQLite åŒæ­¥å®Œæˆ**\n\n"
                        text += f"ğŸ“Š **çµ±è¨ˆè³‡è¨Š**:\n"
                        text += f"- ç¸½å°ˆæ¡ˆæ•¸: {result['total_projects']}\n"
                        text += f"- å·²åŒæ­¥: {result['synced']} âœ…\n"
                        text += f"- è·³é: {result['skipped']} â­ï¸\n"
                        text += f"- éŒ¯èª¤: {result['errors']} âŒ\n\n"
                        
                        if result['log']:
                            text += f"ğŸ“‹ **è©³ç´°æ—¥èªŒ**:\n"
                            for log_entry in result['log']:
                                status_icon = {
                                    'synced': 'âœ…',
                                    'skipped': 'â­ï¸', 
                                    'error': 'âŒ',
                                    'preview': 'ğŸ‘ï¸'
                                }.get(log_entry['action'], 'â“')
                                
                                text += f"{status_icon} **{log_entry['project_id']}**: {log_entry['message']}\n"
                        
                        if result['synced'] > 0:
                            text += f"\nğŸ‰ æˆåŠŸåŒæ­¥ {result['synced']} å€‹å°ˆæ¡ˆåˆ° SQLite å¾Œç«¯ï¼"
                        elif result['total_projects'] == 0:
                            text += f"\nğŸ’¡ æ²’æœ‰æ‰¾åˆ° Markdown å°ˆæ¡ˆéœ€è¦åŒæ­¥ã€‚"
                        else:
                            text += f"\nâš ï¸ æ‰€æœ‰å°ˆæ¡ˆéƒ½è¢«è·³éï¼Œå¯èƒ½å·²ç¶“å­˜åœ¨æ–¼ SQLite ä¸­ã€‚"
                    else:
                        text = f"âŒ åŒæ­¥å¤±æ•—: {result.get('message', 'æœªçŸ¥éŒ¯èª¤')}"
                    
                    return self._success_response(text)
                    
                except Exception as e:
                    logger.error(f"Sync operation failed: {e}")
                    return self._error_response(-32603, f"åŒæ­¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

            elif tool_name == 'export_project_memory':
                try:
                    project_id = arguments['project_id']
                    export_format = arguments.get('format', 'markdown')
                    output_path = arguments.get('output_path')
                    include_metadata = arguments.get('include_metadata', True)
                    
                    # ç²å–å°ˆæ¡ˆè¨˜æ†¶å…§å®¹
                    memory_content = self.memory_manager.get_memory(project_id)
                    if not memory_content:
                        return self._error_response(-32603, f"No memory found for project: {project_id}")
                    
                    # ç²å–å°ˆæ¡ˆçµ±è¨ˆè³‡è¨Š
                    stats = self.memory_manager.get_memory_stats(project_id)
                    
                    # æ ¹æ“šæ ¼å¼åŒ¯å‡º
                    if export_format == 'markdown':
                        exported_content = self._export_to_markdown(project_id, memory_content, stats, include_metadata)
                    elif export_format == 'json':
                        exported_content = self._export_to_json(project_id, memory_content, stats, include_metadata)
                    elif export_format == 'csv':
                        exported_content = self._export_to_csv(project_id, memory_content, stats, include_metadata)
                    elif export_format == 'txt':
                        exported_content = self._export_to_txt(project_id, memory_content, stats, include_metadata)
                    else:
                        return self._error_response(-32603, f"Unsupported export format: {export_format}")
                    
                    # å¦‚æœæŒ‡å®šäº†è¼¸å‡ºè·¯å¾‘ï¼Œå¯«å…¥æª”æ¡ˆ
                    if output_path:
                        try:
                            output_file = Path(output_path)
                            output_file.parent.mkdir(parents=True, exist_ok=True)
                            
                            if export_format == 'json':
                                with open(output_file, 'w', encoding='utf-8') as f:
                                    json.dump(exported_content, f, ensure_ascii=False, indent=2)
                            else:
                                with open(output_file, 'w', encoding='utf-8') as f:
                                    f.write(exported_content)
                            
                            text = f"ğŸ“¤ **å°ˆæ¡ˆåŒ¯å‡ºå®Œæˆ / Project Export Complete**\n\n"
                            text += f"- å°ˆæ¡ˆ / Project: **{project_id}**\n"
                            text += f"- æ ¼å¼ / Format: **{export_format.upper()}**\n"
                            text += f"- è¼¸å‡ºæª”æ¡ˆ / Output File: `{output_file}`\n"
                            if stats['exists']:
                                text += f"- æ¢ç›®æ•¸é‡ / Entries: {stats['total_entries']}\n"
                                text += f"- ç¸½å­—æ•¸ / Words: {stats['total_words']}\n"
                            text += f"\nâœ… æª”æ¡ˆå·²æˆåŠŸå„²å­˜ï¼"
                            
                        except Exception as e:
                            return self._error_response(-32603, f"Failed to write export file: {str(e)}")
                    else:
                        # ç›´æ¥è¿”å›å…§å®¹
                        if export_format == 'json':
                            text = f"ğŸ“¤ **å°ˆæ¡ˆåŒ¯å‡º / Project Export** - {project_id} ({export_format.upper()})\n\n"
                            text += f"```json\n{json.dumps(exported_content, ensure_ascii=False, indent=2)}\n```"
                        else:
                            text = f"ğŸ“¤ **å°ˆæ¡ˆåŒ¯å‡º / Project Export** - {project_id} ({export_format.upper()})\n\n"
                            text += f"```{export_format}\n{exported_content}\n```"
                    
                    return self._success_response(text)
                    
                except Exception as e:
                    logger.error(f"Export operation failed: {e}")
                    return self._error_response(-32603, f"åŒ¯å‡ºéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

            elif tool_name == 'save_global_memory':
                success = self.memory_manager.save_memory(
                    '__global__',
                    arguments['content'],
                    arguments.get('title', ''),
                    arguments.get('category', '')
                )
                return self._success_response(
                    f"Global memory {'saved' if success else 'failed to save'}: {arguments.get('title', 'Untitled')}"
                )

            elif tool_name == 'get_global_memory':
                memory = self.memory_manager.get_memory('__global__')
                return self._success_response(
                    memory or "No global memory found. Use save_global_memory to start building your knowledge base."
                )

            elif tool_name == 'search_global_memory':
                results = self.memory_manager.search_memory(
                    '__global__',
                    arguments['query'],
                    arguments.get('limit', 10)
                )
                
                if results:
                    text = f"Found {len(results)} global memory matches for \"{arguments['query']}\":\n\n"
                    for i, result in enumerate(results, 1):
                        text += f"**{i}. {result['timestamp']}"
                        if result['title']:
                            text += f" - {result['title']}"
                        if result['category']:
                            text += f" #{result['category']}"
                        text += f"**\n{result['content']}\n\n"
                else:
                    text = f"No global memory matches found for \"{arguments['query']}\""
                
                return self._success_response(text)

            elif tool_name == 'get_global_memory_stats':
                stats = self.memory_manager.get_memory_stats('__global__')
                
                if stats['exists']:
                    text = f"ğŸ“Š **Global Memory Statistics**:\n\n"
                    text += f"- Total entries: {stats['total_entries']}\n"
                    text += f"- Total words: {stats['total_words']}\n"
                    text += f"- Total characters: {stats['total_characters']}\n"
                    if stats['categories']:
                        text += f"- Categories: {', '.join(stats['categories'])}\n"
                    if stats['latest_entry']:
                        text += f"- Latest entry: {stats['latest_entry']}\n"
                    if stats['oldest_entry']:
                        text += f"- Oldest entry: {stats['oldest_entry']}\n"
                    text += f"\nğŸ’¡ Global memory contains cross-project knowledge and standards."
                else:
                    text = f"ğŸ“ **Global Memory is Empty**\n\nStart building your global knowledge base with save_global_memory!"
                
                return self._success_response(text)

            elif tool_name == 'get_backend_status':
                # ç²å–ç•¶å‰å¾Œç«¯é¡å‹
                backend_type = type(self.memory_manager).__name__
                backend_name = "SQLite" if "SQLite" in backend_type else "Markdown"
                
                # ç²å–å­˜å„²è·¯å¾‘ä¿¡æ¯
                if hasattr(self.memory_manager, 'db_path'):
                    storage_path = str(self.memory_manager.db_path)
                    storage_type = "Database file"
                elif hasattr(self.memory_manager, 'memory_dir'):
                    storage_path = str(self.memory_manager.memory_dir)
                    storage_type = "Directory"
                else:
                    storage_path = "Unknown"
                    storage_type = "Unknown"
                
                # ç²å–é …ç›®çµ±è¨ˆ
                projects = self.memory_manager.list_projects()
                total_projects = len(projects)
                total_entries = sum(p['entries_count'] for p in projects)
                
                # æ§‹å»ºç‹€æ…‹ä¿¡æ¯
                text = f"ğŸ”§ **Backend Status / å¾Œç«¯ç‹€æ…‹**\n\n"
                text += f"**Current Backend / ç•¶å‰å¾Œç«¯**: {backend_name}\n"
                text += f"**Backend Class / å¾Œç«¯é¡åˆ¥**: `{backend_type}`\n"
                text += f"**Storage Type / å­˜å„²é¡å‹**: {storage_type}\n"
                text += f"**Storage Path / å­˜å„²è·¯å¾‘**: `{storage_path}`\n\n"
                text += f"ğŸ“Š **Quick Stats / å¿«é€Ÿçµ±è¨ˆ**:\n"
                text += f"- Projects / å°ˆæ¡ˆæ•¸: **{total_projects}**\n"
                text += f"- Total Entries / ç¸½æ¢ç›®æ•¸: **{total_entries}**\n\n"
                
                # æ·»åŠ å¾Œç«¯ç‰¹æ€§èªªæ˜
                if backend_name == "SQLite":
                    text += f"âœ… **SQLite Features / SQLite ç‰¹æ€§**:\n"
                    text += f"- ğŸ” Full-text search / å…¨æ–‡æœå°‹\n"
                    text += f"- ğŸš€ High performance / é«˜æ•ˆèƒ½\n"
                    text += f"- ğŸ”’ ACID transactions / ACID äº‹å‹™\n"
                    text += f"- ğŸ“Š Complex queries / è¤‡é›œæŸ¥è©¢\n"
                else:
                    text += f"âœ… **Markdown Features / Markdown ç‰¹æ€§**:\n"
                    text += f"- ğŸ“ Human-readable / äººé¡å¯è®€\n"
                    text += f"- ğŸ”„ Version control friendly / ç‰ˆæœ¬æ§åˆ¶å‹å¥½\n"
                    text += f"- ğŸ“ File-based storage / æª”æ¡ˆå¼å­˜å„²\n"
                    text += f"- ğŸ”„ Easy backup / å®¹æ˜“å‚™ä»½\n"
                
                text += f"\nğŸ’¡ Use `list_memory_projects` to see all projects"
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_universal':
                result = self.importer.import_universal(
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **é€šç”¨åŒ¯å…¥æˆåŠŸ / Universal Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **åŒ¯å…¥å¤±æ•— / Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_from_markdown':
                result = self.importer.import_from_markdown(
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **Markdown åŒ¯å…¥æˆåŠŸ / Markdown Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **Markdown åŒ¯å…¥å¤±æ•— / Markdown Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_from_json':
                result = self.importer.import_from_json(
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **JSON åŒ¯å…¥æˆåŠŸ / JSON Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **JSON åŒ¯å…¥å¤±æ•— / JSON Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_from_csv':
                result = self.importer.import_from_csv(
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **CSV åŒ¯å…¥æˆåŠŸ / CSV Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **CSV åŒ¯å…¥å¤±æ•— / CSV Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            elif tool_name == 'import_project_memory_from_txt':
                result = self.importer.import_from_markdown(  # TXT ç•¶ä½œ Markdown è™•ç†
                    arguments['file_path'],
                    arguments.get('project_id'),
                    arguments.get('merge_strategy', 'append')
                )
                
                if result['success']:
                    text = f"ğŸ“¥ **TXT åŒ¯å…¥æˆåŠŸ / TXT Import Successful**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- ç›®æ¨™å°ˆæ¡ˆ / Target Project: **{result['project_id']}**\n"
                    text += f"- åŒ¯å…¥ç­–ç•¥ / Merge Strategy: {arguments.get('merge_strategy', 'append')}\n"
                    text += f"- æˆåŠŸåŒ¯å…¥ / Imported: **{result['imported_count']}** æ¢ç›®\n"
                    if result['skipped_count'] > 0:
                        text += f"- è·³éé‡è¤‡ / Skipped: **{result['skipped_count']}** æ¢ç›®\n"
                    text += f"- ç¸½æ¢ç›®æ•¸ / Total Entries: {result['total_entries']}\n\n"
                    text += f"âœ… {result['message']}"
                else:
                    text = f"âŒ **TXT åŒ¯å…¥å¤±æ•— / TXT Import Failed**\n\n"
                    text += f"- æª”æ¡ˆè·¯å¾‘ / File Path: `{arguments['file_path']}`\n"
                    text += f"- éŒ¯èª¤è¨Šæ¯ / Error: {result.get('error', 'Unknown error')}\n\n"
                    text += result.get('message', 'Import operation failed')
                
                return self._success_response(text)

            else:
                return self._error_response(-32601, f"Unknown tool: {tool_name}")

        except Exception as e:
            logger.error(f"Error calling tool {tool_name}: {e}")
            return self._error_response(-32603, str(e))

    def _success_response(self, text: str) -> Dict[str, Any]:
        """å»ºç«‹æˆåŠŸå›æ‡‰"""
        return {
            'jsonrpc': '2.0',
            'result': {
                'content': [{
                    'type': 'text',
                    'text': text
                }]
            }
        }

    def _error_response(self, code: int, message: str) -> Dict[str, Any]:
        """å»ºç«‹éŒ¯èª¤å›æ‡‰"""
        return {
            'jsonrpc': '2.0',
            'error': {
                'code': code,
                'message': message
            }
        }

    def _export_to_markdown(self, project_id: str, memory_content: str, stats: Dict, include_metadata: bool) -> str:
        """åŒ¯å‡ºç‚º Markdown æ ¼å¼"""
        content = f"# {project_id}\n\n"
        
        if include_metadata and stats['exists']:
            content += f"## å°ˆæ¡ˆè³‡è¨Š / Project Information\n\n"
            content += f"- **å°ˆæ¡ˆåç¨± / Project Name:** {project_id}\n"
            content += f"- **æ¢ç›®æ•¸é‡ / Total Entries:** {stats['total_entries']}\n"
            content += f"- **ç¸½å­—æ•¸ / Total Words:** {stats['total_words']}\n"
            content += f"- **ç¸½å­—ç¬¦æ•¸ / Total Characters:** {stats['total_characters']}\n"
            if stats['categories']:
                content += f"- **åˆ†é¡ / Categories:** {', '.join(stats['categories'])}\n"
            if stats['latest_entry']:
                content += f"- **æœ€æ–°æ¢ç›® / Latest Entry:** {stats['latest_entry']}\n"
            if stats['oldest_entry']:
                content += f"- **æœ€èˆŠæ¢ç›® / Oldest Entry:** {stats['oldest_entry']}\n"
            content += f"- **åŒ¯å‡ºæ™‚é–“ / Export Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            content += "---\n\n"
        
        content += f"## è¨˜æ†¶å…§å®¹ / Memory Content\n\n"
        content += memory_content
        
        return content

    def _export_to_json(self, project_id: str, memory_content: str, stats: Dict, include_metadata: bool) -> Dict:
        """åŒ¯å‡ºç‚º JSON æ ¼å¼"""
        export_data = {
            'project_id': project_id,
            'content': memory_content,
            'export_time': datetime.now().isoformat()
        }
        
        if include_metadata and stats['exists']:
            export_data['metadata'] = {
                'total_entries': stats['total_entries'],
                'total_words': stats['total_words'],
                'total_characters': stats['total_characters'],
                'categories': stats['categories'],
                'latest_entry': stats['latest_entry'],
                'oldest_entry': stats['oldest_entry']
            }
        
        return export_data

    def _export_to_csv(self, project_id: str, memory_content: str, stats: Dict, include_metadata: bool) -> str:
        """åŒ¯å‡ºç‚º CSV æ ¼å¼ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼Œä¸»è¦ç”¨æ–¼æ¢ç›®åˆ—è¡¨ï¼‰"""
        import csv
        from io import StringIO
        
        output = StringIO()
        writer = csv.writer(output)
        
        # CSV æ¨™é¡Œ
        if include_metadata:
            writer.writerow(['Timestamp', 'Title', 'Category', 'Content'])
        else:
            writer.writerow(['Content'])
        
        # å˜—è©¦è§£æè¨˜æ†¶å…§å®¹ä¸­çš„æ¢ç›®
        lines = memory_content.split('\n')
        current_entry = {'timestamp': '', 'title': '', 'category': '', 'content': ''}
        
        for line in lines:
            line = line.strip()
            if line.startswith('**') and line.endswith('**'):
                # å¯èƒ½æ˜¯æ™‚é–“æˆ³è¨˜è¡Œ
                if current_entry['content']:
                    # å¯«å…¥å‰ä¸€å€‹æ¢ç›®
                    if include_metadata:
                        writer.writerow([current_entry['timestamp'], current_entry['title'], 
                                       current_entry['category'], current_entry['content'].strip()])
                    else:
                        writer.writerow([current_entry['content'].strip()])
                    current_entry = {'timestamp': '', 'title': '', 'category': '', 'content': ''}
                
                # è§£ææ–°æ¢ç›®çš„æ¨™é¡Œè¡Œ
                header = line[2:-2]  # ç§»é™¤ **
                if ' - ' in header:
                    parts = header.split(' - ', 1)
                    current_entry['timestamp'] = parts[0]
                    title_and_category = parts[1]
                    if ' #' in title_and_category:
                        title_parts = title_and_category.split(' #')
                        current_entry['title'] = title_parts[0]
                        current_entry['category'] = title_parts[1] if len(title_parts) > 1 else ''
                    else:
                        current_entry['title'] = title_and_category
                else:
                    current_entry['timestamp'] = header
            else:
                if line:
                    current_entry['content'] += line + '\n'
        
        # å¯«å…¥æœ€å¾Œä¸€å€‹æ¢ç›®
        if current_entry['content']:
            if include_metadata:
                writer.writerow([current_entry['timestamp'], current_entry['title'], 
                               current_entry['category'], current_entry['content'].strip()])
            else:
                writer.writerow([current_entry['content'].strip()])
        
        return output.getvalue()

    def _export_to_txt(self, project_id: str, memory_content: str, stats: Dict, include_metadata: bool) -> str:
        """åŒ¯å‡ºç‚ºç´”æ–‡å­—æ ¼å¼"""
        content = f"å°ˆæ¡ˆ: {project_id}\n"
        content += "=" * 50 + "\n\n"
        
        if include_metadata and stats['exists']:
            content += f"å°ˆæ¡ˆè³‡è¨Š:\n"
            content += f"- æ¢ç›®æ•¸é‡: {stats['total_entries']}\n"
            content += f"- ç¸½å­—æ•¸: {stats['total_words']}\n"
            content += f"- ç¸½å­—ç¬¦æ•¸: {stats['total_characters']}\n"
            if stats['categories']:
                content += f"- åˆ†é¡: {', '.join(stats['categories'])}\n"
            if stats['latest_entry']:
                content += f"- æœ€æ–°æ¢ç›®: {stats['latest_entry']}\n"
            if stats['oldest_entry']:
                content += f"- æœ€èˆŠæ¢ç›®: {stats['oldest_entry']}\n"
            content += f"- åŒ¯å‡ºæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            content += "-" * 50 + "\n\n"
        
        content += "è¨˜æ†¶å…§å®¹:\n\n"
        # ç§»é™¤ Markdown æ ¼å¼æ¨™è¨˜
        clean_content = memory_content.replace('**', '').replace('*', '').replace('#', '')
        content += clean_content
        
        return content

    async def run(self):
        """é‹è¡Œ MCP ä¼ºæœå™¨"""
        logger.info(f"Starting Markdown Memory MCP Server v{self.version}")
        
        try:
            while True:
                # å¾ stdin è®€å–è¨Šæ¯
                try:
                    line = await asyncio.get_event_loop().run_in_executor(
                        None, sys.stdin.readline
                    )
                except EOFError:
                    break
                
                if not line:
                    break
                
                try:
                    message = json.loads(line.strip())
                    response = await self.handle_message(message)
                    
                    # åªæœ‰éé€šçŸ¥æ¶ˆæ¯æ‰éœ€è¦å›æ‡‰
                    if response is not None:
                        # è¨­å®š response ID
                        if 'id' in message:
                            response['id'] = message['id']
                        
                        # å°‡å›æ‡‰å¯«å…¥ stdout
                        print(json.dumps(response, ensure_ascii=False))
                        sys.stdout.flush()
                    
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON received: {e}")
                    continue
                    
        except KeyboardInterrupt:
            logger.info("Server stopped by user")
        except Exception as e:
            logger.error(f"Server error: {e}")
        finally:
            logger.info("Server shutdown complete")

def create_backend(backend_type: str, db_path: str = None) -> MemoryBackend:
    """æ ¹æ“šé¡å‹å‰µå»ºè¨˜æ†¶å¾Œç«¯"""
    if backend_type == "markdown":
        return MarkdownMemoryManager()
    elif backend_type == "sqlite":
        if db_path:
            # å±•é–‹ ~ å®¶ç›®éŒ„ç¬¦è™Ÿ
            expanded_path = os.path.expanduser(db_path)
            return SQLiteBackend(expanded_path)
        else:
            return SQLiteBackend()
    else:
        raise ValueError(f"Unknown backend type: {backend_type}")

def main():
    """ä¸»ç¨‹å¼å…¥å£é»"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Markdown Memory MCP Server")
    parser.add_argument(
        "--backend", 
        choices=["markdown", "sqlite"], 
        default="markdown",
        help="é¸æ“‡è¨˜æ†¶å¾Œç«¯é¡å‹ (default: markdown)"
    )
    parser.add_argument(
        "--info", 
        action="store_true",
        help="é¡¯ç¤ºç•¶å‰é…ç½®è³‡è¨Š"
    )
    parser.add_argument(
        "--sync-from-markdown",
        action="store_true",
        help="å°‡ Markdown å°ˆæ¡ˆåŒæ­¥åˆ° SQLite å¾Œç«¯"
    )
    parser.add_argument(
        "--sync-mode",
        choices=["auto", "interactive", "preview"],
        default="auto",
        help="åŒæ­¥æ¨¡å¼: auto(è‡ªå‹•), interactive(äº’å‹•), preview(é è¦½)"
    )
    parser.add_argument(
        "--similarity-threshold",
        type=float,
        default=0.8,
        help="ç›¸ä¼¼åº¦é–¾å€¼ (0.0-1.0)ï¼Œç”¨æ–¼æ±ºå®šæ˜¯å¦è‡ªå‹•åˆä½µ"
    )
    parser.add_argument(
        "--db-path",
        type=str,
        help="SQLite è³‡æ–™åº«è·¯å¾‘ (é è¨­: ai-memory/memory.db)ï¼Œæ”¯æ´ ~ å®¶ç›®éŒ„ç¬¦è™Ÿ"
    )
    
    args = parser.parse_args()
    
    if args.info:
        print(f"Markdown Memory MCP Server")
        print(f"Backend: {args.backend}")
        print(f"Python version: {sys.version}")
        print(f"Working directory: {os.getcwd()}")
        print(f"Script path: {__file__}")
        
        # é¡¯ç¤ºå¾Œç«¯ç‰¹å®šè³‡è¨Šï¼ˆä¸åˆå§‹åŒ–ï¼‰
        if args.backend == "sqlite":
            if args.db_path:
                db_path = Path(os.path.expanduser(args.db_path))
                print(f"SQLite database (custom): {db_path}")
            else:
                db_path = Path("ai-memory/memory.db")
                print(f"SQLite database (default): {db_path}")
            print(f"Database exists: {db_path.exists()}")
        elif args.backend == "markdown":
            memory_dir = Path(__file__).parent.resolve() / "ai-memory"
            print(f"Memory directory: {memory_dir}")
            print(f"Directory exists: {memory_dir.exists()}")
            
        return
    
    # è™•ç†åŒæ­¥åŠŸèƒ½
    if args.sync_from_markdown:
        if args.backend != "sqlite":
            print("éŒ¯èª¤: åŒæ­¥åŠŸèƒ½åªèƒ½åœ¨ SQLite å¾Œç«¯æ¨¡å¼ä¸‹ä½¿ç”¨")
            print("è«‹ä½¿ç”¨: python memory_mcp_server_dev.py --backend=sqlite --sync-from-markdown")
            sys.exit(1)
        
        try:
            # å‰µå»ºå…©å€‹å¾Œç«¯å¯¦ä¾‹
            markdown_backend = MarkdownMemoryManager()
            sqlite_backend = SQLiteBackend()
            
            # å‰µå»ºåŒæ­¥ç®¡ç†å™¨
            sync_manager = DataSyncManager(markdown_backend, sqlite_backend)
            
            print(f"é–‹å§‹åŒæ­¥ Markdown å°ˆæ¡ˆåˆ° SQLite...")
            print(f"åŒæ­¥æ¨¡å¼: {args.sync_mode}")
            print(f"ç›¸ä¼¼åº¦é–¾å€¼: {args.similarity_threshold}")
            print("-" * 50)
            
            # åŸ·è¡ŒåŒæ­¥
            result = sync_manager.sync_all_projects(
                mode=args.sync_mode,
                similarity_threshold=args.similarity_threshold
            )
            
            # é¡¯ç¤ºçµæœ
            print(f"\nåŒæ­¥å®Œæˆ!")
            print(f"ç¸½å°ˆæ¡ˆæ•¸: {result['total_projects']}")
            print(f"å·²åŒæ­¥: {result['synced']}")
            print(f"è·³é: {result['skipped']}")
            print(f"éŒ¯èª¤: {result['errors']}")
            
            if result['log']:
                print("\nè©³ç´°æ—¥èªŒ:")
                for log_entry in result['log']:
                    status_icon = {
                        'synced': 'âœ…',
                        'skipped': 'â­ï¸',
                        'error': 'âŒ',
                        'preview': 'ğŸ‘ï¸'
                    }.get(log_entry['action'], 'â“')
                    
                    print(f"{status_icon} {log_entry['project_id']}: {log_entry['message']}")
            
            print(f"\nåŒæ­¥æ“ä½œå®Œæˆ!")
            
        except Exception as e:
            print(f"åŒæ­¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            logger.error(f"Sync error: {e}")
            sys.exit(1)
        
        return
    
    # å‰µå»ºå¾Œç«¯ï¼ˆåªæœ‰å¯¦éš›é‹è¡Œæ™‚æ‰åˆå§‹åŒ–ï¼‰
    try:
        backend = create_backend(args.backend, args.db_path)
        if args.backend == "sqlite" and args.db_path:
            logger.info(f"Using {args.backend} backend with custom path: {args.db_path}")
        else:
            logger.info(f"Using {args.backend} backend")
    except Exception as e:
        logger.error(f"Failed to create backend: {e}")
        sys.exit(1)
    
    # ç¢ºä¿è¼¸å‡ºæ˜¯ UTF-8
    if sys.stdout.encoding != 'utf-8':
        sys.stdout.reconfigure(encoding='utf-8')
    
    # æ·»åŠ èª¿è©¦è³‡è¨Š
    logger.info(f"Python version: {sys.version}")
    logger.info(f"Working directory: {os.getcwd()}")
    logger.info(f"Script path: {__file__}")
    
    server = MCPServer(backend)
    try:
        asyncio.run(server.run())
    except KeyboardInterrupt:
        logger.info("Server interrupted")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()